{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6bd69a-3b4f-4243-8010-e767ec036cfb",
   "metadata": {},
   "source": [
    "Understanding Frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b5f600-afdb-4258-881a-06fa7409df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Started with AI\n",
      "Euriel Chukwu\n",
      "2025-09-23\n",
      "['ai', 'machine-learning', 'tutorial']\n",
      "beginner\n",
      "# Welcome\n",
      "\n",
      "This is a tutorial on getting started with AI Agent.\n"
     ]
    }
   ],
   "source": [
    "import frontmatter\n",
    "\n",
    "raw = \"\"\"---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"Euriel Chukwu\"\n",
    "date: \"2025-09-23\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Welcome\n",
    "\n",
    "This is a tutorial on getting started with AI Agent.\n",
    "\"\"\"\n",
    "\n",
    "post = frontmatter.loads(raw)\n",
    "\n",
    "print(post.metadata['title'])  # \"Getting Started with AI\"\n",
    "print(post.metadata['author'])\n",
    "print(post.metadata['date'])\n",
    "print(post.metadata['tags'])   # [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "print(post.metadata['difficulty'])\n",
    "print(post.content)            # Markdown content without frontmatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfbc98c-24b9-41dc-b6fb-b03ebe7db6d3",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3578f57c-5871-45a0-a214-dfc9dadd917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c455a-2c30-4a13-ad28-aede4a490efe",
   "metadata": {},
   "source": [
    "Download repository as a zip file using github URL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb308ca2-50c7-432c-8e05-b6fc88f32ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/ceuriel/atlite/zip/refs/heads/master'\n",
    "resp = requests.get(url)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560377db-3ee6-41d9-b09b-8bbf120b7f6c",
   "metadata": {},
   "source": [
    "Process the zip file in memory without saving to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67cf8b6d-fb49-4ab9-8258-b0429a28f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca68dc-e7cf-4bb9-b7e4-bc41ee3a26c8",
   "metadata": {},
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2150c64c-a13c-4a37-9443-d44f84f88bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"<!--\\nSPDX-FileCopyrightText: Contributors to atlite <https://github.com/pypsa/atlite>\\n\\nSPDX-License-Identifier: CC0-1.0\\n-->\\n\\nCloses # (if applicable).\\n\\n## Changes proposed in this Pull Request\\n\\n\\n## Checklist\\n\\n- [ ] Code changes are sufficiently documented; i.e. new functions contain docstrings and further explanations may be given in `doc`.\\n- [ ] Unit tests for new features were added (if applicable).\\n- [ ] Newly introduced dependencies are added to `environment.yaml`, `environment_docs.yaml` and `setup.py` (if applicable).\\n- [ ] A note for the release notes `doc/release_notes.rst` of the upcoming release is included.\\n- [ ] I consent to the release of this PR's code under the MIT license.\", 'filename': 'atlite-master/.github/pull_request_template.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919e510-3ef6-411a-b868-b13c60fd28d1",
   "metadata": {},
   "source": [
    " ### Complete implementation in a reusable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1652dcb4-703d-4513-9b88-88dee788b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/master'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccd1c6-d9f6-4a48-836b-b96432e901fc",
   "metadata": {},
   "source": [
    "Function can be used for different repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987acd6f-67b8-48b4-b75e-491a64695402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlite documents: 3\n",
      "h2o-3 documents: 89\n"
     ]
    }
   ],
   "source": [
    "h2oai = read_repo_data('h2oai', 'h2o-3')\n",
    "ceuriel_atlite = read_repo_data('ceuriel', 'atlite')\n",
    "\n",
    "print(f\"Atlite documents: {len(ceuriel_atlite)}\")\n",
    "print(f\"h2o-3 documents: {len(h2oai)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043638ca-5d35-4563-b935-ec04be868a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2o-3-master/.github/ISSUE_TEMPLATE/bug_report.md\n",
      "h2o-3-master/.github/ISSUE_TEMPLATE/feature_request.md\n",
      "h2o-3-master/CONTRIBUTING.md\n",
      "h2o-3-master/Changes-prior-3.28.0.1.md\n",
      "h2o-3-master/Changes.md\n",
      "h2o-3-master/DEVEL.md\n",
      "h2o-3-master/README.md\n",
      "h2o-3-master/README_DATA.md\n",
      "h2o-3-master/SECURITY.md\n",
      "h2o-3-master/ec2/README.md\n",
      "h2o-3-master/examples/deeplearning/notebooks/README.md\n",
      "h2o-3-master/gradle/README.md\n",
      "h2o-3-master/h2o-algos/src/main/java/hex/deeplearning/README.md\n",
      "h2o-3-master/h2o-assemblies/main/README.md\n",
      "h2o-3-master/h2o-assemblies/minimal/README.md\n",
      "h2o-3-master/h2o-bindings/bin/readme.md\n",
      "h2o-3-master/h2o-clustering/README.md\n",
      "h2o-3-master/h2o-core/src/main/resources/docs/pieces/columnSummary.md\n",
      "h2o-3-master/h2o-dist/README.md\n",
      "h2o-3-master/h2o-docs/README.md\n",
      "h2o-3-master/h2o-docs/StyleGuide.md\n",
      "h2o-3-master/h2o-docs/src/api/README.md\n",
      "h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md\n",
      "h2o-3-master/h2o-docs/src/api/data-science-example-1/README.md\n",
      "h2o-3-master/h2o-docs/src/api/data-science-example-1/example-flow.md\n",
      "h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md\n",
      "h2o-3-master/h2o-docs/src/booklets/source/DeepWaterBookletErrata.md\n",
      "h2o-3-master/h2o-docs/src/booklets/v2_2015/source/LaTeX_StyleGuide.md\n",
      "h2o-3-master/h2o-docs/src/cheatsheets/H2O_R_Python_Parity.md\n",
      "h2o-3-master/h2o-docs/src/cheatsheets/Python_H2OFrame_PandasDataFrame_Parity.md\n",
      "h2o-3-master/h2o-docs/src/dev/README.md\n",
      "h2o-3-master/h2o-docs/src/dev/custom_functions.md\n",
      "h2o-3-master/h2o-docs/src/dev/lifecycle.md\n",
      "h2o-3-master/h2o-docs/src/front/README.md\n",
      "h2o-3-master/h2o-docs/src/front/assets/fonts/Heebo/README.md\n",
      "h2o-3-master/h2o-docs/src/product/flow/README.md\n",
      "h2o-3-master/h2o-docs/src/product/flow/RecentChanges.md\n",
      "h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md\n",
      "h2o-3-master/h2o-docs/src/product/flow/packs/examples/readme.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/Connecting_RStudio_to_Sparkling_Water.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/DemosAndTests.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/FAQ.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/H2O-DevDocker.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/H2O-DevHadoop.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/H2O-DevLogs.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/H2O-DevS3Creds.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/H2O-Networking.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/LDAP.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/MOJO_QuickStart.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/POJO_QuickStart.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/Videos.md\n",
      "h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/GainsLift.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/Interactions.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/dl/dl.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/gbm/gbm.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/glm/glm.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/glossary.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/kmeans/kmeans.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/pca/pca.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md\n",
      "h2o-3-master/h2o-docs/src/product/tutorials/rf/rf.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/H2OBenefits.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/JavaChanges.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/Migration.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/PressRelease.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/PythonParity.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/RChanges.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/Rdoc.md\n",
      "h2o-3-master/h2o-docs/src/product/upgrade/Upgrade.md\n",
      "h2o-3-master/h2o-genmodel/src/test/resources/hex/genmodel/algos/glrm/experimental/README.md\n",
      "h2o-3-master/h2o-genmodel/src/test/resources/hex/genmodel/algos/isofor/experimental/README.md\n",
      "h2o-3-master/h2o-genmodel/src/test/resources/hex/genmodel/algos/isoforextended/experimental/README.md\n",
      "h2o-3-master/h2o-k8s/README.md\n",
      "h2o-3-master/h2o-k8s/tests/clustering/README.md\n",
      "h2o-3-master/h2o-py-cloud-extensions/README.md\n",
      "h2o-3-master/h2o-py/README.md\n",
      "h2o-3-master/h2o-py/demos/README.md\n",
      "h2o-3-master/h2o-r/README.md\n",
      "h2o-3-master/h2o-r/demos/README.md\n",
      "h2o-3-master/h2o-r/ensemble/README.md\n",
      "h2o-3-master/h2o-web/README.md\n",
      "h2o-3-master/templates/gcp/README.md\n",
      "h2o-3-master/templates/gcp/network/README.md\n",
      "h2o-3-master/vagrant/README.md\n"
     ]
    }
   ],
   "source": [
    "for record in h2oai:\n",
    "    print(record['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e91e779-3920-40aa-809e-5943c9d39cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlite-master/.github/ISSUE_TEMPLATE/feature_request.md\n",
      "atlite-master/.github/pull_request_template.md\n",
      "atlite-master/CONTRIBUTING.md\n"
     ]
    }
   ],
   "source": [
    "for record in ceuriel_atlite:\n",
    "    print(record['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6971a55-1b89-4920-b828-650b9e33e13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa57426-3cc5-4ec2-b7ed-02cfbccd8db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c131c9eb-e7a1-43b5-8626-5da01103a832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLM regression testing',\n",
       " 'description': 'How to run regression testing for LLM outputs.',\n",
       " 'content': 'In this tutorial, you will learn...'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'title': 'LLM regression testing',\n",
    " 'description': 'How to run regression testing for LLM outputs.',\n",
    " 'content': 'In this tutorial, you will learn...'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dbc13-3edf-4b26-a8f5-8b32e7991626",
   "metadata": {},
   "source": [
    "Applying Sliding window method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a721f1-65a9-4882-9c1a-3ceb39b29ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60632ee8-c7ac-4a6a-b9b6-3baeda152686",
   "metadata": {},
   "source": [
    "Process the entire documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5edea32a-629d-4895-a7a9-e51b713f6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2oai_chunks = []\n",
    "\n",
    "for doc in h2oai:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    h2oai_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7dd9d2f-4df1-4c4d-a4b2-d20df31a0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1639\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(h2oai_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729b6d2-86d5-4640-a519-ea4f7e898907",
   "metadata": {},
   "source": [
    "#### Splitting by Paragraphs and Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772630f-03c9-4eb3-8b8d-a7dff588b106",
   "metadata": {},
   "source": [
    "splitting by paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e452c5f-431c-491f-ba17-f7b8ff6f3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = h2oai[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447e89c-623e-4ac5-91f1-b509b379b267",
   "metadata": {},
   "source": [
    "splitting by section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df897ae4-659c-4584-983d-4910fdfde3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7123d0-af7a-4a24-bc97-6ac8f9cd7bd1",
   "metadata": {},
   "source": [
    "Final result by iterating over all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f77be44-4738-4147-9af4-ad2c2727a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "h2oai_chunks = []\n",
    "\n",
    "for doc in h2oai:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        h2oai_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a64a91-8ac7-4e43-8a31-14e23d0d36a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(h2oai_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48085a8e-fa2b-4dde-afb5-a0f4606524e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'h2o-3-master/CONTRIBUTING.md', 'section': \"## Bug Reports and Feature Requests\\n\\nThe single most important contribution that you can make is to report bugs and make feature requests.  The development work on H2O is largely driven by these, so please make your voice heard!  \\n\\nBug reports are most helpful if you send us a script which reproduces the problem.\\n\\nIf you're a customer with an Enterprise Support contract you should send these to support@h2o.ai.\\n\\nIf you're an Open Source community member you should send these to one of:\\n\\n* The h2ostream mailing list, at: [https://groups.google.com/forum/#!forum/h2ostream](https://groups.google.com/forum/#!forum/h2ostream)\\n* Gitter chat, at [https://gitter.im/h2oai/h2o-3](https://gitter.im/h2oai/h2o-3)\\n\\n### How to File Bugs and Feature Requests\\n\\nYou can file a bug report or feature request directly on the [GitHub issues](https://github.com/h2oai/h2o-3/issues) page.\\n\\nOnce inside the Github issues page, click the **New issue** button.\\n\\n ![create](h2o-docs/src/product/images/issue_create.png)\\n\\nA form will display allowing you to enter information about the bug or feature request.\"}\n",
      "{'filename': 'h2o-3-master/CONTRIBUTING.md', 'section': \"## Help and Documentation\\n\\nYou can help others directly and help improve the resources that others read to learn and use H2O by contributing to the formal documentation or the forums.\\n\\nThere are several places that users find information about using H2O:\\n\\n* Formal documentation, at: [http://docs.h2o.ai/](http://docs.h2o.ai/)\\n* The h2ostream mailing list, at: [https://groups.google.com/forum/#!forum/h2ostream](https://groups.google.com/forum/#!forum/h2ostream)\\n* Gitter chat, at [https://gitter.im/h2oai/h2o-3](https://gitter.im/h2oai/h2o-3)\\n* General community sites like Stack Overflow: [http://stackoverflow.com/search?q=h2o](http://stackoverflow.com/search?q=h2o)\\n* Individuals' blogs\\n\\n### Formal Documentation\\n\\nAll of the documentation comes directly from the source tree in GitHub.  To contribute improvements to the formal documentation you may either:\\n\\n* Send the suggestions or changes to support@h2o.ai, h2ostream or Gitter, or\\n* Use Git to make the changes yourself and submit them via a pull request (see below for details)\\n\\n### Forums\\n\\nAnswering questions for other users on h2ostream, Gitter, Stack Overflow and other forums builds the community knowledge base and is a very valuable contribution to H2O.\\n\\n### Blogs\\n\\nSome of the most interesting written materials on the use of H2O for real world problems has been published by community members to their personal blogs.  If you've written something about H2O that you think should be more widely known contact us on h2ostream or Gitter and we will help you get the word out.\"}\n",
      "{'filename': 'h2o-3-master/CONTRIBUTING.md', 'section': '## Tests and Demos\\n\\nThe H2O code base contains tests and demos written in R, Python, Java, Scala and Flow.  These get run as part of every build of the software, either by `gradlew build` on the development machine, or by Jenkins.  Standalone demos are conformed into xUnit tests as part of the build process.  All tests must succeed before we release a stable build.\\n\\nIf you are able to you should clone the H2O git repository, add your test case(s) there, and submit a pull request (see below).  If not, please send your code to h2ostream, Gitter or support@h2o.ai; see above for the links.\\n\\nTest directories include:\\n\\n* user-level tests in R: `h2o-r/tests/`\\n* user-level tests in Python: `h2o-py/tests/`\\n* REST API tests in Python: `py/testdir_multi_jvm/`\\n* platform tests in Java: `h2o-core/src/test/java/`\\n* algorithm tests in Java: `h2o-algos/src/test/java/`\\n* Flow tests in saved notebooks: `h2o-docs/src/product/flow/packs`\\n\\nFor Scala tests see the Sparkling Water GitHub repo.'}\n",
      "{'filename': 'h2o-3-master/CONTRIBUTING.md', 'section': '## Contribute Code!\\n\\nYou can contribute R, Python, Java or Scala code for H2O, either for bug fixes or new features.  If you have your own idea about what to work on a good place to begin is to discuss it with us on [Gitter](https://gitter.im/h2oai/h2o-3) so that we can help point you in the right direction.\\n\\nFor ideas about what to work on see the H2O-3 [Github issues](https://github.com/h2oai/h2o-3/issues).\\n\\nTo contribute code, fork the H2O-3 GitHub repo, create a branch for your work and when you\\'re done, create a pull request.  Once a PR has been created, it will trigger the H2O-3 Jenkins test system and should start automatically running tests (this will show up in the comment history on the PR).  Make sure all the tests pass.  A few notes:\\n\\n* If there\\'s not already a GitHub issue associated with this task, please create one.\\n* If there is a GitHub issue associated with your changes, choose a branch name that includes that  number.  e.g. `gh-1234_new_pca`\\n* New code must come with unit tests.  Here are some examples of [runits](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests), [pyunits](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests) and [junits](https://github.com/h2oai/h2o-3/tree/master/h2o-algos/src/test/java/hex) to help get you started.\\n* Use the GitHub number in the PR title.  e.g. \"GH-1234: Added new `pca_method` option in the PCA algorithm\".\\n* Write a summary of all changes & additions to the code in the PR description and add a link to the GitHub issue.'}\n",
      "{'filename': 'h2o-3-master/Changes-prior-3.28.0.1.md', 'section': '## H2O-Dev\\n\\n### Shackleford (0.2.3.6) - 5/8/15\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o-dev/rel-shackleford/6/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o-dev/rel-shackleford/6/index.html</a>\\n\\n\\n#### New Features\\n\\n##### Python\\n\\n- Set up POJO download for Python client [(#13894)](https://github.com/h2oai/h2o-3/issues/13894) [(github)](https://github.com/h2oai/h2o-dev/commit/4b06cc2415f5d5b0bb0be6a6ef419ed6ff065ada)\\n\\n### ##Sparkling Water\\n\\n- Publish h2o-scala and h2o-app latest version to maven central [(#13422)](https://github.com/h2oai/h2o-3/issues/13422)\\n\\n#### Enhancements\\n\\n##### Algorithms\\n\\n- Use AUC\\'s default threshold for label-making for binomial classifiers predict() [(#14043)](https://github.com/h2oai/h2o-3/issues/14043) [(github)](https://github.com/h2oai/h2o-dev/commit/588a95df335d534080737832adf846e4c12ba7c6)\\n- GLM update [(github)](https://github.com/h2oai/h2o-dev/commit/c1c8e2e428554307870ac1a595bb35f60e258245)\\n- Cleanup AUC2, make incremental version [(github)](https://github.com/h2oai/h2o-dev/commit/2d7d064229f9577cafc9a6d08b47efc653e0c546)\\n- Name change: `override_with_best_model` -> `overwrite_with_best_model` [(github)](https://github.com/h2oai/h2o-dev/commit/f14dca82a529e2cb080800e258ca23dcb6ac9535)\\n- Couple of GLM updates [(github)](https://github.com/h2oai/h2o-dev/commit/05cec9710a3578789bb34f04a5134f4320ac7547)\\n- Disable `_replicate_training_data` for data that\\'s larger than 10GB [(github)](https://github.com/h2oai/h2o-dev/commit/4a1fed5f292826a4bc89eafffc6c04bb7449644c)\\n- Added `replicate_training_data` param for DL [(github)](https://github.com/h2oai/h2o-dev/commit/e95e4870869d159f8d468e4193fc7201887f1661)\\n- Change a few kmeans output parameters so no longer dividing by `nrows` or `num_clusters` [(github)](https://github.com/h2oai/h2o-dev/commit/9933486a61113af5ef6d3ed329c70eb7fbdc61a8)\\n- GLMValidation Updated auc computation [(github)](https://github.com/h2oai/h2o-dev/commit/280e8f8390dfc5b4d6b5a571f06930bab9b5c7e5)\\n- Do not delete model metrics at end of GBM/DRF [(github)](https://github.com/h2oai/h2o-dev/commit/d10d4522eae38bfc3bf45208266b8b5e5806d524)\\n\\n\\n##### API\\n\\n- Clean REST api for Parse [(#13977)](https://github.com/h2oai/h2o-3/issues/13977)\\n- Removes `is_valid`, `invalid_lines`, and domains from REST api [(github)](https://github.com/h2oai/h2o-dev/commit/f5997de8f59f2eefd454afeb0e91a6a1d5c6672b)\\n- Annotate domains output field as expert level [(github)](https://github.com/h2oai/h2o-dev/commit/523af95008d3fb3b5d2269bb87a1de3235f6f828)\\n\\n##### Python\\n\\n- Implement h2o.interaction() [(#13842)](https://github.com/h2oai/h2o-3/issues/13842) [(github)](https://github.com/h2oai/h2o-dev/commit/3d43cb22afa0892c2c913b15e7b4bb5d4889443b)\\n- nice tables in ipython! [(github)](https://github.com/h2oai/h2o-dev/commit/fc6ecdc3d000375307f5731569a36a3c4e4fbf4c)\\n- added deeplearning weights and biases accessors and respective pyunit. [(github)](https://github.com/h2oai/h2o-dev/commit/7eb9f22262533ca7e335e9580af8afc3cf54c4b0)\\n\\n##### R\\n\\n- Cleaner client POJO download for R [(#13893)](https://github.com/h2oai/h2o-3/issues/13893)\\n- Implement h2o.interaction() [(#13842)](https://github.com/h2oai/h2o-3/issues/13842) [(github)](https://github.com/h2oai/h2o-dev/commit/58fa2f1e89bddd97b13a3884e15385ad0a5905d8)\\n- R: h2o.impute missing [(#13783)](https://github.com/h2oai/h2o-3/issues/13783)\\n- `validation_frame` is passed through to h2o [(github)](https://github.com/h2oai/h2o-dev/commit/184fe3a546e43c9b3d5664a808f6b30d3eaddab8)\\n- Adding GBM accessor function runits [(github)](https://github.com/h2oai/h2o-dev/commit/41d039196088df081ad77610d3e2d6550868f11b)\\n- Adding changes to `h2o.hit_ratio_table` to be like other accessors (i.e., no train) [(github)](https://github.com/h2oai/h2o-dev/commit/dc4a20151d9b415fe4708cff1bafc4fe61e802e0)\\n- add h2o.getPOJO to R, fix impute ast build in python [(github)](https://github.com/h2oai/h2o-dev/commit/8f192a7c87fa30782249af2e85ea2470fae491da)\\n\\n\\n\\n##### System\\n\\n- Change NA strings to an array in ParseSetup [(#13979)](https://github.com/h2oai/h2o-3/issues/13979)\\n- Document way of passing S3 credentials for S3N [(#13930)](https://github.com/h2oai/h2o-3/issues/13930)\\n- Add H2O-dev doc on docs.h2o.ai via a new structure (proposed below) [(#13355)](https://github.com/h2oai/h2o-3/issues/13355)\\n- Rapids Ref Doc [(#13659)](https://github.com/h2oai/h2o-3/issues/13659)\\n- Show Timestamp and Duration for all model scoring histories [(#13999)](https://github.com/h2oai/h2o-3/issues/13999) [(github)](https://github.com/h2oai/h2o-dev/commit/c02aa5efaf28ac21915c6fc427fc9b099aabee23)\\n- Logs slow reads, mainly meant for noting slow S3 reads [(github)](https://github.com/h2oai/h2o-dev/commit/d3b19e38ab083ea327ecea60a354cc91a22b68a8)\\n- Make prediction frame column names non-integer [(github)](https://github.com/h2oai/h2o-dev/commit/7fb855ca5eb546c03d1b7ea84b5b48093958ae9a)\\n- Add String[] factor_columns instead of int[] factors [(github)](https://github.com/h2oai/h2o-dev/commit/c381da2ae1a51b268b1f359d0594f3aea5feef04)\\n- change the runtime exception to a Log.info() if interface doesn\\'t support multicast [(github)](https://github.com/h2oai/h2o-dev/commit/68f277c0ba8508bbebb34afac19f6233129bb55e)\\n- More robust way to copy Flow files to web root per Prithvi [(github)](https://github.com/h2oai/h2o-dev/commit/4e1b067e6456074107332c10b1af66443395325a)\\n- Switches `na_string` from a single value per column to an array per column [(github)](https://github.com/h2oai/h2o-dev/commit/a37ec777c10158a7afb29d1d5502f3c8082f6453)\\n\\n##### Web UI\\n\\n- Model output improvements [(private-#512)](https://github.com/h2oai/private-h2o-3/issues/512)\\n\\n\\n#### Bug Fixes\\n\\n\\n##### Algorithms\\n\\n- H2O cloud shuts down with some H2O.fail error, while building some kmeans clusters [(#14031)](https://github.com/h2oai/h2o-3/issues/14031) [(github)](https://github.com/h2oai/h2o-dev/commit/d95dec2a412e87e054fc000032da375023b87dce)\\n- GLM:beta constraint does not seem to be working [(#14063)](https://github.com/h2oai/h2o-3/issues/14063)\\n- GBM - random attack bug (probably because `max_after_balance_size` is really small) [(#14041)](https://github.com/h2oai/h2o-3/issues/14041) [(github)](https://github.com/h2oai/h2o-dev/commit/8625632c4759b07f75ac85acc43d69cdb9b38e15)\\n- GLM: LBFGS objval java lang assertion error [(#14023)](https://github.com/h2oai/h2o-3/issues/14023) [(github)](https://github.com/h2oai/h2o-dev/commit/dc4a20151d9b415fe4708cff1bafc4fe61e802e0)\\n- PCA Cholesky NPE [(#13906)](https://github.com/h2oai/h2o-3/issues/13906)\\n- GBM: H2o returns just 5525 trees, when ask for a much larger number of trees [(#13848)](https://github.com/h2oai/h2o-3/issues/13848)\\n- CM returned by AUC2 doesn\\'t agree with manual-made labels from F1-optimal threshold [(private-#426)](https://github.com/h2oai/private-h2o-3/issues/426)\\n- AUC: h2o reporting wrong auc on a modified covtype data [(#13877)](https://github.com/h2oai/h2o-3/issues/13877)\\n- GLM: Build model => Predict => Residual deviance/Null deviance different from training/validation metrics [(#15421)](https://github.com/h2oai/h2o-3/issues/15421)\\n- KMeans metrics incomplete [(#14010)](https://github.com/h2oai/h2o-3/issues/14010)\\n- GLM: Java Assertion Error [(#14006)](https://github.com/h2oai/h2o-3/issues/14006)\\n- Random forest bug [(#13996)](https://github.com/h2oai/h2o-3/issues/13996)\\n- A particular random forest model has an empty (training) metric json `max_criteria_and_metric_scores` [(#13965)](https://github.com/h2oai/h2o-3/issues/13965)\\n- PCA results exhibit numerical inaccuracies compared to R [(#13532)](https://github.com/h2oai/h2o-3/issues/13532)\\n- DRF: reporting wrong depth for attached dataset [(#13988)](https://github.com/h2oai/h2o-3/issues/13988)\\n- added missing \"names\" column name to beta constraints processing [(github)](https://github.com/h2oai/h2o-dev/commit/fedcf159f8e842212812b0636b26ca9aa9ef1097)\\n- Fix `balance_classes` probability correction consistency between H2O and POJO [(github)](https://github.com/h2oai/h2o-dev/commit/5201f6da1196434866be6e70da996fb7c5967b7b)\\n- Fix in GLM scoring - check actual for NaNs as well [(github)](https://github.com/h2oai/h2o-dev/commit/e45c023a767dc26083f7fb26d9616ee234c03d2e)\\n\\n##### Python\\n\\n- Cannot import_file path=url python interface [(#14039)](https://github.com/h2oai/h2o-3/issues/14039)\\n- head()/tail() should show labels, rather than number encoding, for enum columns [(#13998)](https://github.com/h2oai/h2o-3/issues/13998)\\n- h2o.py: for binary response printing transpose and hence wrong cm [(#13994)](https://github.com/h2oai/h2o-3/issues/13994)\\n\\n##### R\\n\\n- Broken Summary in R [(#14053](https://github.com/h2oai/h2o-3/issues/14053)\\n- h2oR summary: displaying no labels in summary [(#13990)](https://github.com/h2oai/h2o-3/issues/13990)\\n- R/Python impute bugs [(#14035)](https://github.com/h2oai/h2o-3/issues/14035)\\n- R: h2o.varimp doubles the print statement [(#14048)](https://github.com/h2oai/h2o-3/issues/14048)\\n- R: h2o.varimp returns NULL when model has no variable importance [(#14058)](https://github.com/h2oai/h2o-3/issues/14058)\\n- h2oR: h2o.confusionMatrix(my_gbm, validation=F) should not show a null [(#13837)](https://github.com/h2oai/h2o-3/issues/13837)\\n- h2o.impute doesn\\'t impute [(#14005)](https://github.com/h2oai/h2o-3/issues/14005)\\n- R: as.h2o cutting entries when trying to import data.frame into H2O [(private-#397)](https://github.com/h2oai/private-h2o-3/issues/397)\\n- The default names are too long, for an R-datafile parsed to H2O, and needs to be changed [(#13959)](https://github.com/h2oai/h2o-3/issues/13959)\\n- H2o.confusionMatrix: when invoked with threshold gives error [(#13991)](https://github.com/h2oai/h2o-3/issues/13991)\\n- removing train and adding error messages for valid = TRUE when there\\'s not validation metrics [(github)](https://github.com/h2oai/h2o-dev/commit/cc3cf212300e252f987992e98d22a9fb6e46be3f)\\n\\n\\n\\n##### System\\n\\n- Download logs is returning the same log file bundle for every node [(#14036)](https://github.com/h2oai/h2o-3/issues/14036)\\n- ParseSetup is useless and misleading for SVMLight [(#13978)](https://github.com/h2oai/h2o-3/issues/13978)\\n- Fixes bug that was short circuiting the setting of column names [(github)](https://github.com/h2oai/h2o-dev/commit/5296456c425d9f9c0a467a2b65d448940f76c6a6)\\n\\n##### Web UI\\n\\n- Flow: Predict should not show mse confusion matrix etc [(#13972)](https://github.com/h2oai/h2o-3/issues/13972) [(github)](https://github.com/h2oai/h2o-dev/commit/6bc90e19cfefebd0db3ec4a46d3a157e258ff858)\\n- Flow: Raw frames left out after importing files from directory [(#14026)](https://github.com/h2oai/h2o-3/issues/14026)\\n\\n---\\n\\n### Shackleford (0.2.3.5) - 5/1/15\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o-dev/rel-shackleford/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o-dev/rel-shackleford/5/index.html</a>\\n\\n#### New Features\\n\\n##### API\\n\\n- Need a /Log REST API to log client-side errors to H2O\\'s log [(private-#399)](https://github.com/h2oai/private-h2o-3/issues/399)\\n\\n\\n### ##Python\\n\\n- add impute to python interface [(github)](https://github.com/h2oai/h2o-dev/commit/8a4d39e8bca6a4acfb8fc5f01a8febe07e519a08)\\n\\n##### System\\n\\n- Job admission control [(#13520)](https://github.com/h2oai/h2o-3/issues/13520) [(github)](https://github.com/h2oai/h2o-dev/commit/f5ef7323c72cf4be2dabf57a298fcc3d6687e9dd)\\n- Get Flow Exceptions/Stack Traces in H2O Logs [(#13905)](https://github.com/h2oai/h2o-3/issues/13905)\\n\\n#### Enhancements\\n\\n##### Algorithms\\n\\n- GLM: Name to be changed from normalized to standardized in output to be consistent between input/output [(#13937)](https://github.com/h2oai/h2o-3/issues/13937)\\n- GLM: It would be really useful if the coefficient magnitudes are reported in descending order [(#13908)](https://github.com/h2oai/h2o-3/issues/13908)\\n- #13520: Limit DL models to 100M parameters [(github)](https://github.com/h2oai/h2o-dev/commit/5678a26447704021d8905e7c37dfcd37b74b7327)\\n- #13520: Add accurate memory-based admission control for GBM/DRF [(github)](https://github.com/h2oai/h2o-dev/commit/fc06a28c64d24ecb3a46a6a84d90809d2aae4875)\\n- relax the tolerance a little more...[(github)](https://github.com/h2oai/h2o-dev/commit/a24f4886b94b93f71452848af3a7d0f7b440779c)\\n- Tree depth correction [(github)](https://github.com/h2oai/h2o-dev/commit/2ad89a3eff0d8aa411b94b1d6f387051671b9bf8)\\n- Comment out `duration_in_ms` for now, as it\\'s always left at 0 [(github)](https://github.com/h2oai/h2o-dev/commit/8008f017e10424623f966c141280d080f08f80b5)\\n- Updated min mem computation for glm [(github)](https://github.com/h2oai/h2o-dev/commit/446d5c30cdffcf04a4b7e0feaefa501187049efb)\\n- GLM update: added lambda search info to scoring history [(github)](https://github.com/h2oai/h2o-dev/commit/90ac3bb9cc07e4f50b50b08aad8a33279a0ff43d)\\n\\n##### Python\\n\\n- python .show() on model and metric objects should match R/Flow as much as possible [(private-#401)](https://github.com/h2oai/private-h2o-3/issues/401)\\n- GLM model output, details from Python [(private-#564)](https://github.com/h2oai/private-h2o-3/issues/564)\\n- GBM model output, details from Python [(private-#557)](https://github.com/h2oai/private-h2o-3/issues/557)\\n- Run GBM from Python [(private-#560)](https://github.com/h2oai/private-h2o-3/issues/560)\\n- map domain to result from /Frames if needed [(github)](https://github.com/h2oai/h2o-dev/commit/b1746a52cd4399d58385cd29914fa54870680093)\\n- added confusion matrix to metric output [(github)](https://github.com/h2oai/h2o-dev/commit/f913cc1643774e9c2ec5455620acf11cbd613711)\\n- update `metrics_base_confusion_matrices()` [(github)](https://github.com/h2oai/h2o-dev/commit/41c0a4b0079426860ac3b65079d6be0e46c6f69c)\\n- fetch out `string_data` if type is string [(github)](https://github.com/h2oai/h2o-dev/commit/995e135e0a49e492cccfb65974160b04c764eb11)\\n\\n##### R\\n\\n- GBM model output, details from R [(private-#558)](https://github.com/h2oai/private-h2o-3/issues/558)\\n- Run GBM from R [(private-#561)](https://github.com/h2oai/private-h2o-3/issues/561)\\n- check if it\\'s a frame then check NA [(github)](https://github.com/h2oai/h2o-dev/commit/d61de7d0b8a9dac7d5d6c7f841e19c88983308a1)\\n\\n##### System\\n\\n- Report MTU to logs [(#13606)](https://github.com/h2oai/h2o-3/issues/13606) [(github)](https://github.com/h2oai/h2o-dev/commit/bbc3ad54373a2c865ce913917ef07c9892d62603)\\n- Make parameter changes Log.info() instead of Log.warn() [(github)](https://github.com/h2oai/h2o-dev/commit/7047a46fff612f41cc678f297cfcbc57ed8165fd)\\n\\n##### Web UI\\n\\n- Flow: Confusion matrix: good to have consistency in the column and row name (letter) case [(#13954)](https://github.com/h2oai/h2o-3/issues/13954)\\n- Run GBM Multinomial from Flow [(private-#548)](https://github.com/h2oai/private-h2o-3/issues/548)\\n- Run GBM Regression from Flow [(private-#547)](https://github.com/h2oai/private-h2o-3/issues/547)\\n- Sort model types in alphabetical order in Flow [(#13992)](https://github.com/h2oai/h2o-3/issues/13992)\\n\\n\\n\\n#### Bug Fixes\\n\\nThe following changes are to resolve incorrect software behavior:\\n\\n##### Algorithms\\n\\n- GLM: Model output display issues [(#13939)](https://github.com/h2oai/h2o-3/issues/13939)\\n- h2o.glm: ignores validation set [(#13941)](https://github.com/h2oai/h2o-3/issues/13941)\\n- DRF: reports wrong number of leaves in a summary [(#13915)](https://github.com/h2oai/h2o-3/issues/13915)\\n- h2o.glm: summary of a prediction frame gives na\\'s as labels [(#13942)](https://github.com/h2oai/h2o-3/issues/13942)\\n- GBM: reports wrong max depth for a binary model on german data [(#13827)](https://github.com/h2oai/h2o-3/issues/13827)\\n- GLM: Confusion matrix missing in R for binomial models [(#13933)](https://github.com/h2oai/h2o-3/issues/13933) [(github)](https://github.com/h2oai/h2o-dev/commit/d8845e3245491a85c2cc6c932d5fad2c260c19d3)\\n- GLM: On airlines(40g) get ArrayIndexOutOfBoundsException [(#13950)](https://github.com/h2oai/h2o-3/issues/13950)\\n- GLM: Build model => Predict => Residual deviance/Null deviance different from training/validation metrics [(#15421)](https://github.com/h2oai/h2o-3/issues/15421)\\n- Domains returned by GLM for binomial classification problem are integers, but should be mapped to their label [(#13981)](https://github.com/h2oai/h2o-3/issues/13981)\\n- GLM: Validation on non training data gives NaN Res Deviance and AIC [(#13987)](https://github.com/h2oai/h2o-3/issues/13987)\\n- Confusion matrix has nan\\'s in it [(#13969)](https://github.com/h2oai/h2o-3/issues/13969)\\n- glm fix: pass `model_id` from R (was being dropped) [(github)](https://github.com/h2oai/h2o-dev/commit/9d8698177a9d0a70668d2d51005947d0adda0292)\\n\\n##### Python\\n\\n- H2OPy: warns about version mismatch even when installed the latest from master [(#13962)](https://github.com/h2oai/h2o-3/issues/13962)\\n- Columns of type enum lose string label in Python H2OFrame.show() [(#13948)](https://github.com/h2oai/h2o-3/issues/13948)\\n- Bug in H2OFrame.show() [(private-#396)](https://github.com/h2oai/private-h2o-3/issues/396) [(github)](https://github.com/h2oai/h2o-dev/commit/b319969cff0f0e7a805e49563e863a1dbb0e1aa0)\\n\\n\\n##### R\\n\\n- h2o.confusionMatrix for binary response gives not-found thresholds [(#13940)](https://github.com/h2oai/h2o-3/issues/13940)\\n- GLM: model_id param is ignored in R [(#13989)](https://github.com/h2oai/h2o-3/issues/13989)\\n- h2o.confusionmatrix: mixing cases(letter) for categorical labels while printing multinomial cm [(#13980)](https://github.com/h2oai/h2o-3/issues/13980)\\n- fix the dupe thresholds error [(github)](https://github.com/h2oai/h2o-dev/commit/e40d4fd50cfd9438b2f693228ca20ad4d6648b46)\\n- extra arg in impute example [(github)](https://github.com/h2oai/h2o-dev/commit/5a41e7672fa30b2e66a1261df8976d18e89f0057)\\n- fix missing param data [(github)](https://github.com/h2oai/h2o-dev/commit/6719d94b30caf214fac2c61759905c7d5d57a9ac)\\n\\n\\n##### System\\n\\n- Builds : Failing intermittently due to java.lang.StackOverflowError [(#13955)](https://github.com/h2oai/h2o-3/issues/13955)\\n- Get H2O cloud hang with NPE and roll up stats problem, when click on build model glm from flow, on laptop after running a few python demos and R scripts [(#13946)](https://github.com/h2oai/h2o-3/issues/13946)\\n\\n##### Web UI\\n\\n- Flow :=> Airlines dataset => Build models glm/gbm/dl => water.DException$DistributedException: from /172.16.2.183:54321; by class water.fvec.RollupStats$ComputeRollupsTask; class java.lang.NullPointerException: null [(#13595)](https://github.com/h2oai/h2o-3/issues/13595)\\n- Flow => Preview Pojo => collapse not working [(#13960)](https://github.com/h2oai/h2o-3/issues/13960)\\n- Flow => Any algorithm => Select response => Select Add all for ignored columns => Try to unselect some from ignored columns => Build => Response column IsDepDelayed not found in frame: allyears_1987_2013.hex. [(#13961)](https://github.com/h2oai/h2o-3/issues/13961)\\n- Flow => ROC curve select something on graph => Table is displayed for selection => Collapse ROC curve => Doesn\\'t collapse table, collapses only graph [(#13985)](https://github.com/h2oai/h2o-3/issues/13985)\\n\\n\\n---\\n\\n### Severi (0.2.2.16) - 4/29/15\\n\\n\\n#### New Features\\n\\n##### Python\\n\\n- Release h2o-dev to PyPi [(#13751)](https://github.com/h2oai/h2o-3/issues/13751)\\n- Python Documentation [(#13887)](https://github.com/h2oai/h2o-3/issues/13887)\\n- Python docs Wrap Up [(#13949)](https://github.com/h2oai/h2o-3/issues/13949)\\n- add getters for res/null dev, fix kmeans,dl getters [(github)](https://github.com/h2oai/h2o-dev/commit/3f9839c25628e44cba77b44905c38c21bee60a9c)\\n\\n\\n\\n#### Enhancements\\n\\n##### Algorithms\\n\\n- Use partial-sum version of mat-vec for DL POJO [(#13920)](https://github.com/h2oai/h2o-3/issues/13920)\\n- Always store weights and biases for DLTest Junit [(github)](https://github.com/h2oai/h2o-dev/commit/5bcbad8e07fd592e2db701adf9b4974a5b4470b1)\\n- Show the DL model size in the model summary [(github)](https://github.com/h2oai/h2o-dev/commit/bdba19a99b863cd2f49ff1bdcd4ca648b60d1372)\\n- Remove assertion in hot loop [(github)](https://github.com/h2oai/h2o-dev/commit/9d1682e2821fc648dda02497ba5200e45bd6b6f5)\\n- Rename ADMM to IRLSM [(github)](https://github.com/h2oai/h2o-dev/commit/6a108d38e7b9473a792a5ba36b58a860166c84c4)\\n- Added no intercept option to glm [(github)](https://github.com/h2oai/h2o-dev/commit/6d99bd194cbc4500f519e306f28384d7dca407e1)\\n- Code cleanup. Moved ModelMetricsPCAV3 out of H2O-algos [(github)](https://github.com/h2oai/h2o-dev/commit/1f691681407b579ed0b71e4e6d452120dc3263dd)\\n- Improve DL model checkpoint logic [(github)](https://github.com/h2oai/h2o-dev/commit/9a13070c0de6ac2bf34b0e60c305de7358711965)\\n- Updated glm output [(github)](https://github.com/h2oai/h2o-dev/commit/4359a17f573bf27f0ac5e078143299de09011325)\\n- Renamed normalized coefficients to standardized coefficients in glm output [(github)](https://github.com/h2oai/h2o-dev/commit/39b814d37e9e161d1dd943741afcff59fd83d745)\\n- Use proper tie breaking for NB [(github)](https://github.com/h2oai/h2o-dev/commit/4bbbd1b6161e8d2d62f8d3d9cb600e3c6d678653)\\n- Add check that DL parameters aren\\'t modified by model training [(github)](https://github.com/h2oai/h2o-dev/commit/84d4ab6bc63b314bab4f38e629e77fb8207f705f)\\n- Reduce tolerances [(github)](https://github.com/h2oai/h2o-dev/commit/0654d3c2d644abb9aa0d0c25e032db1a4fd219ad)\\n- If no observations of a response leveland prediction is numeric, assume it is drawn from standard normal distribution (mean 0, standard deviation 1). Add validation test with split frame for naive Bayes [(github)](https://github.com/h2oai/h2o-dev/commit/50a5d9cbb1f77db568a23573f6cff0cf45cb36af)\\n\\n\\n\\n##### Python\\n\\n- replaced H2OFrame.send_frame() calls with cbind Exprs so that lazy evaluation is enforced [(github)](https://github.com/h2oai/h2o-dev/commit/2799b8cb2d01270556d4481a40af4a8da6f0519f)\\n- change default xmx/s behavior of h2o.init() [(github)](https://github.com/h2oai/h2o-dev/commit/843a232c52e6b357dbd84db3253b3e33b8297803)\\n- better handling of single row return and print [(github)](https://github.com/h2oai/h2o-dev/commit/b2e782bf17352009992ad1252762f43977f95c8b)\\n\\n\\n##### R\\n\\n- Added interpolation to quantile to match R type 7 [(github)](https://github.com/h2oai/h2o-dev/commit/a330ffb6ff30c5500e3fb6a80fe92ac8b123a4be)\\n- Removed and tidied if\\'s in quantile.H2OFrame since it now uses match.arg [(github)](https://github.com/h2oai/h2o-dev/commit/237306039a3e2483c92ac310e157ec515b885530)\\n- Connected validation dataset to glm in R [(github)](https://github.com/h2oai/h2o-dev/commit/e71895bd3fc7507092f65cbde6a914f74dacf85d)\\n- Removing h2o.aic from seealso link (doesn\\'t exist) and updating documentation [(github)](https://github.com/h2oai/h2o-dev/commit/8fa994efea831722dd333327789a858ed902bc79)\\n\\n\\n##### System\\n\\n- Add number of rows (per node) to ChunkSummary [(#13922)](https://github.com/h2oai/h2o-3/issues/13922) [(github)](https://github.com/h2oai/h2o-dev/commit/06d33469e0fabb0ae452f29dc633647aef8c9bb3)\\n- allow nrow as alias for count in groupby [(github)](https://github.com/h2oai/h2o-dev/commit/fbeef36b9dfea422dfed7f209a196731d9312e8b)\\n- Only launches task to fill in SVM zeros if the file is SVM [(github)](https://github.com/h2oai/h2o-dev/commit/d816c52a34f2e8f549f8a3b0bf7d976333366553)\\n- Adds more log traces to track progress of post-ingest actions [(github)](https://github.com/h2oai/h2o-dev/commit/c0073164d8392fd2d079db840b84e6330bebe2e6)\\n- Adds svm as a file extension to the hex name cleanup [(github)](https://github.com/h2oai/h2o-dev/commit/0ad9eec48650491f5ec2e01c010be9987dac0a21)\\n\\n##### Web UI\\n\\n- Flow: Inspect data => Round decimal points to 1 to be consistent with h2o1 [(#13445)](https://github.com/h2oai/h2o-3/issues/13445)\\n- Setup POJO download method for Flow [(#13895)](https://github.com/h2oai/h2o-3/issues/13895)\\n- Pretty-print POJO preview in flow [(#13924)](https://github.com/h2oai/h2o-3/issues/13924)\\n- Flow: It would be good if \\'get predictions\\' also shows the data [(#13870)](https://github.com/h2oai/h2o-3/issues/13870)\\n- GBM model output, details in Flow [(private-#556)](https://github.com/h2oai/private-h2o-3/issues/556)\\n- Display a linked data table for each visualization in Flow [(#13319)](https://github.com/h2oai/h2o-3/issues/13319)\\n- Run GBM binomial from Flow (needs proper CM) [(#13927)](https://github.com/h2oai/h2o-3/issues/13927)\\n\\n\\n\\n#### Bug Fixes\\n\\n\\n##### Algorithms\\n\\n- GLM: results from model and prediction on the same dataset do not match [(#13907)](https://github.com/h2oai/h2o-3/issues/13907)\\n- GLM: when select AUTO as solver, for prostate, glm gives all zero coefficients [(#13902)](https://github.com/h2oai/h2o-3/issues/13902)\\n- Large (DL) models cause oversize issues during serialization [(#13925)](https://github.com/h2oai/h2o-3/issues/13925)\\n- Fixed name change for ADMM [(github)](https://github.com/h2oai/h2o-dev/commit/bc126aa8d4d7c5901ef90120c7997c67466922ae)\\n\\n##### API\\n\\n- Fix schema warning on startup [(#13929)](https://github.com/h2oai/h2o-3/issues/13929) [(github)](https://github.com/h2oai/h2o-dev/commit/bd9ae8013bc0de261e7258af85784e9e6f20df5e)\\n\\n\\n##### Python\\n\\n- H2OVec.row_select(H2OVec) fails on case where only 1 row is selected [(#13931)](https://github.com/h2oai/h2o-3/issues/13931)\\n- fix pyunit [(github)](https://github.com/h2oai/h2o-dev/commit/79344be836d9111fee77ddebe034234662d7064f)\\n\\n##### R\\n\\n- R: Parse of zip file fails, Summary fails on citibike data [(#13823)](https://github.com/h2oai/h2o-3/issues/13823)\\n- h2o. performance reports a different Null Deviance than the model object for the same dataset [(#13804)](https://github.com/h2oai/h2o-3/issues/13804)\\n- h2o.glm: no example on h2o.glm help page [(#13945)](https://github.com/h2oai/h2o-3/issues/13945)\\n- H2O R: Confusion matrices from R still confused [(#13890)](https://github.com/h2oai/h2o-3/issues/13890) [(github)](https://github.com/h2oai/h2o-dev/commit/36c887ddadd47682745b64812e081dcb2fa36659)\\n- R: h2o.confusionMatrix(\"H2OModel\", ...) extra parameters not working [(#13936)](https://github.com/h2oai/h2o-3/issues/13936) [(github)](https://github.com/h2oai/h2o-dev/commit/ca59b2be46dd07caad60882b5c1daed0ee4837c6)\\n- h2o.confusionMatrix for binomial gives not-found thresholds on S3 -airlines 43g [(#13940)](https://github.com/h2oai/h2o-3/issues/13940)\\n- H2O summary quartiles outside tolerance of (max-min)/1000 [(#13663)](https://github.com/h2oai/h2o-3/issues/13663)\\n- fix space headers issue from R (was not url-encoding the column strings) [(github)](https://github.com/h2oai/h2o-dev/commit/f121b0324e981e229cd2704df11a0a946d4b2aeb)\\n- R CMD fixes [(github)](https://github.com/h2oai/h2o-dev/commit/62a1d7df8bceeea181b87d83f922db854f28b6db)\\n- Fixed broken R interface - make `validation_frame` non-mandatory [(github)](https://github.com/h2oai/h2o-dev/commit/18fba95392f94e566b80797839e5eb2899057333)\\n\\n##### Sparkling Water\\n\\n- Sparkling water : #UDP-Recv ERRR: UDP Receiver error on port 54322java.lang.ArrayIndexOutOfBoundsException:[(#13314)](https://github.com/h2oai/h2o-3/issues/13314)\\n\\n\\n##### System\\n\\n- Mapr 3.1.1 : Memory is not being allocated for what is asked for instead the default is what cluster gets [(#13921)](https://github.com/h2oai/h2o-3/issues/13921)\\n- GLM: AIOOBwith msg \\'-14\\' at water.RPC$2.compute2(RPC.java:593) [(#13903)](https://github.com/h2oai/h2o-3/issues/13903)\\n- h2o.glm: model summary listing same info twice [(#13901)](https://github.com/h2oai/h2o-3/issues/13901)\\n- Parse: Detect and reject UTF-16 encoded files [(private-#404)](https://github.com/h2oai/private-h2o-3/issues/404)\\n- DataInfo Row categorical encoding AIOOBE [(private-#406)](https://github.com/h2oai/private-h2o-3/issues/406)\\n- Fix POJO Preview exception [(github)](https://github.com/h2oai/h2o-dev/commit/d553710f66ef989dc33a86608c5cf352a7d98168)\\n- Fix NPE in ChunkSummary [(github)](https://github.com/h2oai/h2o-dev/commit/cd113515257ee1c493fe84616deb0643400ef32c)\\n- fix global name collision [(github)](https://github.com/h2oai/h2o-dev/commit/bde0b6d8fed4009367b2e2ddf999bd71cbda3b3f)\\n\\n\\n### Severi (0.2.2.15) - 4/25/15\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o-dev/rel-severi/15/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o-dev/rel-severi/15/index.html</a>\\n\\n#### New Features\\n\\n\\n##### Python\\n\\n- added min, max, sum, median for H2OVecs and respective pyunit [(github)](https://github.com/h2oai/h2o-dev/commit/3ec14f0bfe2d045ac57b3133a7ae12ea8e70aa3c)\\n- added min(), max(), and sum() functionality on H2OFrames and respective pyunits [(github)](https://github.com/h2oai/h2o-dev/commit/c86cf2bfa396f38b2a035405553a1f4bb34f55c0)\\n\\n\\n##### Web UI\\n\\n- View POJO in Flow [(#13769)](https://github.com/h2oai/h2o-3/issues/13769)\\n- help > about page or add version on main page for easy bug reporting. [(#13792)](https://github.com/h2oai/h2o-3/issues/13792)\\n- POJO generation: GLM [(#13703)](https://github.com/h2oai/h2o-3/issues/13703) [(github)](https://github.com/h2oai/h2o-dev/commit/35683e29e39489bc2349461e78524328e4b24e63)\\n- GLM model output, details in Flow [(private-#563)](https://github.com/h2oai/private-h2o-3/issues/563)\\n\\n\\n#### Enhancements\\n\\n##### Algorithms\\n\\n- K means output clean up [(private-#486)](https://github.com/h2oai/private-h2o-3/issues/486)\\n- Add FNR/TNR/FPR/TPR to threshold tables, remove recall, specificity [(github)](https://github.com/h2oai/h2o-dev/commit/1de4910b8d295b2eaa79b8e96422f45746458d92)\\n- Add accessor for variable importances for DL [(github)](https://github.com/h2oai/h2o-dev/commit/e11323bca7cc4e58fb2d899a3c307f42f4a8624e)\\n- Relax CM error tolerance for F1-optimal threshold now that AUC2 doesn\\'t necessarily create consistent thresholds with its own CMs. [(github)](https://github.com/h2oai/h2o-dev/commit/3ab3af08e28a64acc9a406ef5ff19bf6b1c7855a)\\n- Added scoring history to glm [(github)](https://github.com/h2oai/h2o-dev/commit/a652ba0388784bb54f0a69f524d21f08d66eabc5)\\n- Added model summary to glm [(github)](https://github.com/h2oai/h2o-dev/commit/c0d221cb964a072358602b2c13fd2c33b9fa9f4b)\\n- Add flag to support reading data from S3N [(github)](https://github.com/h2oai/h2o-dev/commit/b4efd2c9802a8e39bc5d24ea6593e420ecfbaea9)\\n- Added degrees of freedom to GLM metrics schemas [(github)](https://github.com/h2oai/h2o-dev/commit/6f153381b085e94358cc0e5e317d36dce3072131)\\n- Allow DL scoring_history to be unlimited in length [(github)](https://github.com/h2oai/h2o-dev/commit/5485b46d240415afa3ff3e7bc8a532791ae12419)\\n- add plotting for binomial models [(github)](https://github.com/h2oai/h2o-dev/commit/d332e98a12bcd40ceb9714067eefce64dad97125)\\n- Ignore certain parameters that are not applicable (class balancing, max CM size, etc.) [(github)](https://github.com/h2oai/h2o-dev/commit/5c70787a6e43697f57c0df918bb4cdbf93d18018)\\n- Updated glm scoring, fill training/validation metrics in model output [(github)](https://github.com/h2oai/h2o-dev/commit/9b3cc3ec2a8f81771e0eddaf663dbfd6690dbd04)\\n- Rename gbm loss parameter to distribution [(github)](https://github.com/h2oai/h2o-dev/commit/d9a1e9730f3296bc125965647e5aef2ae114368c)\\n- Fix GBM naming: loss -> distribution [(github)](https://github.com/h2oai/h2o-dev/commit/ef93923dc83f03a9ef16ed23bb1c411bd26e067e)\\n- GLM LBFGS update [(github)](https://github.com/h2oai/h2o-dev/commit/3c75a2edc20b7abc9a17b9732a0bac9c7f194feb)\\n- na.rm for quantile is default behavior [(github)](https://github.com/h2oai/h2o-dev/commit/3ac19b6f1cb7e2a64fa6b783a19e8ddb42713caf)\\n- GLM update: enabled `max_predictors` in REST, updated lbfgs [(github)](https://github.com/h2oai/h2o-dev/commit/a58d515364e749b1147452a98399eb8dfadd11af)\\n- Remove `keep_cross_validation_splits` for now from DL [(github)](https://github.com/h2oai/h2o-dev/commit/569ae442a4905a3dbbf47a3d5c03461ce68be36a)\\n- Get rid of sigma in the model metrics, instead show r2 [(github)](https://github.com/h2oai/h2o-dev/commit/b12bf9496a46f25f066f3bab512cd7d81795f0f4)\\n- Don\\'t show `score_every_iteration` for DL [(github)](https://github.com/h2oai/h2o-dev/commit/089aedfed90ca30e715a58363c19f3f1fe47318c)\\n- Don\\'t print too large confusion matrices in Tree models [(github)](https://github.com/h2oai/h2o-dev/commit/56d51f51e5fdc5f9f25d8838003236909637b272)\\n\\n##### API\\n\\n- publish h2o-model.jar via REST API [(#13767)](https://github.com/h2oai/h2o-3/issues/13767)\\n- move all schemas and endpoints to v3 [(#13463)](https://github.com/h2oai/h2o-3/issues/13463)\\n- clean up routes (remove AddToNavbar, fix /Quantiles, etc) [(#13610)](https://github.com/h2oai/h2o-3/issues/13610) [(github)](https://github.com/h2oai/h2o-dev/commit/7f6eff5b47aa1e273de4710a3b26408e3516f5af)\\n- More data in chunk_homes call. Add num_chunks_per_vec. Add num_vec. [(github)](https://github.com/h2oai/h2o-dev/commit/635d020b2dfc45364331903c282e82e3f20d028d)\\n- Added chunk_homes route for frames [(github)](https://github.com/h2oai/h2o-dev/commit/1ae94079762fdbfcdd1e39d65578752860c278c6)\\n- Update to use /3 routes [(github)](https://github.com/h2oai/h2o-dev/commit/be422ff963bb47daf9c8e7cbcb478e6a6dbbaea5)\\n\\n##### Python\\n\\n- Python client should check that version number == server version number [(#13786)](https://github.com/h2oai/h2o-3/issues/13786)\\n- Add asfactor for month [(github)](https://github.com/h2oai/h2o-dev/commit/43c9b82ab463e712910d1353013d499684021858)\\n- in Expr.show() only show 10 or less rows. remove locate from runit test because full path used [(github)](https://github.com/h2oai/h2o-dev/commit/51f4f69deba9b76837b35bf2a0b85ee2e4b20db7)\\n- change nulls to () [(github)](https://github.com/h2oai/h2o-dev/commit/a138cc25edc9f948d263732f665d352e44ee39c1)\\n- sigma is no longer part of ModelMetricsRegressionV3 [(github)](https://github.com/h2oai/h2o-dev/commit/6f2a7390ce0feb0a3d880f1bb42168642a665bb0)\\n\\n\\n##### R\\n\\n- Fix integer -> int in R [(github)](https://github.com/h2oai/h2o-dev/commit/ce05247e29b5756108999689d0b10fa17edb84a8)\\n- add autoencoder show method [(github)](https://github.com/h2oai/h2o-dev/commit/31d70f3ddb4bad63b42ec12c8fd70b9d5745a7d1)\\n- accessor is $ not @ [(github)](https://github.com/h2oai/h2o-dev/commit/a43e3d6924004e34aa7b5400d149c7dab26afe70)\\n- add `hit_ratio_table` and `varimp` calls to R [(github)](https://github.com/h2oai/h2o-dev/commit/caa7dc001edc63928ca7a8dadba773dd25983f1d)\\n- add h2o.predict as alternative [(github)](https://github.com/h2oai/h2o-dev/commit/e5a48f8faaededa3fd445d4b1415665c96f1291c)\\n- update model output in R [(github)](https://github.com/h2oai/h2o-dev/commit/e5d101ad60c12513f2e4c7b1d16534962eb86291)\\n\\n\\n##### System\\n\\n- Port MissingValueInserter EndPoint to h2o-dev. [(#13457)](https://github.com/h2oai/h2o-3/issues/13457)\\n- Rapids: require a (put \"key\" %frame) [(#13856)](https://github.com/h2oai/h2o-3/issues/13856)\\n- Need pojo base model jar file embedded in h2o-dev via build process [(#13768)](https://github.com/h2oai/h2o-3/issues/13768) [(github)](https://github.com/h2oai/h2o-dev/commit/85f73202157f0ab4ee3487de8fc095951e761196)\\n- Make .json the default [(#13611)](https://github.com/h2oai/h2o-3/issues/13611) [(github)](https://github.com/h2oai/h2o-dev/commit/f3e88060da1a6af73940587c16fef669b1d5bbd5)\\n- Rename class for clarification [(github)](https://github.com/h2oai/h2o-dev/commit/89c4fe32d333940865112d8922249fc48eebe096)\\n- Classifies all NA columns as numeric. Also improves preview sampling accuracy by trimming partial lines at end of chunk. [(github)](https://github.com/h2oai/h2o-dev/commit/6b1cf7a180428c04cdd445974a318f5777c7f607)\\n- Implements sampling of files within the ParseSetup preview. This prevents poor column type guesses from only sampling the beginning of a file. [(github)](https://github.com/h2oai/h2o-dev/commit/038da7398941558656c1bda52b8429f4022c449e).\\n- Rename fields `drop_na20_col` [(github)](https://github.com/h2oai/h2o-dev/commit/75131e9f1e6d1cd6788f239d72e11cf104028c3f)\\n- allow for many deletes as final statements in a block [(github)](https://github.com/h2oai/h2o-dev/commit/aa3e2d3ef00761ca4a4c942f33ffaf80951abc7b)\\n- rename initF -> init_f, dropNA20Cols -> drop_na20_cols [(github)](https://github.com/h2oai/h2o-dev/commit/e81eae78267d4981c74d866e40a48015d2086371)\\n- Removed tweedie param [(github)](https://github.com/h2oai/h2o-dev/commit/03902225aa912473ceb01e9cce045846949faecf)\\n- thresholds -> threshold [(github)](https://github.com/h2oai/h2o-dev/commit/69adcc8639c889b68ca0c97b7385a45c41d93401)\\n- JSON of TwoDimTable with all null values in the first column (no row headers) now doesn\\'t have an empty column for of \"\" or nulls. [(github)](https://github.com/h2oai/h2o-dev/commit/de54085fe94aaa1e23aa74254fc5b8b64b85f76d)\\n- move H2O_Load, fix all the timezone functions [(github)](https://github.com/h2oai/h2o-dev/commit/871959887825aec1e246ae8e19e11d03db9637c5)\\n- Add extra verbose printout in case Frames don\\'t match identically [(github)](https://github.com/h2oai/h2o-dev/commit/b8943f9228fe996887377f521ec135745d957033)\\n- allow delayed column lookup [(github)](https://github.com/h2oai/h2o-dev/commit/5060436d4d7ea7363dc74b9c0850258a38b2715a)\\n- add mixed type list [(github)](https://github.com/h2oai/h2o-dev/commit/99eb7106eadb0fcbe815752b181e085ba57349db)\\n- Added WaterMeterIo to count persist info [(github)](https://github.com/h2oai/h2o-dev/commit/2fa38aaff08584bcbf92ee2287343c2c40765d76)\\n- Remove special setChunkSize code in HDFS and NFS file vec [(github)](https://github.com/h2oai/h2o-dev/commit/136e7667a438a856ff06478b8ba7f6b716aced7b)\\n- add check for Frame on string parse [(github)](https://github.com/h2oai/h2o-dev/commit/f835768b080df1bc395bdbe0f60c2d35db8da0d8)\\n- Disable Memory Cleaner [(github)](https://github.com/h2oai/h2o-dev/commit/644f38f38c9f75a0008cb012c25c399a06805786)\\n- Handle \\'<\\' chars in Keys when swapping [(github)](https://github.com/h2oai/h2o-dev/commit/65e936912f236cacacd706bc30406f13b46acf7e)\\n- allow for colnames in slicing [(github)](https://github.com/h2oai/h2o-dev/commit/947e6cc1f0becb58a5d36387a6500b303293c6a8)\\n- Adjusts parse type detection. If column is all one string value, declare it an enum [(github)](https://github.com/h2oai/h2o-dev/commit/08e7845b786c445862554d4f4c5dac7c78204284)\\n\\n##### Web UI\\n\\n- nice algo names in the Flow dropdown (full word names) [(#13698)](https://github.com/h2oai/h2o-3/issues/13698)\\n- Compute and Display Hit Ratios [(#13622)](https://github.com/h2oai/h2o-3/issues/13622)\\n- Limit POJO preview to 1000 lines [(github)](https://github.com/h2oai/h2o-dev/commit/ce82fe74da9641d72c47dabd03514c7402998f76)\\n\\n\\n#### Bug Fixes\\n\\n\\n##### Algorithms\\n\\n- GLM: lasso i.e alpha =1 seems to be giving wrong answers [(#13758)](https://github.com/h2oai/h2o-3/issues/13758)\\n- AUC: h2o reports .5 auc when actual auc is 1 [(#13867)](https://github.com/h2oai/h2o-3/issues/13867)\\n- h2o.glm: No output displayed for the model [(#13846)](https://github.com/h2oai/h2o-3/issues/13846)\\n- h2o.glm model object output needs a fix [(#13803)](https://github.com/h2oai/h2o-3/issues/13803)\\n- h2o.glm model object says : fill me in GLMModelOutputV2; I think I\\'m redundant [1] FALSE [(#13754)](https://github.com/h2oai/h2o-3/issues/13754)\\n- GLM : Build GLM Model => Java Assertion error [(#13678)](https://github.com/h2oai/h2o-3/issues/13678)\\n- GLM :=> Progress shows -100% [(#13849)](https://github.com/h2oai/h2o-3/issues/13849)\\n- GBM: Negative sign missing in initF value for ad dataset [(#13868)](https://github.com/h2oai/h2o-3/issues/13868)\\n- K-Means takes a validation set but doesn\\'t use it [(#13814)](https://github.com/h2oai/h2o-3/issues/13814)\\n- Absolute_MCC is NaN (sometimes) [(#13836)](https://github.com/h2oai/h2o-3/issues/13836) [(github)](https://github.com/h2oai/h2o-dev/commit/4480f22b6b3a38abb776339bee506b356f589c90)\\n- GBM: A proper error msg should be thrown when the user sets the max depth =0 [(#13826)](https://github.com/h2oai/h2o-3/issues/13826) [(github)](https://github.com/h2oai/h2o-dev/commit/df77f3de5e8940f3598af67d520f185d1e478ec4)\\n- DRF Regression Assertion Error [(#13812)](https://github.com/h2oai/h2o-3/issues/13812)\\n- h2o.randomForest: if h2o is not returning the mse for the 0th tree then it should not be reported in the model object [(#13799)](https://github.com/h2oai/h2o-3/issues/13799)\\n- GBM: Got exception `class java.lang.AssertionError` with msg `null` java.lang.AssertionError at hex.tree.gbm.GBM$GBMDriver$GammaPass.map [(#13685)](https://github.com/h2oai/h2o-3/issues/13685)\\n- GBM: Got exception `class java.lang.AssertionError` with msg `null` java.lang.AssertionError at hex.ModelMetricsMultinomial$MetricBuildMultinomial.perRow [(private-#434)](https://github.com/h2oai/private-h2o-3/issues/434)\\n- GBM get java.lang.AssertionError: Coldata 2199.0 out of range C17:5086.0-19733.0 step=57.214844 nbins=256 isInt=1 [(private-#439)](https://github.com/h2oai/private-h2o-3/issues/439)\\n- GLM: glmnet objective function better than h2o.glm [(#13739)](https://github.com/h2oai/h2o-3/issues/13739)\\n- GLM: get AIOOB:-36 at hex.glm.GLMTask$GLMIterationTask.postGlobal(GLMTask.java:733) [(#13880)](https://github.com/h2oai/h2o-3/issues/13880) [(github)](https://github.com/h2oai/h2o-dev/commit/5bba2df2e208a0a7c7fd19732971575eb9dc2259)\\n- Fixed glm behavior in case no rows are left after filtering out NAs [(github)](https://github.com/h2oai/h2o-dev/commit/57dc0f3a168ed835c48aa29f6e0d6322c6a5523a)\\n- Fix memory leak in validation scoring in K-Means [(github)](https://github.com/h2oai/h2o-dev/commit/f3f01e4dfe66e0181df0ff85a2a9a108295df94c)\\n\\n##### API\\n\\n- API unification: DataFrame should be able to accept URI referencing file on local filesystem [(#13700)](https://github.com/h2oai/h2o-3/issues/13700) [(github)](https://github.com/h2oai/h2o-dev/commit/a72e77388c0f7b17e4595482f9afe42f14055ce9)\\n\\n\\n##### Python\\n\\n- Python: describe returning all zeros [(#13863)](https://github.com/h2oai/h2o-3/issues/13863)\\n- python/R & merge() [(#13822)](https://github.com/h2oai/h2o-3/issues/13822)\\n- python Expr min, max, median, sum bug [(#13833)](https://github.com/h2oai/h2o-3/issues/13833) [(github)](https://github.com/h2oai/h2o-dev/commit/7839efd5899366a3b51ef79156717a718ab01c38)\\n\\n\\n\\n\\n##### R\\n\\n- (R and Python) clients must not pass response to DL AutoEncoder model builder [(#13883)](https://github.com/h2oai/h2o-3/issues/13883) [(github)](https://github.com/h2oai/h2o-dev/commit/bc78ecfa5e0c37cebd55ed9ba7b3ae6163ebdc66)\\n- h2o.varimp, h2o.hit_ratio_table missing in R [(#13830)](https://github.com/h2oai/h2o-3/issues/13830)\\n- GLM: No help for h2o.glm from R [(#13722)](https://github.com/h2oai/h2o-3/issues/13722)\\n- h2o.confusionMatrix not working for binary response [(#13770)](https://github.com/h2oai/h2o-3/issues/13770) [(github)](https://github.com/h2oai/h2o-dev/commit/a834cbc80a62062c55456233ce27ba5e9c3a87a3)\\n- h2o.splitframe complains about destination keys [(#13771)](https://github.com/h2oai/h2o-3/issues/13771)\\n- h2o.assign does not work [(#13772)](https://github.com/h2oai/h2o-3/issues/13772) [(github)](https://github.com/h2oai/h2o-dev/commit/b007c0b59dbb03716571384adb3271fbe8385a55)\\n- H2oR: should display only first few entries of the variable importance in model object [(#13838)](https://github.com/h2oai/h2o-3/issues/13838)\\n- R: h2o.confusion matrix needs formatting [(#13753)](https://github.com/h2oai/h2o-3/issues/13753)\\n- R: h2o.confusionMatrix => No Confusion Matrices for H2ORegressionMetrics [(#13701)](https://github.com/h2oai/h2o-3/issues/13701)\\n- h2o.deeplearning: model object output needs a fix [(#13809)](https://github.com/h2oai/h2o-3/issues/13809)\\n- h2o.varimp, h2o.hit_ratio_table missing in R [(#13830)](https://github.com/h2oai/h2o-3/issues/13830)\\n- force gc more frequently [(github)](https://github.com/h2oai/h2o-dev/commit/0db9a3716ecf573ef4b3c71ec1116cc8b27e62c6)\\n\\n##### System\\n\\n- MapR FS loads are too slow [(#13912)](https://github.com/h2oai/h2o-3/issues/13912)\\n- ensure that HDFS works from Windows [(#13800)](https://github.com/h2oai/h2o-3/issues/13800)\\n- Summary: on a time column throws,\\'null\\' is not an object (evaluating \\'column.domain[level.index]\\') in Flow [(#13855)](https://github.com/h2oai/h2o-3/issues/13855)\\n- Parse: An enum column gets parsed as int for the attached file [(#13598)](https://github.com/h2oai/h2o-3/issues/13598)\\n- Parse => 40Mx1_uniques => class java.lang.RuntimeException [(#13719)](https://github.com/h2oai/h2o-3/issues/13719)\\n- if there are fewer than 5 unique values in a dataset column, mins/maxs reports e+308 values [(#13160)](https://github.com/h2oai/h2o-3/issues/13160) [(github)](https://github.com/h2oai/h2o-dev/commit/49c966791a146687039350689bc09cee10f38820)\\n- Sparkling water - `DataFrame[T_UUID]` to `SchemaRDD[StringType]` [(#13760)](https://github.com/h2oai/h2o-3/issues/13760)\\n- Sparkling water - `DataFrame[T_NUM(Long)]` to `SchemaRDD[LongType]` [(#13756)](https://github.com/h2oai/h2o-3/issues/13756)\\n- Sparkling water - `DataFrame[T_ENUM]` to `SchemaRDD[StringType]` [(#13755)](https://github.com/h2oai/h2o-3/issues/13755)\\n- Inconsistency in row and col slicing [(private-#424)](https://github.com/h2oai/private-h2o-3/issues/424) [(github)](https://github.com/h2oai/h2o-dev/commit/edd8923a438282e3c24d086e1a03b88471d58114)\\n- rep_len expects literal length only [(private-#421)](https://github.com/h2oai/private-h2o-3/issues/421) [(github)](https://github.com/h2oai/h2o-dev/commit/1783a889a54d2b23da8bd8ec42774f52efbebc60)\\n- cbind and = don\\'t work within a single rapids block [(private-#443)](https://github.com/h2oai/private-h2o-3/issues/443)\\n- Rapids response for c(value) does not have frame key [(private-#430)](https://github.com/h2oai/private-h2o-3/issues/430)\\n- S3 parse takes forever [(#13864)](https://github.com/h2oai/h2o-3/issues/13864)\\n- Parse => Enum unification fails in multi-node parse [(#13709)](https://github.com/h2oai/h2o-3/issues/13709) [(github)](https://github.com/h2oai/h2o-dev/commit/0db8c392070583f32849447b65784da18197c14d)\\n- All nodes are not getting updated with latest status of each other nodes info [(#13757)](https://github.com/h2oai/h2o-3/issues/13757)\\n- Cluster creation is sometimes rejecting new nodes (post jenkins-master-1128+) [(#13795)](https://github.com/h2oai/h2o-3/issues/13795)\\n- Parse => Multiple files 1 zip/ 1 csv gives Array index out of bounds [(#13828)](https://github.com/h2oai/h2o-3/issues/13828)\\n- Parse => failed for X5MRows6KCols ==> OOM => Cluster dies [(#13824)](https://github.com/h2oai/h2o-3/issues/13824)\\n- /frame/foo pagination weirded out [(private-#412)](https://github.com/h2oai/private-h2o-3/issues/412) [(github)](https://github.com/h2oai/h2o-dev/commit/c40da923d97720466fb372758d66509aa628e97c)\\n- Removed code that flipped enums to strings [(github)](https://github.com/h2oai/h2o-dev/commit/7d56bcee73cf3c90b498cadf8601610e5f145dbc)\\n\\n\\n\\n\\n##### Web UI\\n\\n- Flow: It would be really useful to have the mse plots back in GBM [(#13875)](https://github.com/h2oai/h2o-3/issues/13875)\\n- State change in Flow is not fully validated [(#13904)](https://github.com/h2oai/h2o-3/issues/13904)\\n- Flows : Not able to load saved flows from hdfs [(#13860)](https://github.com/h2oai/h2o-3/issues/13860)\\n- Save Function in Flow crashes [(#13774)](https://github.com/h2oai/h2o-3/issues/13774) [(github)](https://github.com/h2oai/h2o-dev/commit/ad724bf7af86180d7045a99790602bd52908945f)\\n- Flow: should throw a proper error msg when user supplied response have more categories than algo can handle [(#13854)](https://github.com/h2oai/h2o-3/issues/13854)\\n- Flow display of a summary of a column with all missing values fails. [(private-#450)](https://github.com/h2oai/private-h2o-3/issues/450)\\n- Split frame UI improvements [(private-#414)](https://github.com/h2oai/private-h2o-3/issues/414)\\n- Flow : Decimal point precisions to be consistent to 4 as in h2o1 [(#13832)](https://github.com/h2oai/h2o-3/issues/13832)\\n- Flow: Prediction frame is outputing junk info [(#13813)](https://github.com/h2oai/h2o-3/issues/13813)\\n- EC2 => Cluster of 16 nodes => Water Meter => shows blank page [(#13819)](https://github.com/h2oai/h2o-3/issues/13819)\\n- Flow: Predict - \"undefined is not an object (evaluating `prediction.thresholds_and_metric_scores.name`) [(#13540)](https://github.com/h2oai/h2o-3/issues/13540)\\n- Flow: inspect getModel for PCA returns error [(#13602)](https://github.com/h2oai/h2o-3/issues/13602)\\n- Flow, RF: Can\\'t get Predict results; \"undefined is not an object (evaluating `prediction.confusion_matrices.length`)\" [(#13687)](https://github.com/h2oai/h2o-3/issues/13687)\\n- Flow, GBM: getModel is broken -Error processing GET /3/Models.json/gbm-b1641e2dc3-4bad-9f69-a5f4b67051ba null is not an object (evaluating `source.length`) [(#13787)](https://github.com/h2oai/h2o-3/issues/13787)\\n\\n\\n\\n### Severi (0.2.2.1) - 4/10/15\\n\\n\\n#### New Features\\n\\n##### R\\n\\n- Implement /3/Frames/<my_frame>/summary [(#13020)](https://github.com/h2oai/h2o-3/issues/13020) [(github)](https://github.com/h2oai/h2o-dev/commit/07bc295e1687d88e40d8391ea78f91aff4183a6f)\\n- add allparameters slot to allow default values to be shown [(github)](https://github.com/h2oai/h2o-dev/commit/9699a4c43ce4936dbc3019c75b2a36bd1ef22b45)\\n- add log loss accessor [(github)](https://github.com/h2oai/h2o-dev/commit/22ace748ae4004305ae9edb04f17141d0dbd87d4)\\n\\n\\n#### Enhancements\\n\\n##### Algorithms\\n\\n- POJO generation: GBM [(#13704)](https://github.com/h2oai/h2o-3/issues/13704)\\n- POJO generation: DRF [(#13705)](https://github.com/h2oai/h2o-3/issues/13705)\\n- Compute and Display Hit Ratios [(#13622)](https://github.com/h2oai/h2o-3/issues/13622) [(github)](https://github.com/h2oai/h2o-dev/commit/04b13f2fb05b752dbd04121f50845bebcb6f9955)\\n- Add DL POJO scoring [(#13578)](https://github.com/h2oai/h2o-3/issues/13578)\\n- Allow validation dataset for AutoEncoder [(#13574)](https://github.com/h2oai/h2o-3/issues/13574)\\n- #13573: Add log loss to binomial and multinomial model metric [(github)](https://github.com/h2oai/h2o-dev/commit/8982a0a1ba575bd5ca6ca3e854382e03146743cd)\\n- Port MissingValueInserter EndPoint to h2o-dev [(#13457)](https://github.com/h2oai/h2o-3/issues/13457)\\n- increase tolerance to 2e-3 (was 1e-3 ..failed with 0.001647 relative difference [(github)](https://github.com/h2oai/h2o-dev/commit/9ce26530cc7d4d4aef55b5e0debc978bacc8ac78)\\n- change tolerance to 1e-3 [(github)](https://github.com/h2oai/h2o-dev/commit/bb5aa7806d37e1148029ef848a8df0d7a28cba2a)\\n- Add option to export weights and biases to REST API / Flow. [(github)](https://github.com/h2oai/h2o-dev/commit/2f711045f2678622a7d6d44f7210adb74a513ce6)\\n- Add scree plot for H2O PCA models and fix Runit test. [(github)](https://github.com/h2oai/h2o-dev/commit/5743019075e023590019fab9a4da8c09500643a0)\\n- Remove quantiles from the model builders list. [(github)](https://github.com/h2oai/h2o-dev/commit/6283dfbc626cb2b9a65df2f4b90a87371ef5c752)\\n- GLM update: added row filtering argument to line search task, fixed issues with dfork/asyncExec [(github)](https://github.com/h2oai/h2o-dev/commit/7492ed95915a85121f0042b5800d58bda2805a87)\\n- Updated rho-setting in GLM. [(github)](https://github.com/h2oai/h2o-dev/commit/a130fd6abbd13fff44e0eb813d31cc04afcedef7)\\n- No threshold 0.5; use the default (max F1) instead [(github)](https://github.com/h2oai/h2o-dev/commit/e56425d6f83aa0e1dc523acc3ed4b5a49d0223fc)\\n- GLM update: updated initilization, NA row filtering, default lambda is now empty, will be picked based on the fraction of lambda_max. [(github)](https://github.com/h2oai/h2o-dev/commit/04a3f8e496c00de9e35c8ee33a6d3ddb8466a3d8)\\n- Updated ADMM solver. [(github)](https://github.com/h2oai/h2o-dev/commit/1a6ef44a24463b2538731065fc39eef4531e062e)\\n- Added makeGLMModel call. [(github)](https://github.com/h2oai/h2o-dev/commit/9792ff032356982915d814c7918c48582bf3ffea)\\n- Start with classification error NaN at t=0 for DL, not with 1. [(github)](https://github.com/h2oai/h2o-dev/commit/c33ca1f385844c90c473fe2941bbb8b2c2ab663f)\\n- Relax DL POJO relative tolerance to 1e-2. [(github)](https://github.com/h2oai/h2o-dev/commit/f7a2fe37845c00980a23f8e68b34ad044fa647e2)\\n- Override nfeatures() method in DLModelOutput. [(github)](https://github.com/h2oai/h2o-dev/commit/7c6bcf844c8e162b8fb16ee1f7e208717b82d606)\\n- Renaming of fields in GLM [(github)](https://github.com/h2oai/h2o-dev/commit/d21180ab5ea973848d4cdcb896c32400c3d77d38)\\n- GLM: Take out Balance Classes [(#13782)](https://github.com/h2oai/h2o-3/issues/13782)\\n\\n\\n\\n##### API\\n\\n- schema metadata for Map fields should include the key and value types [(#13742)](https://github.com/h2oai/h2o-3/issues/13742) [(github)](https://github.com/h2oai/h2o-dev/commit/4b55db36f259740043b8418e23e298fb0ed5a43d)\\n- schema metadata should include the superclass [(#13743)](https://github.com/h2oai/h2o-3/issues/13743)\\n- rest api naming convention: n_folds vs ntrees [(#13727)](https://github.com/h2oai/h2o-3/issues/13727)\\n- schema metadata for Map fields should include the key and value types [(#13742)](https://github.com/h2oai/h2o-3/issues/13742)\\n- Create REST Endpoint for exposing .java pojo models [(#13766)](https://github.com/h2oai/h2o-3/issues/13766)\\n\\n\\n\\n\\n\\n\\n##### Python\\n\\n- Run GLM from Python (including LBFGS) [(private-#567)](https://github.com/h2oai/private-h2o-3/issues/567)\\n- added H2OFrame show(), as_list(), and slicing pyunits [(github)](https://github.com/h2oai/h2o-dev/commit/b1febc33faa336924ffdb416d8d4a3cb8bba37fa)\\n- changed solver parameter to \"L_BFGS\" [(github)](https://github.com/h2oai/h2o-dev/commit/93e71509bcfa0e76d344819214a08b944ccbfb89)\\n- added multidimensional slicing of H2OFrames and Exprs. [(github)](https://github.com/h2oai/h2o-dev/commit/7d9be09ff0b68f92e46a0c7336dcf8134d026b88)\\n- add h2o.groupby to python interface [(github)](https://github.com/h2oai/h2o-dev/commit/aee9522f0c7edbd960ded78f5ba01daf6d54925b)\\n- added H2OModel.confusionMatrix() to return confusion matrix of a prediction [(github)](https://github.com/h2oai/h2o-dev/commit/6e6bc378f3a10c094752470de786be600a0a98b3)\\n\\n\\n\\n\\n\\n##### R\\n\\n- #13571, #13571, #13571.\\n  -R client now sends the data frame column names and data types to ParseSetup.\\n  -R client can get column names from a parsed frame or a list.\\n  -Respects client request for column data types [(github)](https://github.com/h2oai/h2o-dev/commit/ba063be25d3fbb658b016ff514083284e2d95d78)\\n- R: Cannot create new columns through R [(#13565)](https://github.com/h2oai/h2o-3/issues/13565)\\n- H2O-R: it would be more useful if h2o.confusion matrix reports the actual class labels instead of [,1] and [,2] [(#13536)](https://github.com/h2oai/h2o-3/issues/13536)\\n- Support both multinomial and binomial CM [(github)](https://github.com/h2oai/h2o-dev/commit/4ad2ed007635a7e8c2fd4fb0ae985cf00a81df15)\\n\\n\\n\\n##### System\\n\\n- Flow: Standardize `max_iters`/`max_iterations` parameters [(#13439)](https://github.com/h2oai/h2o-3/issues/13439) [(github)](https://github.com/h2oai/h2o-dev/commit/6586f1f2f233518a7ee6179ec2bc19d9d7b61d15)\\n- Add ERROR logging level for too-many-retries case [(#13156)](https://github.com/h2oai/h2o-3/issues/13156) [(github)](https://github.com/h2oai/h2o-dev/commit/ae5bdf26453643b58403a6a4fb136259ac9acd6b)\\n- Simplify checking of cluster health. Just report the status immediately. [(github)](https://github.com/h2oai/h2o-dev/commit/25fde3914460e7572cf3500f236d43e50a502aab)\\n- reduce timeout [(github)](https://github.com/h2oai/h2o-dev/commit/4c93ddfd92801fdef60961d44ccb7cf512f37a90)\\n- strings can have \\' or \" beginning [(github)](https://github.com/h2oai/h2o-dev/commit/034243f094ae67fb15e8d575146f6e64c8727d39)\\n- Throw a validation error in flow if any training data cols are non-numeric [(github)](https://github.com/h2oai/h2o-dev/commit/091c18331f19a5a1db8b3eb0b000ca72abd29f81)\\n- Add getHdfsHomeDirectory(). [(github)](https://github.com/h2oai/h2o-dev/commit/68c3f730576c21bd1191f8af9dd7fd9445b89f83)\\n- Added --verbose.  [(github)](https://github.com/h2oai/h2o-dev/commit/5e772f8314a340666e4e80b3480b2105ceb91251)\\n\\n\\n##### Web UI\\n\\n- #13698: nice algo names in the Flow dropdown (full word names) [(github)](https://github.com/h2oai/h2o-dev/commit/ab87c26ae8ac17691034f4d9014ee17ba2168d89)\\n- Unbreak Flow\\'s ConfusionMatrix display. [(github)](https://github.com/h2oai/h2o-dev/commit/45911f2ff28e2357d5545ac23135f090c10f13e0)\\n- POJO generation: DL [(#13706)](https://github.com/h2oai/h2o-3/issues/13706)\\n\\n\\n\\n#### Bug Fixes\\n\\n\\n##### Algorithms\\n\\n- GLM : Build GLM model with nfolds brings down the cloud => FATAL: unimplemented [(#13721)](https://github.com/h2oai/h2o-3/issues/13721) [(github)](https://github.com/h2oai/h2o-dev/commit/79123971fdea5660355f57de4e9a02d3712250b1)\\n- DL : Build DL Model => FATAL: unimplemented: n_folds >= 2 is not (yet) implemented => SHUTSDOWN CLOUD [(#13718)](https://github.com/h2oai/h2o-3/issues/13718) [(github)](https://github.com/h2oai/h2o-dev/commit/6f59755f28c3fc3cee549630bb5e22a985d185ab)\\n- GBM => Build GBM model => No enum constant  hex.tree.gbm.GBMModel.GBMParameters.Family.AUTO [(#13714)](https://github.com/h2oai/h2o-3/issues/13714)\\n- GBM: When run with loss = auto with a numeric column get- error :No enum constant hex.tree.gbm.GBMModel.GBMParameters.Family.AUTO\\n  [(#13699)](https://github.com/h2oai/h2o-3/issues/13699) [(github)](https://github.com/h2oai/h2o-dev/commit/15d5b5a6108d165f230a856aa3c38a4eb158ee93)\\n- gbm: does not complain when min_row >dataset size [(#13686)](https://github.com/h2oai/h2o-3/issues/13686) [(github)](https://github.com/h2oai/h2o-dev/commit/a3d9d1cca2aa070c536084ca1bb90eecfbf609e7)\\n- GLM: reports wrong residual degrees of freedom [(#13660)](https://github.com/h2oai/h2o-3/issues/13660)\\n- H2O dev reports less accurate aucs than H2O [(#13594)](https://github.com/h2oai/h2o-3/issues/13594)\\n- GLM : Build GLM model fails => ArrayIndexOutOfBoundsException [(#13593)](https://github.com/h2oai/h2o-3/issues/13593)\\n- divide by zero in modelmetrics for deep learning [(#13546)](https://github.com/h2oai/h2o-3/issues/13546)\\n- GBM: reports 0th tree mse value for the validation set, different than the train set ,When only train sets is provided [(#13562)](https://github.com/h2oai/h2o-3/issues/13562)\\n- GBM: Initial mse in bernoulli seems to be off [(#13556)](https://github.com/h2oai/h2o-3/issues/13556)\\n- GLM : Build Model fails with Array Index Out of Bound exception [(#13446)](https://github.com/h2oai/h2o-3/issues/13446) [(github)](https://github.com/h2oai/h2o-dev/commit/78773be9f40e1403457e42378baf0d1aeaf3e32d)\\n- Custom Functions don\\'t work in apply() in R [(#13431)](https://github.com/h2oai/h2o-3/issues/13431)\\n- GLM failure: got NaNs and/or Infs in beta on airlines [(#13362)](https://github.com/h2oai/h2o-3/issues/13362)\\n- MetricBuilderMultinomial.perRow AssertionError while running GBM [(private-#440)](https://github.com/h2oai/private-h2o-3/issues/440)\\n- Problems during Train/Test adaptation between Enum/Numeric [(private-#451)](https://github.com/h2oai/private-h2o-3/issues/451)\\n- DRF/GBM balance_classes=True throws unimplemented exception [(private-#454)](https://github.com/h2oai/private-h2o-3/issues/454) [(github)](https://github.com/h2oai/h2o-dev/commit/3a4f7ee3fdb159187b5ae1789d55752192d893e6)\\n- AUC reported on training data is 0, but should be 1 [(private-#457)](https://github.com/h2oai/private-h2o-3/issues/457) [(github)](https://github.com/h2oai/h2o-dev/commit/312558524749a0b28bf22ffd8c34ebcd6996b350)\\n- glm pyunit intermittent failure [(private-#477)](https://github.com/h2oai/private-h2o-3/issues/477)\\n- Inconsistency in GBM results:Gives different results even when run with the same set of params [(private-#480)](https://github.com/h2oai/private-h2o-3/issues/480)\\n- get rid of nfolds= param since it\\'s not supported in GLM yet [(github)](https://github.com/h2oai/h2o-dev/commit/8603ad35d4243ef598acadbfaa084c6852acd7ce)\\n- Fixed degrees of freedom (off by 1) in glm, added test. [(github)](https://github.com/h2oai/h2o-dev/commit/09e6d6f5222c40cb73f28c6df4e30d92b98f8361)\\n- GLM fix: fix filtering of rows with NAs and fix in sparse handling. [(github)](https://github.com/h2oai/h2o-dev/commit/5bad9b5c7bc2a3a4d4a2496ade7194a0438f17d9)\\n- Fix GLM job fail path to call Job.fail(). [(github)](https://github.com/h2oai/h2o-dev/commit/912663fb0e05b4670d014a0a4c7bff03410c467e)\\n- Full AUC computation, bug fixes [(github)](https://github.com/h2oai/h2o-dev/commit/9124cc321defb0b4defba7bef02cf387ff238c28)\\n- Fix ADMM for upper/lower bounds. (updated rho settings + update u-vector in ADMM for intercept) [(github)](https://github.com/h2oai/h2o-dev/commit/47a09ffe2271db050bd6d8042dfeaa40c4874b8a)\\n- Few glm fixes [(github)](https://github.com/h2oai/h2o-dev/commit/04a344ebede1f34b58e9aa82889bac1af9bd5f47)\\n- DL : KDD Algebra data set => Build DL model => ArrayIndexOutOfBoundsException [(#13688)](https://github.com/h2oai/h2o-3/issues/13688)\\n- GBm: Dev vs H2O for depth 5, minrow=10, on prostate, give different trees [(#13748)](https://github.com/h2oai/h2o-3/issues/13748)\\n- GBM param min_rows doesn\\'t throw exception for negative values [(#13689)](https://github.com/h2oai/h2o-3/issues/13689)\\n- GBM : Build GBM Model => Too many levels in response column! (java.lang.IllegalArgumentException) => Should display proper error message [(#13690)](https://github.com/h2oai/h2o-3/issues/13690)\\n- GBM:Got exception \\'class java.lang.AssertionError\\', with msg \\'Something is wrong with GBM trees since returned prediction is Infinity [(#13713)](https://github.com/h2oai/h2o-3/issues/13713)\\n\\n\\n##### API\\n\\n- Cannot adapt numeric response to factors made from numbers [(#13612)](https://github.com/h2oai/h2o-3/issues/13612)\\n- not specifying response\\\\_column gets NPE (deep learning build_model()) I think other algos might have same thing [(#13148)](https://github.com/h2oai/h2o-3/issues/13148)\\n- NPE response has null msg, exception\\\\_msg and dev\\\\_msg [(private-#455)](https://github.com/h2oai/private-h2o-3/issues/455)\\n- Flow :=> Save Flow => On Mac and Windows 8.1 => NodePersistentStorage failure while attempting to overwrite (?) a flow [(#13558)](https://github.com/h2oai/h2o-3/issues/13558) [(github)](https://github.com/h2oai/h2o-dev/commit/db710a4dc7dda4570f5b87cb9e386be6c76f001e)\\n- the can_build field in ModelBuilderSchema needs values[] to be set [(#13744)](https://github.com/h2oai/h2o-3/issues/13744)\\n- value field in the field metadata isn\\'t getting serialized as its native type [(#13745)](https://github.com/h2oai/h2o-3/issues/13745)\\n\\n\\n##### Python\\n\\n- python api asfactor() on -1/1 column issue [(private-#474)](https://github.com/h2oai/private-h2o-3/issues/474)\\n\\n\\n##### R\\n\\n- Rapids: Operations %/% and %% returns Illegal Argument Exception in R [(#13726)](https://github.com/h2oai/h2o-3/issues/13726)\\n- quantile: H2oR displays wrong quantile values when call the default quantile without specifying the probs [(#13680)](https://github.com/h2oai/h2o-3/issues/13680)[(github)](https://github.com/h2oai/h2o-dev/commit/9ef5e2befe08a5ff7ce13e8b4b39acf7171e8a1f)\\n- as.factor: If a user reruns as.factor on an already factor column, h2o should not show an exception [(#13614)](https://github.com/h2oai/h2o-3/issues/13614)\\n- as.factor works only on positive integers [(#13609)](https://github.com/h2oai/h2o-3/issues/13609) [(github)](https://github.com/h2oai/h2o-dev/commit/08f3acb62bec0f2c3808841d6b7f8d1382f616f0)\\n- H2O-R: model detail lists three mses, the first MSE slot does not contain any info about the model and hence, should be removed from the model details [(#13597)](https://github.com/h2oai/h2o-3/issues/13597) [(github)](https://github.com/h2oai/h2o-dev/commit/55f975d551432114a0088d19bd2397894410dd94)\\n- H2O-R: Strings: While slicing get Error From H2O: water.DException$DistributedException [(#13585)](https://github.com/h2oai/h2o-3/issues/13585)\\n- R: h2o.confusionMatrix should handle both models and model metric objects [(#13583)](https://github.com/h2oai/h2o-3/issues/13583)\\n- R: as.Date not functional with H2O objects [(#13576)](https://github.com/h2oai/h2o-3/issues/13576) [(github)](https://github.com/h2oai/h2o-dev/commit/f2f64b1ed29c8d7ab47252d84d8634240b3889d0)\\n- R: some apply functions don\\'t work on H2OFrame objects [(#13572)](https://github.com/h2oai/h2o-3/issues/13572) [(github)](https://github.com/h2oai/h2o-dev/commit/10f1245dbbc5ac36024e8ce51932dd991ff50688)\\n- h2o.confusionMatrices for multinomial does not work [(#13570)](https://github.com/h2oai/h2o-3/issues/13570)\\n- R: slicing issues [(#13567)](https://github.com/h2oai/h2o-3/issues/13567)\\n- R: length and is.factor don\\'t work in h2o.ddply [(#13566)](https://github.com/h2oai/h2o-3/issues/13566) [(github)](https://github.com/h2oai/h2o-dev/commit/bdc55a95a91af784a8b4497bbc8e4835fa1049bf)\\n- R: apply(hex, c(1,2), ...) doesn\\'t properly raise an error [(#13548)](https://github.com/h2oai/h2o-3/issues/13548) [(github)](https://github.com/h2oai/h2o-dev/commit/75ddf7f82b4acabe77d0928b66ea7a51dbc5a8b4)\\n- R: Slicing negative indices to negative indices fails [(#13547)](https://github.com/h2oai/h2o-3/issues/13547) [(github)](https://github.com/h2oai/h2o-dev/commit/bf6620f70a3f09a8a57d2da563188c342d67aeb7)\\n- h2o.ddply: doesn\\'t accept anonymous functions [(#13545)](https://github.com/h2oai/h2o-3/issues/13545) [(github)](https://github.com/h2oai/h2o-dev/commit/3c3c4e7134fe03e5a8a5cdd8530f59094264b7f3)\\n- ifelse() cannot return H2OFrames in R [(#13526)](https://github.com/h2oai/h2o-3/issues/13526)\\n- as.h2o loses track of headers [(#13524)](https://github.com/h2oai/h2o-3/issues/13524)\\n- H2O-R not showing meaningful error msg [(#13493)](https://github.com/h2oai/h2o-3/issues/13493)\\n- H2O.fail() had better fail [(#13462)](https://github.com/h2oai/h2o-3/issues/13462) [(github)](https://github.com/h2oai/h2o-dev/commit/16939a831a315c5f7ec221bc15fad5826fd4c677)\\n- fix issue in toEnum [(github)](https://github.com/h2oai/h2o-dev/commit/99fe517a00f54dea9ca4e64054c06a6e8cd1ea8c)\\n- fix colnames and new col creation [(github)](https://github.com/h2oai/h2o-dev/commit/61000a75eaa3b9a92dced1c66ecdce687cef64b2)\\n- R: h2o.init() is posting warning messages of an unhealthy cluster when the cluster is fine. [(#13724)](https://github.com/h2oai/h2o-3/issues/13724)\\n- h2o.split frame is failing [(#13541)](https://github.com/h2oai/h2o-3/issues/13541)\\n\\n\\n\\n\\n\\n##### System\\n\\n- key type failure should fail the request, not the cloud [(#13729)](https://github.com/h2oai/h2o-3/issues/13729) [(github)](https://github.com/h2oai/h2o-dev/commit/52ebdf0cd6d972acb15c8cf315e2d1105c5b1703)\\n- Parse => Import Medicare supplier file => Parse = > Illegal argument for field: column_names of schema: ParseV2: string and key arrays\\' values must be quoted, but the client sent: \" [(#13710)](https://github.com/h2oai/h2o-3/issues/13710)\\n- Overwriting a constant vector with strings fails [(#13693)](https://github.com/h2oai/h2o-3/issues/13693)\\n- H2O - gets stuck while calculating quantile,no error msg, just keeps running a job that normally takes less than a sec [(#13677)](https://github.com/h2oai/h2o-3/issues/13677)\\n- Summary and quantile on a column with all missing values should not throw an exception [(#13665)](https://github.com/h2oai/h2o-3/issues/13665) [(github)](https://github.com/h2oai/h2o-dev/commit/7acd14a7d6bbdfa5ab6a7c2e8c2987622b229603)\\n- View Logs => class java.lang.RuntimeException: java.lang.IllegalArgumentException: File /home2/hdp/yarn/usercache/neeraja/appcache/application_1427144101512_0039/h2ologs/h2o_172.16.2.185_54321-3-info.log does not exist [(#13592)](https://github.com/h2oai/h2o-3/issues/13592)\\n- Parse: After parsing Chicago crime dataset => Not able to build models or Get frames [(#15418)](https://github.com/h2oai/h2o-3/issues/15418)\\n- Parse: Numbers completely parsed wrong [(#13568)](https://github.com/h2oai/h2o-3/issues/13568)\\n- Flow: converting a column to enum while parsing does not work [(#13564)](https://github.com/h2oai/h2o-3/issues/13564)\\n- Parse: Fail gracefully when asked to parse a zip file with different files in it [(#13557)](https://github.com/h2oai/h2o-3/issues/13557)[(github)](https://github.com/h2oai/h2o-dev/commit/23a60d68e9d77fe07ae9d940b0ebb6636ef40ee3)\\n- toDataFrame doesn\\'t support sequence format schema (array, vectorUDT) [(#13449)](https://github.com/h2oai/h2o-3/issues/13449)\\n- Parse : Parsing random crap gives java.lang.ArrayIndexOutOfBoundsException: 13 [(#13420)](https://github.com/h2oai/h2o-3/issues/13420)\\n- The quote stripper for column names should report when the stripped chars are not the expected quotes [(#13416)](https://github.com/h2oai/h2o-3/issues/13416)\\n- import directory with large files,then Frames..really slow and disk grinds. Files are unparsed. Shouldn\\'t be grinding [(#13113)](https://github.com/h2oai/h2o-3/issues/13113)\\n- NodePersistentStorage gets wiped out when hadoop cluster is restarted [(private-#488)](https://github.com/h2oai/private-h2o-3/issues/488)\\n- h2o.exec won\\'t be supported [(github)](https://github.com/h2oai/h2o-dev/commit/81f685e5abb990d7f7669b137cfb07d7b01ea471)\\n- fixed import issue [(github)](https://github.com/h2oai/h2o-dev/commit/addf5b85b91b77366bca0a8c900ca2d308f29a09)\\n- fixed init param [(github)](https://github.com/h2oai/h2o-dev/commit/d459d1a7fb405f8a1f7b466caae99281feae370c)\\n- fix repeat as.factor NPE [(github)](https://github.com/h2oai/h2o-dev/commit/49fb24417ecfe26975fbff14bef084da50a034c7)\\n- startH2O set to False in init [(github)](https://github.com/h2oai/h2o-dev/commit/53ca9baf1bd70cd04b2ad03243eb9c7053300c52)\\n- hang on glm job removal [(#13717)](https://github.com/h2oai/h2o-3/issues/13717)\\n- Flow - changed column types need to be reflected in parsed data [(private-#484)](https://github.com/h2oai/private-h2o-3/issues/484)\\n- water.DException$DistributedException while running kmeans in multinode cluster [(#13682)](https://github.com/h2oai/h2o-3/issues/13682)\\n- Frame inspection prior to file parsing, corrupts parsing [(#13417)](https://github.com/h2oai/h2o-3/issues/13417)\\n\\n\\n\\n\\n\\n\\n##### Web UI\\n\\n- Flow, DL: Need better fail message if \"Autoencoder\" and \"use_all_factor_levels\" are both selected [(#13715)](https://github.com/h2oai/h2o-3/issues/13715)\\n- When select AUTO while building a gbm model get ERROR FETCHING INITIAL MODEL BUILDER STATE [(#13588)](https://github.com/h2oai/h2o-3/issues/13588)\\n- Flow : Build h2o-dev-0.1.17.1009 : Building GLM model gives java.lang.ArrayIndexOutOfBoundsException: [(#13215)](https://github.com/h2oai/h2o-3/issues/13215) [(github)](https://github.com/h2oai/h2o-dev/commit/fe3cdad806750f6add0fc4c03bee9e66d61c59fa)\\n- Flow:Summary on flow broken for a long time [(#13773)](https://github.com/h2oai/h2o-3/issues/13773)\\n\\n---\\n\\n###  Serre (0.2.1.1) - 3/18/15\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o-dev/rel-serre/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o-dev/rel-serre/1/index.html</a>\\n\\n\\n\\n\\n#### New Features\\n\\n\\n##### Algorithms\\n\\n- Naive Bayes in H2O-dev [(#13171)](https://github.com/h2oai/h2o-3/issues/13171)\\n- GLM model output, details from R [(private-#565)](https://github.com/h2oai/private-h2o-3/issues/565)\\n- Run GLM Regression from Flow (including LBFGS) [(private-#549)](https://github.com/h2oai/private-h2o-3/issues/549)\\n- PCA [(#13170)](https://github.com/h2oai/h2o-3/issues/13170)\\n- Port Random Forest to h2o-dev [(#13447)](https://github.com/h2oai/h2o-3/issues/13447)\\n- Enable DRF model output [(github)](https://github.com/h2oai/h2o-flow/commit/44ee1bf98dd69f33251a7a959b1000cc7f290427)\\n- Add DRF to Flow (Model Output) [(#13517)](https://github.com/h2oai/h2o-3/issues/13517)\\n- Grid for GBM [(github)](https://github.com/h2oai/h2o-dev/commit/ce96d2859aa86e4df393a13e00fbb7fcf603c166)\\n- Run Deep Learning Regression from Flow [(private-#550)](https://github.com/h2oai/private-h2o-3/issues/550)\\n\\n##### Python\\n\\n- Add Python wrapper for DRF [(#13518)](https://github.com/h2oai/h2o-3/issues/13518)\\n\\n\\n##### R\\n\\n- Add R wrapper for DRF [(#13514)](https://github.com/h2oai/h2o-3/issues/13514)\\n\\n\\n##### System\\n\\n- Include uploadFile [(#13296)](https://github.com/h2oai/h2o-3/issues/13296) [(github)](https://github.com/h2oai/h2o-flow/commit/3f8fb91cf6d81aefdb0ad6deee801084e0cf864f)\\n- Added -flow_dir to hadoop driver [(github)](https://github.com/h2oai/h2o-dev/commit/9883b4d98ae0056e88db449ce1ebd20394d191ac)\\n\\n\\n\\n##### Web UI\\n\\n- Add Flow packs [(private-#483)](https://github.com/h2oai/private-h2o-3/issues/483) [(#13261)](https://github.com/h2oai/h2o-3/issues/13261)\\n- Integrate H2O Help inside Help panel [(#13124)](https://github.com/h2oai/h2o-3/issues/13124) [(github)](https://github.com/h2oai/h2o-flow/commit/62e3c06e91bc0576e15516381bb59f31dbdf38ca)\\n- Add quick toggle button to show/hide the sidebar [(github)](https://github.com/h2oai/h2o-flow/commit/b5fb2b54a04850c9b24bb0eb03769cb519039de6)\\n- Add New, Open toolbar buttons [(github)](https://github.com/h2oai/h2o-flow/commit/b6efd33c9c8c2f5fe73e9ba83c1441d768ec47f7)\\n- Auto-refresh data preview when parse setup input parameters are changed [(#13516)](https://github.com/h2oai/h2o-3/issues/13516)\\n- Flow: Add playbar with Run, Continue, Pause, Progress controls [(private-#481)](https://github.com/h2oai/private-h2o-3/issues/481)\\n- You can now stop/cancel a running flow\\n\\n\\n#### Enhancements\\n\\n\\n##### Algorithms\\n\\n- Display GLM coefficients only if available [(#13458)](https://github.com/h2oai/h2o-3/issues/13458)\\n- Add random chance line to RoC chart [(private-#496)](https://github.com/h2oai/private-h2o-3/issues/496)\\n- Speed up DLSpiral test. Ignore Neurons test (MatVec) [(github)](https://github.com/h2oai/h2o-dev/commit/822862aa29fb63e52703ce91794a64e49bb96aed)\\n- Use getRNG for Dropout [(github)](https://github.com/h2oai/h2o-dev/commit/94a5b4e46a4501e85fb4889e5c8b196c46f74525)\\n- #13590: Add tests for determinism of RNGs [(github)](https://github.com/h2oai/h2o-dev/commit/e77c3ead2151a1202ec0b9c467641bc1c787e122)\\n- #13590: Implement Chi-Square test for RNGs [(github)](https://github.com/h2oai/h2o-dev/commit/690dd333c6bf51ff4e223cd15ef9dab004ed8904)\\n- Add DL model output toString() [(github)](https://github.com/h2oai/h2o-dev/commit/d206bb5b9996e87e8c0058dd8f1d7580d1ea0bb1)\\n- Add LogLoss to MultiNomial ModelMetrics [(#13573)](https://github.com/h2oai/h2o-3/issues/13573)\\n- Print number of categorical levels once we hit >1000 input neurons. [(github)](https://github.com/h2oai/h2o-dev/commit/ccf645af908d4964db3bc36a98c4ff9868838dc6)\\n- Updated the loss behavior for GBM. When loss is set to AUTO, if the response is an integer with 2 levels, then bernoullli (rather than gaussian) behavior is chosen. As a result, the `do_classification` flag is no longer necessary in Flow, since the loss completely specifies the desired behavior, and R users no longer to use `as.factor()` in their response to get the desired bernoulli behavior. The `score_each_iteration` flag has been removed as well. [(github)](https://github.com/h2oai/h2o-dev/commit/cc971e00869197625fefec894ab705c79db05fbb)\\n- Fully remove `_convert_to_enum` in all algos [(github)](https://github.com/h2oai/h2o-dev/commit/7fdf5d98c1f7caf88a3a928a28b2f86b06c5b2eb)\\n- Port MissingValueInserter EndPoint to h2o-dev. [(#13457)](https://github.com/h2oai/h2o-3/issues/13457)\\n\\n\\n\\n\\n\\n##### API\\n\\n- Display point layer for tree vs mse plots in GBM output [(#13551)](https://github.com/h2oai/h2o-3/issues/13551)\\n- Rename API inputs/outputs [(github)](https://github.com/h2oai/h2o-flow/commit/c7fc17afd3ff0a176e80d9d07d71c0bdd8f165eb)\\n- Rename Inf to Infinity [(github)](https://github.com/h2oai/h2o-flow/commit/ef5f5997d044dac9ab676b65174f09aa8785cfb6)\\n\\n\\n##### Python\\n\\n- added H2OFrame.setNames(), H2OFrame.cbind(), H2OVec.cbind(), h2o.cbind(), and pyunit_cbind.py [(github)](https://github.com/h2oai/h2o-dev/commit/84a3ea920f2ea9ee76985f7ccadb1e9d3f935025)\\n- Make H2OVec.levels() return the levels [(github)](https://github.com/h2oai/h2o-dev/commit/ab07275a55930b574407d8c4ea8e2b29cd6acd77)\\n- H2OFrame.dim(), H2OFrame.append(), H2OVec.setName(), H2OVec.isna() additions. demo pyunit addition [(github)](https://github.com/h2oai/h2o-dev/commit/41e6668ca05c59e614e54477a6082345366c75c8)\\n\\n\\n##### System\\n\\n- Customize H2O web UI port [(#13475)](https://github.com/h2oai/h2o-3/issues/13475)\\n- Make parse setup interactive [(#13516)](https://github.com/h2oai/h2o-3/issues/13516)\\n- Added --verbose [(github)](https://github.com/h2oai/h2o-dev/commit/5e772f8314a340666e4e80b3480b2105ceb91251)\\n- Adds some H2OParseExceptions. Removes all H2O.fail in parse (no parse issues should cause a fail)[(github)](https://github.com/h2oai/h2o-dev/commit/687b674d1dfb37f13542d15d1f04fe1b7c181f71)\\n- Allows parse to specify check_headers=HAS_HEADERS, but not provide column names [(github)](https://github.com/h2oai/h2o-dev/commit/ba48c0af1253d4bd6b05024991241fc6f7f8532a)\\n- Port MissingValueInserter EndPoint to h2o-dev [(#13457)](https://github.com/h2oai/h2o-3/issues/13457)\\n\\n\\n\\n##### Web UI\\n\\n- Add \\'Clear cell\\' and \\'Run all cells\\' toolbar buttons [(github)](https://github.com/h2oai/h2o-flow/commit/802b3a31ed8171a43cd1e566e5f77ba7fbf33549)\\n- Add \\'Clear cell\\' and \\'Clear all cells\\' commands [(#13484)](https://github.com/h2oai/h2o-3/issues/13484) [(github)](https://github.com/h2oai/h2o-flow/commit/2ecbe04325c865d0f5d8b2cb753ca15036ea2321)\\n- \\'Run\\' button selects next cell after running\\n- ModelMetrics by model category: Clustering [(#13303)](https://github.com/h2oai/h2o-3/issues/13303)\\n- ModelMetrics by model category: Regression [(#13302)](https://github.com/h2oai/h2o-3/issues/13302)\\n- ModelMetrics by model category: Multinomial [(#13301)](https://github.com/h2oai/h2o-3/issues/13301)\\n- ModelMetrics by model category: Binomial [(#13300)](https://github.com/h2oai/h2o-3/issues/13300)\\n- Add ability to select and delete multiple models [(github)](https://github.com/h2oai/h2o-flow/commit/8a9d033deba68292347c1e027b461a4c9ba7f1e5)\\n- Add ability to select and delete multiple frames [(github)](https://github.com/h2oai/h2o-flow/commit/6d5455b041f5af6b6213694ee1aae8d4e4d57d2b)\\n- Flows now stop running when an error occurs\\n- Print full number of mismatches during POJO comparison check. [(github)](https://github.com/h2oai/h2o-dev/commit/e8b599b59f2117083d2f7979cd1a0ca957a41605)\\n- Make Grid multi-node safe [(github)](https://github.com/h2oai/h2o-dev/commit/915cf0bd4fa589c6d819ba1eba85811e30f87399)\\n- Beautify the vertical axis labels for Flow charts/visualization (more) [(#13330)](https://github.com/h2oai/h2o-3/issues/13330)\\n\\n#### Bug Fixes\\n\\n##### Algorithms\\n\\n- GBM only populates either MSE_train or MSE_valid but displays both [(#13350)](https://github.com/h2oai/h2o-3/issues/13350)\\n- GBM: train error increases after hitting zero on prostate dataset [(#13555)](https://github.com/h2oai/h2o-3/issues/13555)\\n- GBM : Variable importance displays 0\\'s for response param => should not display response in table at all [(#13424)](https://github.com/h2oai/h2o-3/issues/13424)\\n- GLM : R/Flow ==> Build GLM Model hangs at 4% [(#13448)](https://github.com/h2oai/h2o-3/issues/13448)\\n- Import file from R hangs at 75% for 15M Rows/2.2 K Columns [(private-#601)](https://github.com/h2oai/private-h2o-3/issues/601)\\n- Flow: GLM - \\'model.output.coefficients_magnitude.name\\' not found, so can\\'t view model [(#13458)](https://github.com/h2oai/h2o-3/issues/13458)\\n- GBM predict fails without response column [(#13470)](https://github.com/h2oai/h2o-3/issues/13470)\\n- GBM: When validation set is provided, gbm should report both mse_valid and mse_train [(#13490)](https://github.com/h2oai/h2o-3/issues/13490)\\n- PCA Assertion Error during Model Metrics [(#13530)](https://github.com/h2oai/h2o-3/issues/13530) [(github)](https://github.com/h2oai/h2o-dev/commit/69690db57ed9951a57df83b2ce30be30a49ca507)\\n- KMeans: Size of clusters in Model Output is different from the labels generated on the training set [(#13525)](https://github.com/h2oai/h2o-3/issues/13525) [(github)](https://github.com/h2oai/h2o-dev/commit/6f8a857c8a060af0d2434cda91469ef8c23c86ae)\\n- Inconsistency in GBM results:Gives different results even when run with the same set of params [(private-#480)](https://github.com/h2oai/private-h2o-3/issues/480)\\n- #13573: Fix some numerical edge cases [(github)](https://github.com/h2oai/h2o-dev/commit/4affd9baa005c08d6b1669e462ec7bfb4de5ec69)\\n- Fix two missing float -> double conversion changes in tree scoring. [(github)](https://github.com/h2oai/h2o-dev/commit/b2cc99822db9b59766f3293e4dbbeeea547cd81e)\\n- Flow: HIDDEN_DROPOUT_RATIOS for DL does not show default value [(#13232)](https://github.com/h2oai/h2o-3/issues/13232)\\n- Old GLM Parameters Missing [(#13426)](https://github.com/h2oai/h2o-3/issues/13426)\\n- GLM: R/Flow ==> Build GLM Model hangs at 4% [(#13448)](https://github.com/h2oai/h2o-3/issues/13448)\\n\\n\\n\\n\\n\\n##### API\\n\\n- SplitFrame on String column produce C0LChunk instead of CStrChunk [(#13460)](https://github.com/h2oai/h2o-3/issues/13460)\\n-  Error in node$h2o$node : $ operator is invalid for atomic vectors [(#13348)](https://github.com/h2oai/h2o-3/issues/13348)\\n-  Response from /ModelBuilders don\\'t conform to standard error json shape when there are errors [(private-#536)](https://github.com/h2oai/private-h2o-3/issues/536) [(github)](https://github.com/h2oai/h2o-dev/commit/dadf385b3e3b2f68afe88096ecfd51e5bc9e01cb)\\n\\n##### Python\\n\\n- fix python syntax error [(github)](https://github.com/h2oai/h2o-dev/commit/a3c62f099088ac2206b83275ca096d4952f76e28)\\n- Fixes handling of None in python for a returned na_string. [(github)](https://github.com/h2oai/h2o-dev/commit/58c1af54b37909b8e9d06d23ed41fce4943eceb4)\\n\\n\\n\\n##### R\\n\\n- R : Inconsistency - Train set name with and without quotes work but Validation set name with quotes does not work [(#13482)](https://github.com/h2oai/h2o-3/issues/13482)\\n- h2o.confusionmatrices does not work [(#13559)](https://github.com/h2oai/h2o-3/issues/13559)\\n- How do i convert an enum column back to integer/double from R? [(#13529)](https://github.com/h2oai/h2o-3/issues/13529)\\n- Summary in R is faulty [(#13523)](https://github.com/h2oai/h2o-3/issues/13523)\\n- R: as.h2o should preserve R data types [(#13571)](https://github.com/h2oai/h2o-3/issues/13571)\\n- NPE in GBM Prediction with Sliced Test Data [(private-#472)](https://github.com/h2oai/private-h2o-3/issues/472) [(github)](https://github.com/h2oai/h2o-dev/commit/e605ab109488c7630223320fdd8bad486492050a)\\n- Import file from R hangs at 75% for 15M Rows/2.2 K Columns [(private-#601)](https://github.com/h2oai/private-h2o-3/issues/601)\\n- Custom Functions don\\'t work in apply() in R [(#13431)](https://github.com/h2oai/h2o-3/issues/13431)\\n- got water.DException$DistributedException and then got java.lang.RuntimeException: Categorical renumber task [(private-#479)](https://github.com/h2oai/private-h2o-3/issues/479)\\n- H2O-R: as.h2o parses column name as one of the row entries [(#13584)](https://github.com/h2oai/h2o-3/issues/13584)\\n- R-H2O Managing Memory in a loop [(#15604)](https://github.com/h2oai/h2o-3/issues/15604)\\n- h2o.confusionMatrices for multinomial does not work [(#13570)](https://github.com/h2oai/h2o-3/issues/13570)\\n- H2O-R not showing meaningful error msg\\n\\n\\n\\n\\n\\n##### System\\n\\n- Flow: When balance class = F then flow should not show max_after_balance_size = 5 in the parameter listing [(#13550)](https://github.com/h2oai/h2o-3/issues/13550)\\n- 3 jvms, doing ModelMetrics on prostate, class water.KeySnapshot$GlobalUKeySetTask; class java.lang.AssertionError: --- Attempting to block on task (class water.TaskGetKey) with equal or lower priority. Can lead to deadlock! 122 <=  122 [(#13486)](https://github.com/h2oai/h2o-3/issues/13486)\\n- Not able to start h2o on hadoop [(#13479)](https://github.com/h2oai/h2o-3/issues/13479)\\n- one row (one col) dataset seems to get assertion error in parse setup request [(#13111)](https://github.com/h2oai/h2o-3/issues/13111)\\n- Parse : Import file (move.com) => Parse => First row contains column names => column names not selected [(private-#540)](https://github.com/h2oai/private-h2o-3/issues/540) [(github)](https://github.com/h2oai/h2o-dev/commit/6f6d7023f9f2bafcb5461f46cf2825f233779f4a)\\n- The NY0 parse rule, in summary. Doesn\\'t look like it\\'s counting the 0\\'s as NAs like h2o [(#13166)](https://github.com/h2oai/h2o-3/issues/13166)\\n- 0 / Y / N parsing [(#13245)](https://github.com/h2oai/h2o-3/issues/13245)\\n- NodePersistentStorage gets wiped out when laptop is restarted. [(private-#497)](https://github.com/h2oai/private-h2o-3/issues/497)\\n- Building a model and making a prediction accepts invalid frame types [(#13097)](https://github.com/h2oai/h2o-3/issues/13097)\\n- Flow : Import file 15M rows 2.2 Cols => Parse => Error fetching job on UI =>Console : ERROR: Job was not successful Exiting with nonzero exit status [(private-#596)](https://github.com/h2oai/private-h2o-3/issues/596)\\n- Flow : Build GLM Model => Family tweedy => class hex.glm.LSMSolver$ADMMSolver$NonSPDMatrixException\\', with msg \\'Matrix is not SPD, can\\'t solve without regularization [(#13223)](https://github.com/h2oai/h2o-3/issues/13223)\\n- Flow : Import File : File doesn\\'t exist on all the hdfs nodes => Fails without valid message [(#13315)](https://github.com/h2oai/h2o-3/issues/13315)\\n- Check reproducibility on multi-node vs single-node [(#13538)](https://github.com/h2oai/h2o-3/issues/13538)\\n- Parse : After parsing Chicago crime dataset => Not able to build models or Get frames [(#15418)](https://github.com/h2oai/h2o-3/issues/15418)\\n\\n\\n\\n\\n\\n##### Web UI\\n\\n- Flow : Build Model => Parameters => shows meta text for some params [(#13552)](https://github.com/h2oai/h2o-3/issues/13552)\\n- Flow: K-Means - \"None\" option should not appear in \"Init\" parameters [(#13451)](https://github.com/h2oai/h2o-3/issues/13451)\\n- Flow: PCA - \"None\" option appears twice in \"Transform\" list [(private-#487)](https://github.com/h2oai/private-h2o-3/issues/487)\\n- GBM Model : Params in flow show two times [(#13435)](https://github.com/h2oai/h2o-3/issues/13435)\\n- Flow multinomial confusion matrix visualization [(private-#473)](https://github.com/h2oai/private-h2o-3/issues/473)\\n- Flow: It would be good if flow can report the actual distribution, instead of just reporting \"Auto\" in the model parameter listing [(#13554)](https://github.com/h2oai/h2o-3/issues/13554)\\n- Unimplemented algos should be taken out from drop down of build model [(#13497)](https://github.com/h2oai/h2o-3/issues/13497)\\n- [MapR] unable to give hdfs file name from Flow [(#13408)](https://github.com/h2oai/h2o-3/issues/13408)\\n\\n\\n\\n\\n\\n---\\n\\n### Selberg (0.2.0.1) - 3/6/15\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o-dev/rel-selberg/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o-dev/rel-selberg/1/index.html</a>\\n\\n#### New Features\\n\\n\\n##### Algorithms\\n\\n- Naive Bayes in H2O-dev [(#13171)](https://github.com/h2oai/h2o-3/issues/13171)\\n- GLM model output, details from R [(private-#565)](https://github.com/h2oai/private-h2o-3/issues/565)\\n- Run GLM Regression from Flow (including LBFGS) [(private-#549)](https://github.com/h2oai/private-h2o-3/issues/549)\\n- PCA [(#13170)](https://github.com/h2oai/h2o-3/issues/13170)\\n- Port Random Forest to h2o-dev [(#13447)](https://github.com/h2oai/h2o-3/issues/13447)\\n- Enable DRF model output [(github)](https://github.com/h2oai/h2o-flow/commit/44ee1bf98dd69f33251a7a959b1000cc7f290427)\\n- Add DRF to Flow (Model Output) [(#13517)](https://github.com/h2oai/h2o-3/issues/13517)\\n- Grid for GBM [(github)](https://github.com/h2oai/h2o-dev/commit/ce96d2859aa86e4df393a13e00fbb7fcf603c166)\\n- Run Deep Learning Regression from Flow [(private-#550)](https://github.com/h2oai/private-h2o-3/issues/550)\\n\\n##### Python\\n\\n- Add Python wrapper for DRF [(#13518)](https://github.com/h2oai/h2o-3/issues/13518)\\n\\n\\n##### R\\n\\n- Add R wrapper for DRF [(#13514)](https://github.com/h2oai/h2o-3/issues/13514)\\n\\n\\n\\n##### System\\n\\n- Include uploadFile [(#13296)](https://github.com/h2oai/h2o-3/issues/13296) [(github)](https://github.com/h2oai/h2o-flow/commit/3f8fb91cf6d81aefdb0ad6deee801084e0cf864f)\\n- Added -flow_dir to hadoop driver [(github)](https://github.com/h2oai/h2o-dev/commit/9883b4d98ae0056e88db449ce1ebd20394d191ac)\\n\\n\\n\\n##### Web UI\\n\\n- Add Flow packs [(private-#483)](https://github.com/h2oai/private-h2o-3/issues/483) [(#13261)](https://github.com/h2oai/h2o-3/issues/13261)\\n- Integrate H2O Help inside Help panel [(#13124)](https://github.com/h2oai/h2o-3/issues/13124) [(github)](https://github.com/h2oai/h2o-flow/commit/62e3c06e91bc0576e15516381bb59f31dbdf38ca)\\n- Add quick toggle button to show/hide the sidebar [(github)](https://github.com/h2oai/h2o-flow/commit/b5fb2b54a04850c9b24bb0eb03769cb519039de6)\\n- Add New, Open toolbar buttons [(github)](https://github.com/h2oai/h2o-flow/commit/b6efd33c9c8c2f5fe73e9ba83c1441d768ec47f7)\\n- Auto-refresh data preview when parse setup input parameters are changed [(#13516)](https://github.com/h2oai/h2o-3/issues/13516)\\n  -Flow: Add playbar with Run, Continue, Pause, Progress controls [(private-#481)](https://github.com/h2oai/private-h2o-3/issues/481)\\n- You can now stop/cancel a running flow\\n\\n\\n#### Enhancements\\n\\nThe following changes are improvements to existing features (which includes changed default values):\\n\\n##### Algorithms\\n\\n- Display GLM coefficients only if available [(#13458)](https://github.com/h2oai/h2o-3/issues/13458)\\n- Add random chance line to RoC chart [(private-#496)](https://github.com/h2oai/private-h2o-3/issues/496)\\n- Allow validation dataset for AutoEncoder [(#13574)](https://github.com/h2oai/h2o-3/issues/13574)\\n- Speed up DLSpiral test. Ignore Neurons test (MatVec) [(github)](https://github.com/h2oai/h2o-dev/commit/822862aa29fb63e52703ce91794a64e49bb96aed)\\n- Use getRNG for Dropout [(github)](https://github.com/h2oai/h2o-dev/commit/94a5b4e46a4501e85fb4889e5c8b196c46f74525)\\n- #13590: Add tests for determinism of RNGs [(github)](https://github.com/h2oai/h2o-dev/commit/e77c3ead2151a1202ec0b9c467641bc1c787e122)\\n- #13590: Implement Chi-Square test for RNGs [(github)](https://github.com/h2oai/h2o-dev/commit/690dd333c6bf51ff4e223cd15ef9dab004ed8904)\\n- #13573: Add log loss to binomial and multinomial model metric [(github)](https://github.com/h2oai/h2o-dev/commit/8982a0a1ba575bd5ca6ca3e854382e03146743cd)\\n- Add DL model output toString() [(github)](https://github.com/h2oai/h2o-dev/commit/d206bb5b9996e87e8c0058dd8f1d7580d1ea0bb1)\\n- Add LogLoss to MultiNomial ModelMetrics [(#13573)](https://github.com/h2oai/h2o-3/issues/13573)\\n- Port MissingValueInserter EndPoint to h2o-dev [(#13457)](https://github.com/h2oai/h2o-3/issues/13457)\\n- Print number of categorical levels once we hit >1000 input neurons. [(github)](https://github.com/h2oai/h2o-dev/commit/ccf645af908d4964db3bc36a98c4ff9868838dc6)\\n- Updated the loss behavior for GBM. When loss is set to AUTO, if the response is an integer with 2 levels, then bernoullli (rather than gaussian) behavior is chosen. As a result, the `do_classification` flag is no longer necessary in Flow, since the loss completely specifies the desired behavior, and R users no longer to use `as.factor()` in their response to get the desired bernoulli behavior. The `score_each_iteration` flag has been removed as well. [(github)](https://github.com/h2oai/h2o-dev/commit/cc971e00869197625fefec894ab705c79db05fbb)\\n- Fully remove `_convert_to_enum` in all algos [(github)](https://github.com/h2oai/h2o-dev/commit/7fdf5d98c1f7caf88a3a928a28b2f86b06c5b2eb)\\n- Add DL POJO scoring [(#13578)](https://github.com/h2oai/h2o-3/issues/13578)\\n\\n\\n\\n\\n\\n##### API\\n\\n- Display point layer for tree vs mse plots in GBM output [(#13551)](https://github.com/h2oai/h2o-3/issues/13551)\\n- Rename API inputs/outputs [(github)](https://github.com/h2oai/h2o-flow/commit/c7fc17afd3ff0a176e80d9d07d71c0bdd8f165eb)\\n- Rename Inf to Infinity [(github)](https://github.com/h2oai/h2o-flow/commit/ef5f5997d044dac9ab676b65174f09aa8785cfb6)\\n\\n\\n##### Python\\n\\n- added H2OFrame.setNames(), H2OFrame.cbind(), H2OVec.cbind(), h2o.cbind(), and pyunit_cbind.py [(github)](https://github.com/h2oai/h2o-dev/commit/84a3ea920f2ea9ee76985f7ccadb1e9d3f935025)\\n- Make H2OVec.levels() return the levels [(github)](https://github.com/h2oai/h2o-dev/commit/ab07275a55930b574407d8c4ea8e2b29cd6acd77)\\n- H2OFrame.dim(), H2OFrame.append(), H2OVec.setName(), H2OVec.isna() additions. demo pyunit addition [(github)](https://github.com/h2oai/h2o-dev/commit/41e6668ca05c59e614e54477a6082345366c75c8)\\n\\n\\n##### R\\n\\n- #13571, #13571, #13571.\\n  -R client now sends the data frame column names and data types to ParseSetup.\\n  -R client can get column names from a parsed frame or a list.\\n  -Respects client request for column data types [(github)](https://github.com/h2oai/h2o-dev/commit/ba063be25d3fbb658b016ff514083284e2d95d78)\\n\\n##### System\\n\\n- Customize H2O web UI port [(#13475)](https://github.com/h2oai/h2o-3/issues/13475)\\n- Make parse setup interactive [(#13516)](https://github.com/h2oai/h2o-3/issues/13516)\\n- Added --verbose [(github)](https://github.com/h2oai/h2o-dev/commit/5e772f8314a340666e4e80b3480b2105ceb91251)\\n- Adds some H2OParseExceptions. Removes all H2O.fail in parse (no parse issues should cause a fail)[(github)](https://github.com/h2oai/h2o-dev/commit/687b674d1dfb37f13542d15d1f04fe1b7c181f71)\\n- Allows parse to specify check_headers=HAS_HEADERS, but not provide column names [(github)](https://github.com/h2oai/h2o-dev/commit/ba48c0af1253d4bd6b05024991241fc6f7f8532a)\\n- Port MissingValueInserter EndPoint to h2o-dev [(#13457)](https://github.com/h2oai/h2o-3/issues/13457)\\n\\n\\n\\n##### Web UI\\n\\n- Add \\'Clear cell\\' and \\'Run all cells\\' toolbar buttons [(github)](https://github.com/h2oai/h2o-flow/commit/802b3a31ed8171a43cd1e566e5f77ba7fbf33549)\\n- Add \\'Clear cell\\' and \\'Clear all cells\\' commands [(#13484)](https://github.com/h2oai/h2o-3/issues/13484) [(github)](https://github.com/h2oai/h2o-flow/commit/2ecbe04325c865d0f5d8b2cb753ca15036ea2321)\\n- \\'Run\\' button selects next cell after running\\n- ModelMetrics by model category: Clustering [(#13303)](https://github.com/h2oai/h2o-3/issues/13303)\\n- ModelMetrics by model category: Regression [(#13302)](https://github.com/h2oai/h2o-3/issues/13302)\\n- ModelMetrics by model category: Multinomial [(#13301)](https://github.com/h2oai/h2o-3/issues/13301)\\n- ModelMetrics by model category: Binomial [(#13300)](https://github.com/h2oai/h2o-3/issues/13300)\\n- Add ability to select and delete multiple models [(github)](https://github.com/h2oai/h2o-flow/commit/8a9d033deba68292347c1e027b461a4c9ba7f1e5)\\n- Add ability to select and delete multiple frames [(github)](https://github.com/h2oai/h2o-flow/commit/6d5455b041f5af6b6213694ee1aae8d4e4d57d2b)\\n- Flows now stop running when an error occurs\\n- Print full number of mismatches during POJO comparison check. [(github)](https://github.com/h2oai/h2o-dev/commit/e8b599b59f2117083d2f7979cd1a0ca957a41605)\\n- Make Grid multi-node safe [(github)](https://github.com/h2oai/h2o-dev/commit/915cf0bd4fa589c6d819ba1eba85811e30f87399)\\n- Beautify the vertical axis labels for Flow charts/visualization (more) [(#13330)](https://github.com/h2oai/h2o-3/issues/13330)\\n\\n#### Bug Fixes\\n\\nThe following changes are to resolve incorrect software behavior:\\n\\n##### Algorithms\\n\\n- GBM only populates either MSE_train or MSE_valid but displays both [(#13350)](https://github.com/h2oai/h2o-3/issues/13350)\\n- GBM: train error increases after hitting zero on prostate dataset [(#13555)](https://github.com/h2oai/h2o-3/issues/13555)\\n- GBM : Variable importance displays 0\\'s for response param => should not display response in table at all [(#13424)](https://github.com/h2oai/h2o-3/issues/13424)\\n- Inconsistency in GBM results:Gives different results even when run with the same set of params [(private-#480)](https://github.com/h2oai/private-h2o-3/issues/480)\\n- GLM : R/Flow ==> Build GLM Model hangs at 4% [(#13448)](https://github.com/h2oai/h2o-3/issues/13448)\\n- Import file from R hangs at 75% for 15M Rows/2.2 K Columns [(private-#601)](https://github.com/h2oai/private-h2o-3/issues/601)\\n- Flow: GLM - \\'model.output.coefficients_magnitude.name\\' not found, so can\\'t view model [(#13458)](https://github.com/h2oai/h2o-3/issues/13458)\\n- GBM predict fails without response column [(#13470)](https://github.com/h2oai/h2o-3/issues/13470)\\n- GBM: When validation set is provided, gbm should report both mse_valid and mse_train [(#13490)](https://github.com/h2oai/h2o-3/issues/13490)\\n- PCA Assertion Error during Model Metrics [(#13530)](https://github.com/h2oai/h2o-3/issues/13530) [(github)](https://github.com/h2oai/h2o-dev/commit/69690db57ed9951a57df83b2ce30be30a49ca507)\\n- KMeans: Size of clusters in Model Output is different from the labels generated on the training set [(#13525)](https://github.com/h2oai/h2o-3/issues/13525) [(github)](https://github.com/h2oai/h2o-dev/commit/6f8a857c8a060af0d2434cda91469ef8c23c86ae)\\n- Inconsistency in GBM results:Gives different results even when run with the same set of params [(private-#480)](https://github.com/h2oai/private-h2o-3/issues/480)\\n- divide by zero in modelmetrics for deep learning [(#13546)](https://github.com/h2oai/h2o-3/issues/13546)\\n- AUC reported on training data is 0, but should be 1 [(private-#457)](https://github.com/h2oai/private-h2o-3/issues/457) [(github)](https://github.com/h2oai/h2o-dev/commit/312558524749a0b28bf22ffd8c34ebcd6996b350)\\n- GBM: reports 0th tree mse value for the validation set, different than the train set ,When only train sets is provided [(#13562)](https://github.com/h2oai/h2o-3/issues/13562)\\n- #13573: Fix some numerical edge cases [(github)](https://github.com/h2oai/h2o-dev/commit/4affd9baa005c08d6b1669e462ec7bfb4de5ec69)\\n- Fix two missing float -> double conversion changes in tree scoring. [(github)](https://github.com/h2oai/h2o-dev/commit/b2cc99822db9b59766f3293e4dbbeeea547cd81e)\\n- Problems during Train/Test adaptation between Enum/Numeric [(private-#451)](https://github.com/h2oai/private-h2o-3/issues/451)\\n- DRF/GBM balance_classes=True throws unimplemented exception [(private-#454)](https://github.com/h2oai/private-h2o-3/issues/454)\\n- Flow: HIDDEN_DROPOUT_RATIOS for DL does not show default value [(#13232)](https://github.com/h2oai/h2o-3/issues/13232)\\n- Old GLM Parameters Missing [(#13426)](https://github.com/h2oai/h2o-3/issues/13426)\\n- GLM: R/Flow ==> Build GLM Model hangs at 4% [(#13448)](https://github.com/h2oai/h2o-3/issues/13448)\\n- GBM: Initial mse in bernoulli seems to be off [(#13556)](https://github.com/h2oai/h2o-3/issues/13556)\\n\\n\\n\\n\\n##### API\\n\\n- SplitFrame on String column produce C0LChunk instead of CStrChunk [(#13460)](https://github.com/h2oai/h2o-3/issues/13460)\\n-  Error in node$h2o$node : $ operator is invalid for atomic vectors [(#13348)](https://github.com/h2oai/h2o-3/issues/13348)\\n-  Response from /ModelBuilders don\\'t conform to standard error json shape when there are errors [(private-#536)](https://github.com/h2oai/private-h2o-3/issues/536)\\n\\n##### Python\\n\\n- fix python syntax error [(github)](https://github.com/h2oai/h2o-dev/commit/a3c62f099088ac2206b83275ca096d4952f76e28)\\n- Fixes handling of None in python for a returned na_string. [(github)](https://github.com/h2oai/h2o-dev/commit/58c1af54b37909b8e9d06d23ed41fce4943eceb4)\\n\\n\\n##### R\\n\\n- R : Inconsistency - Train set name with and without quotes work but Validation set name with quotes does not work [(#13482)](https://github.com/h2oai/h2o-3/issues/13482)\\n- h2o.confusionmatrices does not work [(#13559)](https://github.com/h2oai/h2o-3/issues/13559)\\n- How do i convert an enum column back to integer/double from R? [(#13529)](https://github.com/h2oai/h2o-3/issues/13529)\\n- Summary in R is faulty [(#13523)](https://github.com/h2oai/h2o-3/issues/13523)\\n- Custom Functions don\\'t work in apply() in R [(#13431)](https://github.com/h2oai/h2o-3/issues/13431)\\n- R: as.h2o should preserve R data types [(#13571)](https://github.com/h2oai/h2o-3/issues/13571)\\n- as.h2o loses track of headers [(#13524)](https://github.com/h2oai/h2o-3/issues/13524)\\n- NPE in GBM Prediction with Sliced Test Data [(private-#472)](https://github.com/h2oai/private-h2o-3/issues/472) [(github)](https://github.com/h2oai/h2o-dev/commit/e605ab109488c7630223320fdd8bad486492050a)\\n- Import file from R hangs at 75% for 15M Rows/2.2 K Columns [(private-#601)](https://github.com/h2oai/private-h2o-3/issues/601)\\n- Custom Functions don\\'t work in apply() in R [(#13431)](https://github.com/h2oai/h2o-3/issues/13431)\\n- got water.DException$DistributedException and then got java.lang.RuntimeException: Categorical renumber task [(private-#479)](https://github.com/h2oai/private-h2o-3/issues/479)\\n- h2o.confusionMatrices for multinomial does not work [(#13570)](https://github.com/h2oai/h2o-3/issues/13570)\\n- R: h2o.confusionMatrix should handle both models and model metric objects [(#13583)](https://github.com/h2oai/h2o-3/issues/13583)\\n- H2O-R: as.h2o parses column name as one of the row entries [(#13584)](https://github.com/h2oai/h2o-3/issues/13584)\\n\\n\\n##### System\\n\\n- Flow: When balance class = F then flow should not show max_after_balance_size = 5 in the parameter listing [(#13550)](https://github.com/h2oai/h2o-3/issues/13550)\\n- 3 jvms, doing ModelMetrics on prostate, class water.KeySnapshot$GlobalUKeySetTask; class java.lang.AssertionError: --- Attempting to block on task (class water.TaskGetKey) with equal or lower priority. Can lead to deadlock! 122 <=  122 [(#13486)](https://github.com/h2oai/h2o-3/issues/13486)\\n- Not able to start h2o on hadoop [(#13479)](https://github.com/h2oai/h2o-3/issues/13479)\\n- one row (one col) dataset seems to get assertion error in parse setup request [(#13111)](https://github.com/h2oai/h2o-3/issues/13111)\\n- Parse : Import file (move.com) => Parse => First row contains column names => column names not selected [(private-#540)](https://github.com/h2oai/private-h2o-3/issues/540) [(github)](https://github.com/h2oai/h2o-dev/commit/6f6d7023f9f2bafcb5461f46cf2825f233779f4a)\\n- The NY0 parse rule, in summary. Doesn\\'t look like it\\'s counting the 0\\'s as NAs like h2o [(#13166)](https://github.com/h2oai/h2o-3/issues/13166)\\n- 0 / Y / N parsing [(#13245)](https://github.com/h2oai/h2o-3/issues/13245)\\n- NodePersistentStorage gets wiped out when laptop is restarted. [(private-#497)](https://github.com/h2oai/private-h2o-3/issues/497)\\n- Parse : Parsing random crap gives java.lang.ArrayIndexOutOfBoundsException: 13 [(#13420)](https://github.com/h2oai/h2o-3/issues/13420)\\n- Flow: converting a column to enum while parsing does not work [(#13564)](https://github.com/h2oai/h2o-3/issues/13564)\\n- Parse: Numbers completely parsed wrong [(#13568)](https://github.com/h2oai/h2o-3/issues/13568)\\n- NodePersistentStorage gets wiped out when hadoop cluster is restarted [(private-#488)](https://github.com/h2oai/private-h2o-3/issues/488)\\n- Parse: Fail gracefully when asked to parse a zip file with different files in it [(#13557)](https://github.com/h2oai/h2o-3/issues/13557)[(github)](https://github.com/h2oai/h2o-dev/commit/23a60d68e9d77fe07ae9d940b0ebb6636ef40ee3)\\n- Building a model and making a prediction accepts invalid frame types [(#13097)](https://github.com/h2oai/h2o-3/issues/13097)\\n- Flow : Import file 15M rows 2.2 Cols => Parse => Error fetching job on UI =>Console : ERROR: Job was not successful Exiting with nonzero exit status [(private-#596)](https://github.com/h2oai/private-h2o-3/issues/596)\\n- Flow : Build GLM Model => Family tweedy => class hex.glm.LSMSolver$ADMMSolver$NonSPDMatrixException\\', with msg \\'Matrix is not SPD, can\\'t solve without regularization [(#13223)](https://github.com/h2oai/h2o-3/issues/13223)\\n- Flow : Import File : File doesn\\'t exist on all the hdfs nodes => Fails without valid message [(#13315)](https://github.com/h2oai/h2o-3/issues/13315)\\n- Check reproducibility on multi-node vs single-node [(#13538)](https://github.com/h2oai/h2o-3/issues/13538)\\n- Parse: After parsing Chicago crime dataset => Not able to build models or Get frames [(#15418)](https://github.com/h2oai/h2o-3/issues/15418)\\n\\n##### Web UI\\n\\n- Flow : Build Model => Parameters => shows meta text for some params [(#13552)](https://github.com/h2oai/h2o-3/issues/13552)\\n- Flow: K-Means - \"None\" option should not appear in \"Init\" parameters [(#13451)](https://github.com/h2oai/h2o-3/issues/13451)\\n- Flow: PCA - \"None\" option appears twice in \"Transform\" list [(private-#487)](https://github.com/h2oai/private-h2o-3/issues/487)\\n- GBM Model : Params in flow show two times [(#13435)](https://github.com/h2oai/h2o-3/issues/13435)\\n- Flow multinomial confusion matrix visualization [(private-#473)](https://github.com/h2oai/private-h2o-3/issues/473)\\n- Flow: It would be good if flow can report the actual distribution, instead of just reporting \"Auto\" in the model parameter listing [(#13554)](https://github.com/h2oai/h2o-3/issues/13554)\\n- Unimplemented algos should be taken out from drop down of build model [(#13497)](https://github.com/h2oai/h2o-3/issues/13497)\\n- [MapR] unable to give hdfs file name from Flow [(#13408)](https://github.com/h2oai/h2o-3/issues/13408)\\n\\n---\\n\\n### Selberg (0.2.0.1) - 3/6/15\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o-dev/rel-selberg/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o-dev/rel-selberg/1/index.html</a>\\n\\n#### New Features\\n\\n##### Web UI\\n\\n- Flow: Delete functionality to be available for import files, jobs, models, frames [(#13256)](https://github.com/h2oai/h2o-3/issues/13256)\\n- Implement \"Download Flow\" [(#13406)](https://github.com/h2oai/h2o-3/issues/13406)\\n- Flow: Implement \"Run All Cells\" [(#13126)](https://github.com/h2oai/h2o-3/issues/13126)\\n\\n##### API\\n\\n- Create python package [(#13192)](https://github.com/h2oai/h2o-3/issues/13192)\\n- as.h2o in Python [(private-#580)](https://github.com/h2oai/private-h2o-3/issues/580)\\n\\n##### System\\n\\n- Add a README.txt to the hadoop zip files [(github)](https://github.com/h2oai/h2o-dev/commit/5a06ba8f0cfead3e30737d336f3c389ca0775b58)\\n- Build a cdh5.2 version of h2o [(github)](https://github.com/h2oai/h2o-dev/commit/eb8855d103e4f3aaf9dfa8c07d40d6c848141245)\\n\\n#### Enhancements\\n\\n##### Web UI\\n\\n- Flow: Job view should have info on start and end time [(#13281)](https://github.com/h2oai/h2o-3/issues/13281)\\n- Flow: Implement \\'File > Open\\' [(#13407)](https://github.com/h2oai/h2o-3/issues/13407)\\n- Display IP address in ADMIN -> Cluster Status [(private-#505)](https://github.com/h2oai/private-h2o-3/issues/505)\\n- Flow: Display alternate UI for splitFrames() [(#13398)](https://github.com/h2oai/h2o-3/issues/13398)\\n\\n\\n##### Algorithms\\n\\n- Added K-Means scoring [(github)](https://github.com/h2oai/h2o-dev/commit/220d2b40dc36dee6975a101e2eacb56a77861194)\\n- Flow: Implement model output for Deep Learning [(#13134)](https://github.com/h2oai/h2o-3/issues/13134)\\n- Flow: Implement model output for GLM [(#13136)](https://github.com/h2oai/h2o-3/issues/13136)\\n- Deep Learning model output [(private-#570, Flow)](https://github.com/h2oai/private-h2o-3/issues/570),[(private-#570, Python)](https://github.com/h2oai/private-h2o-3/issues/570),[(private-#570, R)](https://github.com/h2oai/private-h2o-3/issues/570)\\n- Run GLM Binomial from Flow (including LBFGS) [(private-#569)](https://github.com/h2oai/private-h2o-3/issues/569)\\n- Flow: Display confusion matrices for multinomial models [(#13396)](https://github.com/h2oai/h2o-3/issues/13396)\\n- During PCA, missing values in training data will be replaced with column mean [(github)](https://github.com/h2oai/h2o-dev/commit/166efad882162f7edc5cd8d4baa189476aa72d25)\\n- Update parameters for best model scan [(github)](https://github.com/h2oai/h2o-dev/commit/f183de392cb45adea7af43ffa53b095c3764602f)\\n- Change Quantiles to match h2o-1; both Quantiles and Rollups now have the same default percentiles [(github)](https://github.com/h2oai/h2o-dev/commit/51dc2c12a4281e3a2beeed8adfdfe4b14736fead)\\n- Massive cleanup and removal of old PCA, replacing with quadratically regularized PCA based on alternating minimization algorithm in GLRM [(github)](https://github.com/h2oai/h2o-dev/commit/02b7f168b2efa551a60c4bf2e95b8d506b613c2d)\\n- Add model run time to DL Model Output [(github)](https://github.com/h2oai/h2o-dev/commit/6730cc530b7b5376dfe6a2dd71817065e1edab7d)\\n- Don\\'t gather Neurons/Weights/Biases statistics [(github)](https://github.com/h2oai/h2o-dev/commit/aa1360d1bcfad3628d23211284878d80aa5a3b21)\\n- Only store best model if `override_with_best_model` is enabled [(github)](https://github.com/h2oai/h2o-dev/commit/5bd1e2327a09b649f251b251ff72af9aa8f4824c)\\n- `beta_eps` added, passing tests changed [(github)](https://github.com/h2oai/h2o-dev/commit/5e5acb6bdb89ff966151b0bc1ae20e96577d0368)\\n- For GLM, default values for `max_iters` parameter were changed from 1000 to 50.\\n- For quantiles, probabilities are displayed.\\n- Run Deep Learning Multinomial from Flow [(private-#551)](https://github.com/h2oai/private-h2o-3/issues/551)\\n\\n\\n\\n##### API\\n\\n- Expose DL weights/biases to clients via REST call [(#13344)](https://github.com/h2oai/h2o-3/issues/13344)\\n- Flow: Implement notification bar/API [(#13357)](https://github.com/h2oai/h2o-3/issues/13357)\\n- Variable importance data in REST output for GLM [(#13357)](https://github.com/h2oai/h2o-3/issues/13357)\\n- Add extra DL parameters to R API (`average_activation, sparsity_beta, max_categorical_features, reproducible`) [(github)](https://github.com/h2oai/h2o-dev/commit/8c7b860e29f297ff42ad6f45a1f138a8c6bb6b29)\\n- Update GLRM API model output [(github)](https://github.com/h2oai/h2o-dev/commit/653a9906003c2bab5e65d576420c76093fc92d12)\\n- h2o.anomaly missing in R [(#13429)](https://github.com/h2oai/h2o-3/issues/13429)\\n- No method to get enum levels [(#13427)](https://github.com/h2oai/h2o-3/issues/13427)\\n\\n\\n\\n##### System\\n\\n- Improve memory footprint with latest version of h2o-dev [(github)](https://github.com/h2oai/h2o-dev/commit/c54efaf41bc13677d5acd53a0496cca2b192baef)\\n- For now, let model.delete() of DL delete its best models too. This allows R code to not leak when only calling h2o.rm() on the main model. [(github)](https://github.com/h2oai/h2o-dev/commit/08b151a2bcbef8d56063b576638a6c0250379bd0)\\n- Bind both TCP and UDP ports before clustering [(github)](https://github.com/h2oai/h2o-dev/commit/d83c35841800b2abcc9d479fc74583d6ccdc714c)\\n- Round summary row#. Helps with pctiles for very small row counts. Add a test to check for getting close to the 50% percentile on small rows. [(github)](https://github.com/h2oai/h2o-dev/commit/7f4f7b159de0041894166f62d21e694dbd9c4c5d)\\n- Increase Max Value size in DKV to 256MB [(github)](https://github.com/h2oai/h2o-dev/commit/336b06e2a129509d424156653a2e7e4d5e972ed8)\\n- Flow: make parseRaw() do both import and parse in sequence [(private-#489)](https://github.com/h2oai/private-h2o-3/issues/489)\\n- Remove notion of individual job/job tracking from Flow [(#13441)](https://github.com/h2oai/h2o-3/issues/13441)\\n- Capability to name prediction results Frame in flow [(#13249)](https://github.com/h2oai/h2o-3/issues/13249)\\n\\n\\n\\n#### Bug Fixes\\n\\n##### Algorithms\\n\\n- GLM binomial prediction failing [(#13402)](https://github.com/h2oai/h2o-3/issues/13402)\\n- DL: Predict with auto encoder enabled gives Error processing error [(#13428)](https://github.com/h2oai/h2o-3/issues/13428)\\n- balance_classes in Deep Learning intermittent poor result [(#13432)](https://github.com/h2oai/h2o-3/issues/13432)\\n- Flow: Building GLM model fails [(#13197)](https://github.com/h2oai/h2o-3/issues/13197)\\n- summary returning incorrect 0.5 quantile for 5 row dataset [(#13110)](https://github.com/h2oai/h2o-3/issues/13110)\\n- GBM missing variable importance and balance-classes [(#13313)](https://github.com/h2oai/h2o-3/issues/13313)\\n- H2O Dev GBM first tree differs from H2O 1 [(#13413)](https://github.com/h2oai/h2o-3/issues/13413)\\n- get glm model from flow fails to find coefficient name field [(#13393)](https://github.com/h2oai/h2o-3/issues/13393)\\n- GBM/GLM build model fails on Hadoop after building 100% => Failed to find schema for version: 3 and type: GBMModel [(#13377)](https://github.com/h2oai/h2o-3/issues/13377)\\n- Parsing KDD wrong [(#13392)](https://github.com/h2oai/h2o-3/issues/13392)\\n- GLM AIOOBE [(#13213)](https://github.com/h2oai/h2o-3/issues/13213)\\n- Flow : Build GLM Model with family poisson => java.lang.ArrayIndexOutOfBoundsException: 1 at hex.glm.GLM$GLMLambdaTask.needLineSearch(GLM.java:359) [(#13222)](https://github.com/h2oai/h2o-3/issues/13222)\\n- Flow : GLM Model Error => Enum conversion only works on small integers [(#13365)](https://github.com/h2oai/h2o-3/issues/13365)\\n- GLM binary response, do_classfication=FALSE, family=binomial, prediction error [(#13339)](https://github.com/h2oai/h2o-3/issues/13339)\\n- Epsilon missing from GLM parameters [(#13354)](https://github.com/h2oai/h2o-3/issues/13354)\\n- GLM NPE [(#13394)](https://github.com/h2oai/h2o-3/issues/13394)\\n- Flow: GLM bug (or incorrect output) [(#13265)](https://github.com/h2oai/h2o-3/issues/13265)\\n- GLM binomial prediction failing [(#13402)](https://github.com/h2oai/h2o-3/issues/13402)\\n- GLM binomial on benign.csv gets assertion error in predict [(#13149)](https://github.com/h2oai/h2o-3/issues/13149)\\n- current summary default_pctiles doesn\\'t have 0.001 and 0.999 like h2o1 [(#13109)](https://github.com/h2oai/h2o-3/issues/13109)\\n- Flow: Build GBM/DL Model: java.lang.IllegalArgumentException: Enum conversion only works on integer columns [(#13225)](https://github.com/h2oai/h2o-3/issues/13225) [(github)](https://github.com/h2oai/h2o-dev/commit/57d6d96e4fed0a993bc8017f6e5eb1f60e9ceaa4)\\n- ModelMetrics on cup98VAL_z dataset has response with many nulls [(#13226)](https://github.com/h2oai/h2o-3/issues/13226)\\n- GBM : Predict model category output/inspect parameters shows as Regression when model is built with do classification enabled [(#13436)](https://github.com/h2oai/h2o-3/issues/13436)\\n- Fix double-precision DRF bugs [(github)](https://github.com/h2oai/h2o-dev/commit/cf7910e7bde1d8e3c1d91fadfcf37c5a74882145)\\n\\n##### System\\n\\n- Null columnTypes for /smalldata/arcene/arcene_train.data [(#13405)](https://github.com/h2oai/h2o-3/issues/13405) [(github)](https://github.com/h2oai/h2o-dev/commit/8511114a6ef6444938fb75e9ac9d5d7b7fe088d5)\\n- Flow: Waiting for -1 responses after starting h2o on hadoop cluster of 5 nodes [(#13411)](https://github.com/h2oai/h2o-3/issues/13411)\\n- Parse: airlines_all.csv => Airtime type shows as ENUM instead of Integer [(#13418)](https://github.com/h2oai/h2o-3/issues/13418) [(github)](https://github.com/h2oai/h2o-dev/commit/f6051de374b46376bf178064719fdd9b03e84dfa)\\n- Flow: Typo - \"Time\" option displays twice in column header type menu in Parse [(#13438)](https://github.com/h2oai/h2o-3/issues/13438)\\n- Duplicate validation messages in k-means output [(#13309)](https://github.com/h2oai/h2o-3/issues/13309) [(github)](https://github.com/h2oai/h2o-dev/commit/7905ba668572cb0eb518d791dc3262a2e8ff2fe0)\\n- Fixes Parse so that it returns to supplying generic column names when no column names exist [(github)](https://github.com/h2oai/h2o-dev/commit/d404bff2ef41e9a6e2d559c53c42225f11a81bff)\\n- Flow: Import File: File doesn\\'t exist on all the hdfs nodes => Fails without valid message [(#13315)](https://github.com/h2oai/h2o-3/issues/13315)\\n- Flow: Parse => 1m.svm hangs at 42% [(private-#514)](https://github.com/h2oai/private-h2o-3/issues/514)\\n- Prediction NFE [(#13312)](https://github.com/h2oai/h2o-3/issues/13312)\\n- NPE doing Frame to key before it\\'s fully parsed [(#13093)](https://github.com/h2oai/h2o-3/issues/13093)\\n- `h2o_master_DEV_gradle_build_J8` #351 hangs for past 17 hrs [(#15458)](https://github.com/h2oai/h2o-3/issues/15458)\\n- Sparkling water - container exited due to unavailable port [(#13345)](https://github.com/h2oai/h2o-3/issues/13345)\\n\\n\\n\\n##### API\\n\\n- Flow: Splitframe => java.lang.ArrayIndexOutOfBoundsException [(#13409)](https://github.com/h2oai/h2o-3/issues/13409) [(github)](https://github.com/h2oai/h2o-dev/commit/f5cf2888230df8904f0d87b8d97c31cc9cf26f79)\\n- Incorrect dest.type, description in /CreateFrame jobs [(#13403)](https://github.com/h2oai/h2o-3/issues/13403)\\n- space in windows filename on python [(#13423)](https://github.com/h2oai/h2o-3/issues/13423) [(github)](https://github.com/h2oai/h2o-dev/commit/c3a7f2f95ee41f5eb9bd9f4efd5b870af6cbc314)\\n- Python end-to-end data science example 1 runs correctly [(#13193)](https://github.com/h2oai/h2o-3/issues/13193)\\n- 3/NodePersistentStorage.json/foo/id should throw 404 instead of 500 for \\'not-found\\' [(private-#501)](https://github.com/h2oai/private-h2o-3/issues/501)\\n- POST /3/NodePersistentStorage.json should handle Content-Type:multipart/form-data [(private-#499)](https://github.com/h2oai/private-h2o-3/issues/499)\\n- by class water.KeySnapshot$GlobalUKeySetTask; class java.lang.AssertionError: --- Attempting to block on task (class water.TaskGetKey) with equal or lower priority. Can lead to deadlock! 122 <= 122 [(#13107)](https://github.com/h2oai/h2o-3/issues/13107)\\n- Sparkling water : val train:DataFrame = prostateRDD => Fails with ArrayIndexOutOfBoundsException [(#13391)](https://github.com/h2oai/h2o-3/issues/13391)\\n- Flow : getModels produces error: Error calling GET /3/Models.json [(#13267)](https://github.com/h2oai/h2o-3/issues/13267)\\n- Flow : Splitframe => java.lang.ArrayIndexOutOfBoundsException [(#13409)](https://github.com/h2oai/h2o-3/issues/13409)\\n- ddply \\'Could not find the operator\\' [(private-#503)](https://github.com/h2oai/private-h2o-3/issues/503) [(github)](https://github.com/h2oai/h2o-dev/commit/5f5dca9b9fc7d7d4888af0ab7ddad962f0381993)\\n- h2o.table AIOOBE during NewChunk creation [(private-#504)](https://github.com/h2oai/private-h2o-3/issues/504) [(github)](https://github.com/h2oai/h2o-dev/commit/338d654bd2a80ddf0fba8f65272b3ba07237d2eb)\\n- Fix warning in h2o.ddply when supplying multiple grouping columns [(github)](https://github.com/h2oai/h2o-dev/commit/1a7adb0a1f1bffe7bf77e5332f6291d4325d6a7f)\\n\\n\\n---\\n\\n\\n\\n### 0.1.26.1051 - 2/13/15\\n\\n#### New Features\\n\\n- Flow: Display alternate UI for splitFrames() [(#13398)](https://github.com/h2oai/h2o-3/issues/13398)\\n\\n\\n#### Enhancements\\n\\n##### System\\n\\n-  Embedded H2O config can now provide flat file (needed for Hadoop) [(github)](https://github.com/h2oai/h2o-dev/commit/62c344505b1c1c9154624fd9ca07d9b7217a9cfa)\\n- Don\\'t logging GET of individual jobs to avoid filling up the logs [(github)](https://github.com/h2oai/h2o-dev/commit/9d4a8249ceda49fcc64b5111a62c7a86076d7ec9)\\n\\n##### Algorithms\\n\\n-  Increase GBM/DRF factor binning back to historical levels. Had been capped accidentally at nbins (typically 20), was intended to support a much higher cap. [(github)](https://github.com/h2oai/h2o-dev/commit/4dac6ba640818bf5d482e6352a5e6aa62214ca4b)\\n-  Tweaked rho heuristic in glm [(github)](https://github.com/h2oai/h2o-dev/commit/7aec116974eb14ad6c7d7002a23d952a11339b79)\\n-  Enable variable importances for autoencoders [(github)](https://github.com/h2oai/h2o-dev/commit/19751e56c11f4ab672d47aabde84cf73271925dd)\\n-  Removed `group_split` option from GBM\\n-  Flow: display varimp for GBM output [(#13397)](https://github.com/h2oai/h2o-3/issues/13397)\\n-  variable importance for GBM [(github)](https://github.com/h2oai/h2o-dev/commit/f5085c3964d87d5349f406d1cfcc81fa0b34a27f)\\n-  GLM in H2O-Dev may provide slightly different coefficient values when applying an L1 penalty in comparison with H2O1.\\n\\n#### Bug Fixes\\n\\n##### Algorithms\\n\\n- Fixed bug in GLM exception handling causing GLM jobs to hang [(github)](https://github.com/h2oai/h2o-dev/commit/966a58f93d6cf746a2d6ec205d070247e4aeda01)\\n- Fixed a bug in kmeans input parameter schema where init was always being set to Furthest [(github)](https://github.com/h2oai/h2o-dev/commit/419754634ea30f6b9d9e24a2c62730a3a3b25042)\\n- Fixed mean computation in GLM [(github)](https://github.com/h2oai/h2o-dev/commit/74d9314a2b73812fa6dab03de9e8ea67c8a4693e)\\n- Fixed kmeans.R [(github)](https://github.com/h2oai/h2o-dev/commit/a532a0c850cd3c48b281bd34f83adac9108ac885)\\n- Flow: Building GBM model fails with Error executing javascript [(#13395)](https://github.com/h2oai/h2o-3/issues/13395)\\n\\n##### System\\n\\n- DataFrame propagates absolute path to parser [(github)](https://github.com/h2oai/h2o-dev/commit/0fad77b63512f2a20e20c93830e036a32a7643fe)\\n- Fix flow shutdown bug [(github)](https://github.com/h2oai/h2o-dev/commit/a26bd190dac59750131a2284bdf46e77ad12b67e)\\n\\n\\n---\\n\\n### 0.1.26.1032 - 2/6/15\\n\\n#### New Features\\n\\n##### General Improvements\\n\\n- better model output\\n- support for Python client\\n- support for Maven\\n- support for Sparkling Water\\n- support for REST API schema\\n- support for Hadoop CDH5 [(github)](https://github.com/h2oai/h2o-dev/commit/6a0feaebc9c7e253fe07b43dc383dfe4cbae2f29)\\n\\n\\n\\n##### UI\\n\\n- Display summary visualizations by default in column summary output cells [(#13341)](https://github.com/h2oai/h2o-3/issues/13341)\\n- Display AUC curve by default in binomial prediction output cells [(#13342)](https://github.com/h2oai/h2o-3/issues/13342)\\n- Flow: Implement About H2O/Flow with version information [(#13127)](https://github.com/h2oai/h2o-3/issues/13127)\\n- Add UI for CreateFrame [(#13235)](https://github.com/h2oai/h2o-3/issues/13235)\\n- Flow: Add ability to cancel running jobs [(#13372)](https://github.com/h2oai/h2o-3/issues/13372)\\n- Flow: warn when user navigates away while having unsaved content [(#13323)](https://github.com/h2oai/h2o-3/issues/13323)\\n\\n\\n\\n\\n\\n##### Algorithms\\n\\n- Implement splitFrame() in Flow [(#13356)](https://github.com/h2oai/h2o-3/issues/13356)\\n- Variable importance graph in Flow for GLM [(#13358)](https://github.com/h2oai/h2o-3/issues/13358)\\n- Flow: Implement model building form init and validation [(#13118)](https://github.com/h2oai/h2o-3/issues/13118)\\n- Added a shuffle-and-split-frame function; Use it to build a saner model on time-series data [(github)](https://github.com/h2oai/h2o-dev/commit/730c8d64316c913183a1271d1a2441f92fa11442)\\n- Added binomial model metrics [(github)](https://github.com/h2oai/h2o-dev/commit/2d124bea91474f3f55eb5e33f2494ae52ffba749)\\n- Run KMeans from R [(private-#554)](https://github.com/h2oai/private-h2o-3/issues/554)\\n- Be able to create a new GLM model from an existing one with updated coefficients [(private-#599)](https://github.com/h2oai/private-h2o-3/issues/599)\\n- Run KMeans from Python [(private-#553)](https://github.com/h2oai/private-h2o-3/issues/553)\\n- Run Deep Learning Binomial from Flow [(private-#576)](https://github.com/h2oai/private-h2o-3/issues/576)\\n- Run KMeans from Flow [(private-#555)](https://github.com/h2oai/private-h2o-3/issues/555)\\n- Run Deep Learning from Python [(private-#574)](https://github.com/h2oai/private-h2o-3/issues/574)\\n- Run Deep Learning from R [(private-#575)](https://github.com/h2oai/private-h2o-3/issues/575)\\n- Run Deep Learning Multinomial from Flow [(private-#551)](https://github.com/h2oai/private-h2o-3/issues/551)\\n- Run Deep Learning Regression from Flow [(private-#550)](https://github.com/h2oai/private-h2o-3/issues/550)\\n\\n\\n##### API\\n\\n- Flow: added REST API documentation to the web ui [(#13075)](https://github.com/h2oai/h2o-3/issues/13075)\\n- Flow: Implement visualization API [(#13130)](https://github.com/h2oai/h2o-3/issues/13130)\\n\\n\\n\\n##### System\\n\\n- Dataset inspection from Flow [(private-#586)](https://github.com/h2oai/private-h2o-3/issues/586)\\n- Basic data munging (Rapids) from R [(private-#582)](https://github.com/h2oai/private-h2o-3/issues/582)\\n- Implement stack operator/stacking in Lightning [(private-#531)](https://github.com/h2oai/private-h2o-3/issues/531)\\n\\n\\n\\n\\n\\n#### Enhancements\\n\\n\\n##### UI\\n\\n- Added better message when h2o.init() not yet called (`No active connection to an H2O cluster. Try calling \"h2o.init()\"`) [(github)](https://github.com/h2oai/h2o-dev/commit/b6bbbcee5972624cecc56099c0f95e1b2dd67253)\\n\\n\\n\\n##### Algorithms\\n\\n- Updated column-based gradient task to use sparse interface [(github)](https://github.com/h2oai/h2o-dev/commit/de5685b7c8e109cc39b671ef0bfd016516145d30)\\n- Updated LBFGS (added progress monitor interface, updated some default params), added progress and job support to GLM lbfgs [(github)](https://github.com/h2oai/h2o-dev/commit/6b89bb9201a89df93c4131b7ba10a7d17b45d72e)\\n- Added pretty print [(github)](https://github.com/h2oai/h2o-dev/commit/ebc824f9b081b61337c88e52b682bf42d9825c97)\\n- Added AutoEncoder to R model categories [(github)](https://github.com/h2oai/h2o-dev/commit/7030e7f1fb5779c026e0eed48662571f03f13428)\\n- Added Coefficients table to GLM model [(github)](https://github.com/h2oai/h2o-dev/commit/a432337d9d8b6480efbdaf0a0ebdb2ca3ad3f91a)\\n- Updated glm lbfgs to allow for efficient lambda-search (l2 penalty only) [(github)](https://github.com/h2oai/h2o-dev/commit/302ee73916516f2a25f98d96d9dd8fbff324dc5d)\\n- Removed splitframe shuffle parameter [(github)](https://github.com/h2oai/h2o-dev/commit/27f030721ae71006da7f0cc66be28337973f78f8)\\n- Simplified model builders and added deeplearning model builder [(github)](https://github.com/h2oai/h2o-dev/commit/302c819ea3d7b623af1968a181614d51d7dc68ed)\\n- Add DL model outputs to Flow [(#13371)](https://github.com/h2oai/h2o-3/issues/13371)\\n- Flow: Deep Learning: Expert Mode [(#13231)](https://github.com/h2oai/h2o-3/issues/13231)\\n- Flow: Display multinomial and regression DL model outputs [(#13382)](https://github.com/h2oai/h2o-3/issues/13382)\\n- Display varimp details for DL models [(#13380)](https://github.com/h2oai/h2o-3/issues/13380)\\n- Make binomial response \"0\" and \"1\" by default [(github)](https://github.com/h2oai/h2o-dev/commit/f597d4958ff2200f68e2cead31f3a184bfcaa5f2)\\n- Add Coefficients table to GLM model [(github)](https://github.com/h2oai/h2o-dev/commit/a432337d9d8b6480efbdaf0a0ebdb2ca3ad3f91a)\\n- Removed splitframe shuffle parameter [(github)](https://github.com/h2oai/h2o-dev/commit/27f030721ae71006da7f0cc66be28337973f78f8)\\n-  Update R GBM demos to reflect new input parameter names [(github)](https://github.com/h2oai/h2o-dev/commit/8cb99b5bf5ba828d08deba4647309824829a27a5)\\n-  Rename GLM variable importance to normalized coefficient magnitudes [(github)](https://github.com/h2oai/h2o-dev/commit/8cb99b5bf5ba828d08deba4647309824829a27a5)\\n\\n\\n\\n\\n##### API\\n\\n- Changed `key` to `destination_key` [(github)](https://github.com/h2oai/h2o-dev/commit/22067ae62a23af712d3081d981ae08756e6c071e)\\n- Cleaned up REST API schema interface [(github)](https://github.com/h2oai/h2o-dev/commit/ce581ec9fe670f43e8fb4aa955569cc9e92d013b)\\n- Changed method name, cleaned setup, added a pyunit runner [(github)](https://github.com/h2oai/h2o-dev/commit/26ea2c52440dd6ad8009c72bac8057d1edd9da0a)\\n\\n\\n\\n\\n\\n##### System\\n\\n- Allow changing column types during parse-setup [(#13375)](https://github.com/h2oai/h2o-3/issues/13375)\\n- Display %NAs in model builder column lists [(#13374)](https://github.com/h2oai/h2o-3/issues/13374)\\n- Figure out how to add H2O to PyPl [(#13191)](https://github.com/h2oai/h2o-3/issues/13191)\\n\\n\\n\\n\\n#### Bug Fixes\\n\\n\\n##### UI\\n\\n- Flow: Parse => 1m.svm hangs at 42% [(private-#514)](https://github.com/h2oai/private-h2o-3/issues/514)\\n- cup98 Dataset has columns that prevent validation/prediction [(#13349)](https://github.com/h2oai/h2o-3/issues/13349)\\n- Flow: predict step failed to function [(#13234)](https://github.com/h2oai/h2o-3/issues/13234)\\n- Flow: Arrays of numbers (ex. hidden in deeplearning)require brackets [(#13307)](https://github.com/h2oai/h2o-3/issues/13307)'}\n",
      "{'filename': 'h2o-3-master/Changes.md', 'section': '## H2O\\n\\n### 3.46.0.7 - 3/27/2025\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/7/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/7/index.html</a>\\n\\n#### Bug\\n- [[#16444]](https://github.com/h2oai/h2o-3/issues/16444) - Fix error in the uplift_drf_demo python notebook.\\n- [[#16466]](https://github.com/h2oai/h2o-3/issues/16466) - Fix invalid escape sequences in h2o-py docstrings.\\n- [[#16507]](https://github.com/h2oai/h2o-3/issues/16507) - Remove Hadoop hdp packages from the H2O-3 project because those are no longer available from the vendor.\\n\\n#### Security\\n- [[#16472]](https://github.com/h2oai/h2o-3/issues/16472), [[#16480]](https://github.com/h2oai/h2o-3/issues/16480) - Upgrade mina-core to fix CVE-2024-52046\\n\\n### 3.46.0.6 - 1/11/2024\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/6/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/6/index.html</a>\\n\\n#### Bug\\n- [[#16397]](https://github.com/h2oai/h2o-3/issues/16397) - Removed Sun license from the jps jar.\\n- [[#16382]](https://github.com/h2oai/h2o-3/issues/16382) - Fixed issues with constrained GLM.\\n- [[#16360]](https://github.com/h2oai/h2o-3/issues/16360) - Fixed H2O-3 R package for Windows not allowing the opening of one file by multiple processes.\\n- [[#16333]](https://github.com/h2oai/h2o-3/issues/16333) - Fixed pyplot warning from `learning_curve_plot` call.\\n\\n#### Improvement\\n- [[#15180]](https://github.com/h2oai/h2o-3/issues/15180) - Enabled users to adjust parquet imported timezone.\\n\\n#### New Feature\\n- [[#16361]](https://github.com/h2oai/h2o-3/issues/16361) - Enabled ability to display full PIDs in logs with `sys.ai.h2o.log.max.pid.length` call.\\n- [[#8487]](https://github.com/h2oai/h2o-3/issues/8487) - Implemented HGLM Gaussian as its own independent toolbox.\\n\\n#### Docs\\n- [[#16413]](https://github.com/h2oai/h2o-3/issues/16413) - Added the HGLM algorithm page and removed HGLM as a parameter.\\n- [[#16412]](https://github.com/h2oai/h2o-3/issues/16412) - Added `numpy` requirements to welcome page.\\n- [[#16384]](https://github.com/h2oai/h2o-3/issues/16384) - Fixed broken links throughout the user guide.\\n- [[#16338]](https://github.com/h2oai/h2o-3/issues/16338) - Clarified the `group_by` documentation by expanding the examples.\\n- [[#16208]](https://github.com/h2oai/h2o-3/issues/16208) - Added documentation on constrained GLM.\\n- [[#16182]](https://github.com/h2oai/h2o-3/issues/16182) - Updated the Welcome page to adhere to style guide requirements and broke it up into multiple smaller getting started pages.\\n- [[#15983]](https://github.com/h2oai/h2o-3/issues/15983) - Added examples to Python documentation for Rulefit.\\n\\n#### Security\\n- [[#16425]](https://github.com/h2oai/h2o-3/issues/16425) - Addressed CVE-2024-8862 by adding JDBC parameter validation.\\n- [[#16416]](https://github.com/h2oai/h2o-3/issues/16416) - Addressed CVE-2024-47561 by upgrading avro:avro library from 1.11.3 to 1.11.4.\\n- [[#16391]](https://github.com/h2oai/h2o-3/issues/16391) - Addressed sonatype-2024-3350 by using compatible versions of Apache commons-collections packages.\\n- [[#16351]](https://github.com/h2oai/h2o-3/issues/16351) - Addressed CVE-2024-5979 which caused AstRunTool to crash H2O-3 if bad inputs were provided by not calling `System.exit` from `water.tools`.\\n\\n### 3.46.0.5 - 8/28/2024\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/5/index.html</a>\\n\\n#### Bug\\n- [[#16328]](https://github.com/h2oai/h2o-3/issues/16328) - Updated how ModelSelection handles categorical predictors to preserve the best categorical predictor when the best categorical level performs well relative to other predictors.\\n- [[#16120]](https://github.com/h2oai/h2o-3/issues/16120) - Resolved that MOJO is working for Isolation Forest and Extended Isolation forest for implemented versions.\\n\\n#### New Feature\\n- [[#16327]](https://github.com/h2oai/h2o-3/issues/16327) - Ensured H2O-3 can load data from Snowflake using JDBC connector.\\n\\n#### Docs\\n- [[#16215]](https://github.com/h2oai/h2o-3/issues/16215) - Updated the following user guide pages to adhere to style guide updates: Algorithms, Supported data types, Quantiles, and Early stopping.\\n- [[#16207]](https://github.com/h2oai/h2o-3/issues/16207) - Updated the Starting H2O user guide page to adhere to style guide updates.\\n- [[#15989]](https://github.com/h2oai/h2o-3/issues/15989) - Updated Python documentation for Decision Tree algorithm.\\n\\n#### Security\\n- [[#16349]](https://github.com/h2oai/h2o-3/issues/16349) - Addressed sonatype-2024-0171 by upgrading jackson-databind to 2.17.2.\\n- [[#16342]](https://github.com/h2oai/h2o-3/issues/16342) - Addressed SNYK-JAVA-DNSJAVA-7547403, SNYK-JAVA-DNSJAVA-7547404, SNYK-JAVA-DNSJAVA-7547405, and CVE-2024-25638 by upgrading dnsjava to 3.6.0. \\n\\n### 3.46.0.4 - 7/9/2024\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/4/index.html</a>\\n\\n#### Docs\\n- [[#16212]](https://github.com/h2oai/h2o-3/issues/16212) - Updating user guide - H2O Clients.\\n- [[#16214]](https://github.com/h2oai/h2o-3/issues/16214) - Updating user guide - Data Manipulation.\\n- [[#16213]](https://github.com/h2oai/h2o-3/issues/16213) - Updating user guide - Getting data into your H2O cluster.\\n\\n#### Security\\n- [[#15748]](https://github.com/h2oai/h2o-3/issues/15748) - Addressed PRISMA-2023-0067 by upgrading jackson-databind.\\n\\n### 3.46.0.3 - 6/11/2024\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/3/index.html</a>\\n\\n#### Bug Fix\\n- [[#16274]](https://github.com/h2oai/h2o-3/issues/16274) - Fixed plotting for H2O Explainabilty by resolving issue in the matplotlib wrapper.\\n- [[#16192]](https://github.com/h2oai/h2o-3/issues/16192) - Fixed `h2o.findSynonyms` failing if the `word` parameter is unknown to the Word2Vec model.\\n- [[#15947]](https://github.com/h2oai/h2o-3/issues/15947) - Fixed `skipped_columns` error caused by mismatch during the call to `parse_setup` when constructing an `H2OFrame`.\\n\\n#### Improvement\\n- [[#16278]](https://github.com/h2oai/h2o-3/issues/16278) - Added flag to enable `use_multi_thread` automatically when using `as_data_frame`.\\n\\n#### New Feature\\n- [[#16284]](https://github.com/h2oai/h2o-3/issues/16284) - Added support for Websockets to steam.jar.\\n\\n#### Docs\\n- [[#16189]](https://github.com/h2oai/h2o-3/issues/16189) - Updating user guide - Downloading & Installing H2O.\\n- [[#16288]](https://github.com/h2oai/h2o-3/issues/16288) - Fixed GBM Python example in user guide.\\n- [[#16188]](https://github.com/h2oai/h2o-3/issues/16188) - Updated API-related changes page to adhere to style guide requirements.\\n- [[#16016]](https://github.com/h2oai/h2o-3/issues/16016) - Added examples to Python documentation for Uplift DRF.\\n- [[#15988]](https://github.com/h2oai/h2o-3/issues/15988) - Added examples to Python documentation for Isotonic Regression.\\n\\n### 3.46.0.2 - 5/13/2024\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/2/index.html</a>\\n\\n#### Bug Fix\\n- [[#16161]](https://github.com/h2oai/h2o-3/issues/16161) - Fixed parquet export throwing NPEs when column types are strings. \\n- [[#16149]](https://github.com/h2oai/h2o-3/issues/16149) - Fixed GAM models failing with datasets of certain size by rebalancing the dataset to avoid collision.\\n- [[#16130]](https://github.com/h2oai/h2o-3/issues/16130) - Removed `distutils` version check to stop deprecation warnings with Python 3.12.\\n- [[#16026]](https://github.com/h2oai/h2o-3/issues/16026) - Removed `custom_metric_func` from ModelSelection.\\n- [[#15697]](https://github.com/h2oai/h2o-3/issues/15697) - Fixed MOJO failing to recognize `fold_column` and therefore  using wrong index calculated for the `offset_column`.\\n\\n#### Improvement\\n- [[#16116]](https://github.com/h2oai/h2o-3/issues/16116) - Implemented a warning if you want to use monotone splines for GAM but dont set `non_negative=True` that you will not get a monotone output.\\n- [[#16056]](https://github.com/h2oai/h2o-3/issues/16066) - Added support to XGBoost for all `gblinear` parameters.\\n- [[#6722]](https://github.com/h2oai/h2o-3/issues/6722) - Implemented linear constraint support to GLM toolbox. \\n\\n#### New Feature\\n- [[#16146]](https://github.com/h2oai/h2o-3/issues/16146) - Added ZSTD compression format support. \\n\\n#### Docs\\n- [[#16193]](https://github.com/h2oai/h2o-3/issues/16193) - Added mapr7.0 to the download page for the Install on Hadoop tab.\\n- [[#16180]](https://github.com/h2oai/h2o-3/issues/16180) - Updated Index page to adhere to style guide requirements.\\n- [[#16131]](https://github.com/h2oai/h2o-3/issues/16131) - Added 3.46 release blog to the user guide.\\n\\n#### Security\\n- [[#16170]](https://github.com/h2oai/h2o-3/issues/16170) - Addressed CVE-2024-21634 by upgrading aws-java-sdk-*.\\n- [[#16135]](https://github.com/h2oai/h2o-3/issues/16135) - Addressed CVE-2024-29131 by upgrading commons-configuration2.\\n\\n### 3.46.0.1 - 3/13/2024\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.46.0/1/index.html</a>\\n\\n#### Bug Fix\\n- [[#16079]](https://github.com/h2oai/h2o-3/issues/16079) - Updated warning for multithreading in `H2OFrame.as_data_frame`.\\n- [[#16063]](https://github.com/h2oai/h2o-3/issues/16063) - Added error to explain method explaining incompatibility with UpliftDRF models.\\n- [[#16052]](https://github.com/h2oai/h2o-3/issues/16052) - Fixed finding best split point for UpliftDRF.\\n- [[#16043]](https://github.com/h2oai/h2o-3/issues/16043) - Fixed `isin()`.\\n- [[#16036]](https://github.com/h2oai/h2o-3/issues/16036) - Fixed `AstMatch` failing with multinode.\\n- [[#15978]](https://github.com/h2oai/h2o-3/issues/15978) - Fixed Deep Learning Autoencoder MOJO PredictCSV failure. \\n- [[#15682]](https://github.com/h2oai/h2o-3/issues/15682) - Fixed log when `web_ip` is used.\\n- [[#15677]](https://github.com/h2oai/h2o-3/issues/15677) - Fixed match function only returning 1 and `no match`. \\n\\n#### Improvement\\n- [[#16074]](https://github.com/h2oai/h2o-3/issues/16074) - Improved `perRow` metric calculation by implementing `isGeneric()` method.\\n- [[#16060]](https://github.com/h2oai/h2o-3/issues/16060) - Improved log message to show that Apple silicon is not supported. \\n- [[#16033]](https://github.com/h2oai/h2o-3/issues/16033) - Added optional GBLinear grid step to AutoML.\\n- [[#16015]](https://github.com/h2oai/h2o-3/issues/16015) - Suppressed the genmodel warnings when `verbose=False`.\\n- [[#15809]](https://github.com/h2oai/h2o-3/issues/15809) - Implemented ability to calculate full loglikelihood and AIC for an already-built GLM model. \\n- [[#15791]](https://github.com/h2oai/h2o-3/issues/15791) - Implemented early stopping for UpliftDRF and implemented gridable parameters for UpliftDRF.\\n- [[#15684]](https://github.com/h2oai/h2o-3/issues/15684) - Reconfigured all logs to standard error for level `ERROR` and `FATAL`.\\n- [[#7325]](https://github.com/h2oai/h2o-3/issues/7325) - Implemented prediction consistency check for constrained models. \\n\\n#### New Feature\\n- [[#15993]](https://github.com/h2oai/h2o-3/issues/15993) - Added `custom_metric` as a hyperparameter for grid search.\\n- [[#15967]](https://github.com/h2oai/h2o-3/issues/15967) - Added custom metrics for XGBoost.\\n- [[#15858]](https://github.com/h2oai/h2o-3/issues/15858) - Implemented consistent mechanism that protects frames and their vecs from autodeletion.\\n- [[#15683]](https://github.com/h2oai/h2o-3/issues/15683) - Introduced a warning if `web_ip` is not specified that H2O Rest API is listening on all interfaces.\\n- [[#15654]](https://github.com/h2oai/h2o-3/issues/15654) - Introduced MLFlow flavors for working with H2O-3 MOJOs and POJOs instead of binary models.\\n- [[#6573]](https://github.com/h2oai/h2o-3/issues/6573) - Implemented machine learning interpretability support for UpliftDRF by allowing Uplift models to access partial dependences plots and variable importance.\\n\\n#### Docs\\n- [[#16004]](https://github.com/h2oai/h2o-3/issues/16004) - Updated copyright year in user guide and Python guide.\\n- [[#16000]](https://github.com/h2oai/h2o-3/issues/16000) - Fixed Decision Tree Python example.\\n- [[#15930]](https://github.com/h2oai/h2o-3/issues/15930) - Fixed GLM Python example.\\n- [[#15915]](https://github.com/h2oai/h2o-3/issues/15915) - Added examples to Python documentation for Model Selection algorithm.\\n- [[#15798]](https://github.com/h2oai/h2o-3/issues/15798) - Added examples to Python documentation for GAM algorithm.\\n- [[#15709]](https://github.com/h2oai/h2o-3/issues/15709) - Added examples to Python documentation for ANOVA GLM algorithm.\\n\\n#### Security\\n- [[#16102]](https://github.com/h2oai/h2o-3/issues/16102) - Addressed SNYK-JAVA-COMNIMBUSDS-6247633 by upgrading nimbus-jose-jwt to 9.37.2.\\n- [[#16093]](https://github.com/h2oai/h2o-3/issues/16093) - Addressed CVE-2024-26308 by upgrading org.apache.commons:commons-compress.\\n- [[#16067]](https://github.com/h2oai/h2o-3/issues/16067) - Addressed CVE-2023-35116 in the h2o-steam.jar. \\n- [[#15972]](https://github.com/h2oai/h2o-3/issues/15972) - Addressed CVE-2023-6038 by adding option to filter file system for reading and writing.\\n- [[#15971]](https://github.com/h2oai/h2o-3/issues/15971) - Addressed CVE-2023-6016 by introducing Java property that disables automatic import of POJOs during `import_mojo` or `upload_mojo`.\\n\\n### 3.44.0.3 - 12/20/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.44.0/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.44.0/3/index.html</a>\\n\\n#### Bug Fix\\n- [[#15958]](https://github.com/h2oai/h2o-3/issues/15958) - Fixed maximum likelihood dispersion estimation for GLM tweedie family producing the wrong result for a specific dataset.\\n- [[#15936]](https://github.com/h2oai/h2o-3/issues/15936) - Added data frame transformations using polars since datatable cannot be installed on Python 3.10+. \\n- [[#15894]](https://github.com/h2oai/h2o-3/issues/15894) - Ensured that the functions that are supposed to be exported in the R package are exported.\\n- [[#15891]](https://github.com/h2oai/h2o-3/issues/15891) - Corrected sign in AIC calculation to fix problem with tweedie dispersion parameter estimation, AIC, and loglikelihood.\\n- [[#15887]](https://github.com/h2oai/h2o-3/issues/15887) - Allowed Python H2OFrame constructor to accept an existing H2OFrame.\\n- [[#6725]](https://github.com/h2oai/h2o-3/issues/6725) - Fixed LoggerFactory slf4j related regression. \\n\\n#### Improvement\\n- [[#15937]](https://github.com/h2oai/h2o-3/issues/15937) - Exposed `gainslift_bins` parameter for Deep Learning, GAM, GLM, and Stacked Ensemble algorithms.\\n- [[#15916]](https://github.com/h2oai/h2o-3/issues/15916) - Sped up computation of Friedman-Popescus H statistic.\\n\\n#### New Feature\\n- [[#15927]](https://github.com/h2oai/h2o-3/issues/15927) - Added anomaly score metric to be used as a `sort_by` metric when sorting grid model performances for Isolation Forest with grid search.\\n- [[#15780]](https://github.com/h2oai/h2o-3/issues/15780) - Added `weak_learner_params` parameter for AdaBoost.\\n- [[#15779]](https://github.com/h2oai/h2o-3/issues/15779) - Added `weak_learner=\"deep_learning\"` option for AdaBoost.\\n- [[#7118]](https://github.com/h2oai/h2o-3/issues/7118) - Implemented scoring and scoring history for Extended Isolation Forest by adding `score_each_iteration` and `score_tree_interval`. \\n\\n#### Docs\\n- [[#15817]](https://github.com/h2oai/h2o-3/issues/15817) - Improved default threshold API and documentation for binomial classification.\\n\\n#### Security\\n- [[#15754]](https://github.com/h2oai/h2o-3/issues/15754) - Addressed CVE-2022-21230 by replacing nanohttpd.\\n\\n### 3.44.0.2 - 11/8/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.44.0/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.44.0/2/index.html</a>\\n\\n#### Bug Fix\\n- [[#15906]](https://github.com/h2oai/h2o-3/issues/15906) - Fixed `learning_curve_plot` for CoxPH with specified metric = \\'loglik\\'.\\n- [[#15889]](https://github.com/h2oai/h2o-3/issues/15889) - Fixed inability to call `thresholds_and_metric_scores()` with binomial models and metrics.\\n- [[#15861]](https://github.com/h2oai/h2o-3/issues/15861) - Fixed the warning message that caused `as_data_frame` to fail due to not having datatable installed. \\n- [[#15860]](https://github.com/h2oai/h2o-3/issues/15860) - Fixed `force_col_type` not working with `skipped_columns` when parsing parquet files.\\n- [[#15832]](https://github.com/h2oai/h2o-3/issues/15832) - Fixed UpliftDRF MOJO API and updated the documentation. \\n- [[#15761]](https://github.com/h2oai/h2o-3/issues/15761) - Fixed `relevel_by_frequency` resetting the values of the column.\\n\\n#### Improvement\\n- [[#15893]](https://github.com/h2oai/h2o-3/issues/15893) - Renamed the `data` parameter of the `partial_plot` function to `frame`.\\n\\n#### Docs\\n- [[#15881]](https://github.com/h2oai/h2o-3/issues/15881) - Added security note that Kubernetes images dont apply security settings by default.\\n- [[#15851]](https://github.com/h2oai/h2o-3/issues/15851) - Added the 3.44 major release blog to the user guide.\\n- [[#15842]](https://github.com/h2oai/h2o-3/issues/15842) - Introduced *Known Bug* section to the release notes. \\n- [[#15840]](https://github.com/h2oai/h2o-3/issues/15840) -  Fixed the release notes UI not loading by making them smaller by putting all release notes prior to 3.28.0.1 into a separate file.\\n- [[#6570]](https://github.com/h2oai/h2o-3/issues/6570) - Added information on the Friedman and Popescu H Statistic to XGBoost and GBM.\\n\\n#### Security\\n- [[#15865]](https://github.com/h2oai/h2o-3/issues/15865) - Upgraded org.python.jython to CWE-416 of com.github.jnr:jnr-posix.\\n\\n### 3.44.0.1 - 10/16/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.44.0/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.44.0/1/index.html</a>\\n\\n\\n#### Bug Fix\\n- [[#15743]](https://github.com/h2oai/h2o-3/issues/15743) - Fixed`shap_summary_plot` for H2O Explainability Interface failing when one column was full of zeroes or NaN values.\\n- [[#15669]](https://github.com/h2oai/h2o-3/issues/15669) - Fixed R package to ensure it downloads the fixed version of H2O.\\n- [[#15651]](https://github.com/h2oai/h2o-3/issues/15651) - Upgraded the minimal supported version of `ggplot2` to 3.3.0 to remove the deprecated dot-dot notation.\\n\\n#### Improvement\\n- [[#15801]](https://github.com/h2oai/h2o-3/issues/15801) - Updated Friedman and Popescus H statistic calculation to include missing values support.\\n- [[#15741]](https://github.com/h2oai/h2o-3/issues/15741) - Implemented ability for force column types during parsing.\\n- [[#15713]](https://github.com/h2oai/h2o-3/issues/15713) - Improved the default threshold API for binomial classification.\\n- [[#15582]](https://github.com/h2oai/h2o-3/issues/15582) - Renamed prediction table header for UpliftDRF to be more user-friendly. \\n- [[#12678]](https://github.com/h2oai/h2o-3/issues/12678) - Added check to `mojo_predict_df` to look for a valid R dataframe.\\n- [[#7079]](https://github.com/h2oai/h2o-3/issues/7079) - Added verbosity to H2O initialization. `h2oconn.clust.show_status()` is now guarded and will only be shown when `verbose=True` during initialization.\\n- [[#6768]](https://github.com/h2oai/h2o-3/issues/6768) - Enabled categorical features for single decision tree. \\n\\n#### New Feature\\n- [[#15773]](https://github.com/h2oai/h2o-3/issues/15773) - Implemented `make_metrics` with custom AUUC thresholds for UpliftDRF.\\n- [[#15565]](https://github.com/h2oai/h2o-3/issues/15565) - Implemented custom metric for AutoML.\\n- [[#15559]](https://github.com/h2oai/h2o-3/issues/15559) - Implemented custom metric for Stacked Ensemble.\\n- [[#15556]](https://github.com/h2oai/h2o-3/issues/15556) - Implemented MOJO support for UpliftDRF.\\n- [[#15535]](https://github.com/h2oai/h2o-3/issues/15535) - Implemented Python 3.10 and 3.11 support.\\n- [[#6784]](https://github.com/h2oai/h2o-3/issues/6784) - Implemented custom metric for Deep Learning.\\n- [[#6783]](https://github.com/h2oai/h2o-3/issues/6783) - Implemented custom metric functionalities and the ATE, ATT, and ATC metrics for UpliftDRF.\\n- [[#6779]](https://github.com/h2oai/h2o-3/issues/6779) - Implemented custom metric for leaderboard.\\n- [[#6723]](https://github.com/h2oai/h2o-3/issues/6723) - Implemented new AdaBoost algorithm for binary classification.\\n- [[#6698]](https://github.com/h2oai/h2o-3/issues/6698) - Implemented Shapley values support for ensemble models.\\n\\n#### Security\\n- [[#15815]](https://github.com/h2oai/h2o-3/issues/15815) - Addressed CVE-2023-36478 by upgrading Jetty server.\\n- [[#15805]](https://github.com/h2oai/h2o-3/issues/15805) - Addressed CVE-2023-42503 by upgrading commons-compress to 1.24.0 in Standalone Jars.\\n- [[#15802]](https://github.com/h2oai/h2o-3/issues/15802) - Addressed CVE-2023-39410 by upgrading org.apache.avro:avro to 1.11.3.\\n- [[#15799]](https://github.com/h2oai/h2o-3/issues/15799) - Addressed CVE-2023-43642 by upgrading snappy-java in Standalone Jars to 1.1.10.5.\\n- [[#15759]](https://github.com/h2oai/h2o-3/issues/15759) - Addressed CVE-202-13949, CVE-2019-0205, CVE-2018-1320, and CVE-2018-11798 by excluding org.apache.thrift:libthrift from dependencies of Main Standalone Jar.\\n- [[#15757]](https://github.com/h2oai/h2o-3/issues/15757) - Addressed CVE-2020-29582 and CVE-2022-24329 by upgrading org.jetbrains.kotlin:kotlin-stdlib to 1.6.21 in Main and Steam Standalone Jars.\\n- [[#15755]](https://github.com/h2oai/h2o-3/issues/15755) - Addressed CVE-2023-3635 by upgrading com.squareup.okio:okio to 3.5.0 in Main and Steam Standalone Jars.\\n- [[#15752]](https://github.com/h2oai/h2o-3/issues/15752) - Addressed CVE-2023-34455, CVE-2023-34454, and CVE-2023-34453 by upgrading snappy-java to 1.1.10.3 in Main and Steam Standalone Jars.\\n- [[#15750]](https://github.com/h2oai/h2o-3/issues/15750) - Addressed CVE-2023-1370 by upgrading json-smart to 2.4.10 in Main standalone Jar.\\n- [[#15746]](https://github.com/h2oai/h2o-3/issues/15746) - Addressed CVE-2023-1436, CVE-2022-40149, CVE-2022-40150, CVE-2022-45685, and CVE-2022-45693 by upgrading org.codehaus.jettison:jettison to 1.5.4 in Main Standalone Jar.\\n- [[#15744]](https://github.com/h2oai/h2o-3/issues/15744) - Addressed CVE-2017-12197 by upgrading libpam4j to 1.11.\\n- [[#15706]](https://github.com/h2oai/h2o-3/issues/15706) - Addressed CVE-2023-40167 and CVE-2023-36479 by upgrading the Jetty server.\\n- [[#15470]](https://github.com/h2oai/h2o-3/issues/15470) - Upgraded Hadoop Libraries in Main Standalone Jar to address high and critical vulnerabilities.\\n\\n#### *Known Bug*\\n*(The list of bugs introduced by the changes in this release)*\\n\\n- [[#15832]](https://github.com/h2oai/h2o-3/issues/15832) - Broken Python and R API for UpliftDRF MOJO models. *Resolved in 3.44.0.2.*\\n\\n### 3.42.0.4 - 10/3/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/4/index.html</a>\\n\\n#### Bug Fix\\n- [[#15729]](https://github.com/h2oai/h2o-3/issues/15729) - Implemented multi-thread `as_data_frame` by using Datatable to speedup the conversion process.\\n- [[#15643]](https://github.com/h2oai/h2o-3/issues/15643) - Fixed validation of `include_explanation` and `exclude_explanation` parameters\\n\\n#### Improvement\\n- [[#15719]](https://github.com/h2oai/h2o-3/issues/15719) - Implemented warnings in python and R for accessing `model.negative_log_likelihood()`\\n- [[#13859]](https://github.com/h2oai/h2o-3/issues/13859) - Improved K-Means testing.\\n\\n#### New Feature\\n- [[#15727]](https://github.com/h2oai/h2o-3/issues/15727) - Implemented new `write_checksum` parameter that allows you to disable default Hadoop Parquet writer systematically writing a `.crc` checksum file for each written data file.\\n\\n#### Security\\n- [[#15766]](https://github.com/h2oai/h2o-3/issues/15766) - Addressed CVE-2023-40167 and CVE-2023-36479 in Steam Jar\\n\\n### 3.42.0.3 - 8/22/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/3/index.html</a>\\n\\n#### Bug Fix\\n- [[#15679]](https://github.com/h2oai/h2o-3/issues/15679) - Fixed GBM invalid tree index feature interaction.\\n- [[#15666]](https://github.com/h2oai/h2o-3/issues/15666) - Updated test to showcase GBM checkpointing.\\n- [[#6605]](https://github.com/h2oai/h2o-3/issues/6605) - Fixed `h2o.feature_interaction` failing on cross-validation models with early stopping. \\n\\n#### Improvement\\n- [[#6707]](https://github.com/h2oai/h2o-3/issues/6707) - Added extended message to `h2o.init()` to help users get around version mismatch error.\\n\\n#### Docs\\n- [[#15694]](https://github.com/h2oai/h2o-3/issues/15694) - Added `custom_metric_func` and `upload_custom_metric` to GLM.\\n- [[#15680]](https://github.com/h2oai/h2o-3/issues/15680) - Added security installation disclaimer in documentation and on the download page.\\n- [[#15598]](https://github.com/h2oai/h2o-3/issues/15598) - Updated `import_file` description and added Google Storage support note.\\n\\n#### Security\\n- [[#15687]](https://github.com/h2oai/h2o-3/issues/15687) - Replaced dependencies on no.priv.garshol.duke:duke:1.2 by extracting string comparators from Duke library.\\n\\n### 3.42.0.2 - 7/25/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/2/index.html</a>\\n\\n#### Bug Fix\\n- [[#15637]](https://github.com/h2oai/h2o-3/issues/15637) - Fixed AUCPR plot assigning incorrect values to the variable recalls and precisions. \\n- [[#6545]](https://github.com/h2oai/h2o-3/issues/6545) - Fixed out of memory error on multi-node sorting stage or sorted frame generation process.\\n\\n#### New Feature\\n- [[#15614]](https://github.com/h2oai/h2o-3/issues/15614) - Enabled H2OFrame to pandas DataFrame using multi-thread from datatable to speed-up the conversion process.\\n- [[#15597]](https://github.com/h2oai/h2o-3/issues/15597) - Added support for EMR 6.10.\\n\\n#### Engineering Task\\n- [[#15626]](https://github.com/h2oai/h2o-3/issues/15626) - Updated Jira links in H2O Flow UI with GH issue links.\\n\\n#### Docs\\n- [[#15629]](https://github.com/h2oai/h2o-3/issues/15629) - Fixed typo on Hadoop introduction page.\\n- [[#15606]](https://github.com/h2oai/h2o-3/issues/15606) - Updated major release blog for user guide.\\n- [[#15580]](https://github.com/h2oai/h2o-3/issues/15580) - Added information on UniformRobust method for `histogram_type` and created an accompanying blog post. \\n- [[#15563]](https://github.com/h2oai/h2o-3/issues/15563) - Updated out of date copyright year in user guide and python guide.\\n- [[#6574]](https://github.com/h2oai/h2o-3/issues/6574) - Added a warning to Infogram user guide that it should not be used to remove correlated columns.\\n- [[#6554]](https://github.com/h2oai/h2o-3/issues/6554) - Updated `nfolds` parameter description for AutoML in Python guide.\\n\\n#### Security\\n- [[#15634]](https://github.com/h2oai/h2o-3/issues/15634) - Addressed CVE-2019-10086 by upgrading MOJO2 lib.\\n\\n### 3.42.0.1 - 6/21/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-3.42.0/1/index.html</a>\\n\\n#### Bug Fix\\n- [[#15423]](https://github.com/h2oai/h2o-3/issues/15423) - Fixed Infogram cross-validation with weights.\\n- [[#15482]](https://github.com/h2oai/h2o-3/issues/15482) - Updated R package maintainer.\\n- [[#15461]](https://github.com/h2oai/h2o-3/issues/15461) - Fixed leaks in GLMs Negative Binomial estimation.\\n\\n#### Improvement\\n- [[#6843]](https://github.com/h2oai/h2o-3/issues/6843) - Changed `warning` tag to `info` tag when weights are not provided during validation/test dataset scoring when weights are present in training. \\n- [[#6828]](https://github.com/h2oai/h2o-3/issues/6828) - Removed support for Python 2.7 and 3.5.\\n- [[#6813]](https://github.com/h2oai/h2o-3/issues/6813) - Upgraded the default parquet library to 1.12.3 for standalone jar.\\n- [[#7630]](https://github.com/h2oai/h2o-3/issues/7630) - Upgraded XGBoost to version 1.6.1.\\n\\n#### New Feature\\n- [[#6548]](https://github.com/h2oai/h2o-3/issues/6548) - Implemented AIC metric for all GLM model families.\\n- [[#6880]](https://github.com/h2oai/h2o-3/issues/6880) - Implemented Tweedie variance power maximum likelihood estimation for GLM.\\n- [[#6943]](https://github.com/h2oai/h2o-3/issues/6943) - Added ability to convert H2OAssembly to a MOJO2 artifact.\\n- [[#7008]](https://github.com/h2oai/h2o-3/issues/7008) - Implemented new Decision Tree algorithm.\\n\\n#### Docs\\n- [[#15474]](https://github.com/h2oai/h2o-3/issues/15474) - Added link to AutoML Wave app from AutoML user guide.\\n- [[#15550]](https://github.com/h2oai/h2o-3/issues/15550) - Added documentation on H2OAssembly to MOJO 2 export functionality.\\n- [[#15602]](https://github.com/h2oai/h2o-3/issues/15602) - Added algorithm page in user guide for new Decision Tree algorithm.\\n- [[#15529]](https://github.com/h2oai/h2o-3/issues/15529) - Added AIC metric support for all GLM families to GLM user guide page and GLM booklet.\\n- [[#15466]](https://github.com/h2oai/h2o-3/issues/15466) - Updated authors and editors for GLM booklet.\\n- [[#6884]](https://github.com/h2oai/h2o-3/issues/6884) - Added documentation on Tweedie variance power maximum likelihood estimation to GLM booklet and user guide.\\n- [[#7200]](https://github.com/h2oai/h2o-3/issues/7200) - Improved user guide documentation for Generalized Additive Models algorithm.\\n\\n#### Security\\n- [[#15594]](https://github.com/h2oai/h2o-3/issues/15594) - Addressed CVE-2023-2976 in h2o-steam.jar.\\n- [[#15548]](https://github.com/h2oai/h2o-3/issues/15548) - Addressed CVE-2020-29582 in h2o-steam.jar.\\n- [[#15546]](https://github.com/h2oai/h2o-3/issues/15546) - Addressed CVE-2023-26048 and CVE-2023-26049 by upgrading Jetty for minimal and steam jar.\\n- [[#15540]](https://github.com/h2oai/h2o-3/issues/15540) - Addressed PRISMA-2023-0067 in h2o-steam.jar.\\n- [[#6827]](https://github.com/h2oai/h2o-3/issues/6827) - Addressed CVE-2023-1436, CVE-2022-45693, CVE-2022-45685, and CVE-2022-40150 by upgrading org.codehaus.jettison:jettison in h2o-steam.jar.\\n\\n### Kurka (3.40.0.4) - 4/28/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/4/index.html</a>\\n\\n#### Bug Fix\\n- [[#6758]](https://github.com/h2oai/h2o-3/issues/6758) - Fixed the deprecation warning thrown for Python 2.7 and 3.5.\\n\\n#### Improvement\\n- [[#6756]](https://github.com/h2oai/h2o-3/issues/6756) - Added official support for Python 3.9.\\n\\n#### Docs\\n- [[#6759]](https://github.com/h2oai/h2o-3/issues/6759) - Removed mention of support for Python 2.7 and Python 3.5 from documentation.\\n- [[#7600]](https://github.com/h2oai/h2o-3/issues/7600) - Reorganized supervised and unsupervised algorithm parameters by algorithm-specific, common, and shared-tree (for tree-based algorithms). Updated parameter descriptions for all supervised and unsupervised algorithms. Shifted all shared GLM family parameters to the GLM algorithm page.\\n\\n#### Security\\n- [[#6732]](https://github.com/h2oai/h2o-3/issues/6732) - Addressed CVE-2023-1370 by removing the vulnerability from h2o-steam.jar.\\n\\n### Kurka (3.40.0.3) - 4/4/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/3/index.html</a>\\n\\n#### Improvement\\n- [[#6763]](https://github.com/h2oai/h2o-3/issues/6763) - Added GAM Knot Locations to Model Output.\\n- [[#6764]](https://github.com/h2oai/h2o-3/issues/6764) - Addressed CVE-2014-125087 in h2o-steam.jar\\n\\n#### Engineering Story\\n- [[#6767]](https://github.com/h2oai/h2o-3/issues/6767) - Disabled execution of tests in client mode.\\n- [[#6772]](https://github.com/h2oai/h2o-3/issues/6772) - Deprecated support for Python 2.7 and 3.5.\\n\\n#### Docs\\n- [[#6773]](https://github.com/h2oai/h2o-3/issues/6773) - Introduced a page describing MOJO capabilities.\\n- [[#6790]](https://github.com/h2oai/h2o-3/issues/6790) - Updated the DRF documentation page to reflect what dataset is used to calculate the model metric.\\n- [[#6793]](https://github.com/h2oai/h2o-3/issues/6793) - Updated and rearranged the hyper-parameter list in the Grid Search documentation page.\\n\\n### Kurka (3.40.0.2) - 3/9/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/2/index.html</a>\\n\\n#### Bug Fix\\n- [[#6818]](https://github.com/h2oai/h2o-3/issues/6818) - Fixed dependency on numpy in Fairness-related code.\\n- [[#6819]](https://github.com/h2oai/h2o-3/issues/6819) - Added ability to debug GBM reproducibility by looking at tree structure with `equal_gbm_model_tree_structure`.\\n\\n#### Improvement\\n- [[#6995]](https://github.com/h2oai/h2o-3/issues/6995) - Fixed the deviance computation for GBM Poisson distribution.\\n\\n#### New Feature\\n- [[#6777]](https://github.com/h2oai/h2o-3/issues/6777) - Added `save_plot_path` parameter for Fairness plotting allowing you to save plots.\\n\\n#### Task\\n- [[#6538]](https://github.com/h2oai/h2o-3/issues/6538) - Implemented incremental MaxRSweep without using sweep vectors.\\n- [[#6799]](https://github.com/h2oai/h2o-3/issues/6799) - Removed duplicate predictors for ModelSelections MaxRSweep.\\n\\n#### Engineering Story\\n- [[#6776]](https://github.com/h2oai/h2o-3/issues/6776) - Pointed MLOps integration to internal.dedicated environment.\\n\\n#### Docs\\n- [[#6501]](https://github.com/h2oai/h2o-3/issues/6501) - Added warning that `max_runtime_secs` cannot always produce reproducible models.\\n- [[#6503]](https://github.com/h2oai/h2o-3/issues/6503) - Added example for how to save a file as a parquet.\\n- [[#6811]](https://github.com/h2oai/h2o-3/issues/6811) - Added example for how to connect to an H2O cluster by name.\\n- [[#6887]](https://github.com/h2oai/h2o-3/issues/6887) - Added information on the implementation of the `eval_metric` for XGBoost.\\n\\n### Kurka (3.40.0.1) - 2/8/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zz_kurka/1/index.html</a>\\n\\n#### Bug Fix\\n- [[#6845]](https://github.com/h2oai/h2o-3/issues/6845) - Improved GLM negative binomial calculation time.\\n- [[#6882]](https://github.com/h2oai/h2o-3/issues/6882) - Cleaned up COLLATE field in the description of the R package by allowing Roxygen2 to generate the COLLATE field.\\n- [[#6891]](https://github.com/h2oai/h2o-3/issues/6891) - Changed the exceptions in Stacked Ensembles checks to ModelBuilder warnings.\\n- [[#7090]](https://github.com/h2oai/h2o-3/issues/7090) - Fixed GLM ignoring time budget when trained using cross-validation in AutoML.\\n- [[#7132]](https://github.com/h2oai/h2o-3/issues/7132) - Fixed incorrect actual `ntrees` value reported in tree-based models.\\n\\n#### Improvement\\n- [[#6805]](https://github.com/h2oai/h2o-3/issues/6805) - Increased speed of XGBoost scoring on wide datasets.\\n- [[#6864]](https://github.com/h2oai/h2o-3/issues/6864) - Updated error message for when a user specifies the wrong cluster when connecting to a running H2O instance.\\n- [[#6886]](https://github.com/h2oai/h2o-3/issues/6886) - Improved memory usage in creation of parse-response for wide datasets.\\n- [[#6893]](https://github.com/h2oai/h2o-3/issues/6893) - Increased testing speed by adding ability to train XGBoost cross-validation models concurrently on the same GPU. \\n- [[#6900]](https://github.com/h2oai/h2o-3/issues/6900) - Added ability to score `eval_metric` on validation datasets for XGBoost.\\n- [[#6901]](https://github.com/h2oai/h2o-3/issues/6901) - Added notebook demonstrating `eval_metric` for XGBoost.\\n- [[#6902]](https://github.com/h2oai/h2o-3/issues/6902) - Increased XGBoost model training speed by disabling H2O scoring to rely solely on `eval_metric`. \\n- [[#6910]](https://github.com/h2oai/h2o-3/issues/6910) - Updated to Java 17 from Java 11/openjdk in H2O docker images.\\n- [[#7294]](https://github.com/h2oai/h2o-3/issues/7294) - Updated warning message for when H2O version is outdated.\\n- [[#7598]](https://github.com/h2oai/h2o-3/issues/7598) - Introduced a better format for storing default, input, and actual parameters in H2O model objects for R by using `@params` slots.\\n- [[#7835]](https://github.com/h2oai/h2o-3/issues/7835) - Added `model_summary` to Stacked Ensembles.\\n- [[#7980]](https://github.com/h2oai/h2o-3/issues/7980) - Moved StackedEnsembleModel::checkAndInheritModelProperties to StackedEnsemble class.\\n\\n#### New Feature\\n- [[#6858]](https://github.com/h2oai/h2o-3/issues/6858) - Added ability to publish models to MLOps via Python API.\\n- [[#7009]](https://github.com/h2oai/h2o-3/issues/7009) - Added ability to grid over Infogram.\\n- [[#7044]](https://github.com/h2oai/h2o-3/issues/7044) - Implemented Regression Influence Diagnostics for GLM.\\n- [[#7045]](https://github.com/h2oai/h2o-3/issues/7045) - Enhanced GBM procedures to output which records are used for each tree.\\n- [[#7537]](https://github.com/h2oai/h2o-3/issues/7537) - Added learning curve plot to H2Os Explainability.\\n\\n#### Task\\n- [[#6802]](https://github.com/h2oai/h2o-3/issues/6802) - Added `negative_log_likelihood` and `average_objective` accessor functions in R and Python for GLM.\\n- [[#7088]](https://github.com/h2oai/h2o-3/issues/7088) - Limited the number of iterations when training the final GLM model after cross-validation.\\n\\n#### Technical Task\\n- [[#6898]](https://github.com/h2oai/h2o-3/issues/6898) - Added support for scoring `eval_metric` on a validation set for external XGBoost cluster.\\n- [[#6899]](https://github.com/h2oai/h2o-3/issues/6899) - Added support for scoring `eval_metric` on a validation set for internal XGBoost cluster.\\n- [[#7012]](https://github.com/h2oai/h2o-3/issues/7012) - Implemented GLM dispersion estimation parameter using maximum likelihood method for the negative binomial family.\\n\\n#### Docs\\n- [[#6820]](https://github.com/h2oai/h2o-3/issues/6820) - Highlighted information about how rebalancing makes reproducibility impossible.\\n- [[#6815]](https://github.com/h2oai/h2o-3/issues/6815) - Added documentation on the `negative_log_likelihood` and `average_objective` accessor functions.\\n- [[#6816]](https://github.com/h2oai/h2o-3/issues/6816) - Added information on GLM dispersion estimation using maximum likelihood method for the negative binomial family.\\n- [[#6821]](https://github.com/h2oai/h2o-3/issues/6821) - Added documentation on Regression Influence Diagnostics for GLM.\\n- [[#6803]](https://github.com/h2oai/h2o-3/issues/6803) - Fixed non-functional data paths in code examples throughout the user guide.\\n- [[#6804]](https://github.com/h2oai/h2o-3/issues/6804) - Added information on the `row_to_tree_assignment` function.\\n- [[#6807]](https://github.com/h2oai/h2o-3/issues/6807) - Added documentation on using H2O with Apple M1 chip.\\n- [[#6808]](https://github.com/h2oai/h2o-3/issues/6808) - Added information on `init` parameter being skipped due to `estimate_k=True` for K-Means.\\n\\n### Zygmund (3.38.0.4) - 1/5/2023\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/4/index.html</a>\\n\\n#### Bug Fix\\n- [[#6851]](https://github.com/h2oai/h2o-3/issues/6851) - Fixed error in SHAP values report for DRF. \\n- [[#6865]](https://github.com/h2oai/h2o-3/issues/6865) - Fixed a ModelSelection replacement error stopping too early and implemented incremental forward step and incremental replacement step for numerical predictors.\\n\\n#### Task\\n- [[#6852]](https://github.com/h2oai/h2o-3/issues/6852) - Resolved hyperparameters amongst the algorithms.\\n- [[#6857]](https://github.com/h2oai/h2o-3/issues/6857) - Removed redundant predictors found in `mode=backward` for ModelSelection.\\n\\n#### Engineering Story\\n- [[#6846]](https://github.com/h2oai/h2o-3/issues/6846) - Renamed the docker image `h2o-steam-k8s` to `h2o-open-source-k8s-minimal`.\\n\\n#### Docs\\n- [[#6800]](https://github.com/h2oai/h2o-3/issues/6800) - Updated download page by adding options for steam jar and python client without h2o backend.\\n- [[#6849]](https://github.com/h2oai/h2o-3/issues/6849) - Fixed log likelihood of negative binomial for GLM.\\n- [[#6855]](https://github.com/h2oai/h2o-3/issues/6855) - Added how users can force an unsupported Java version.\\n- [[#6856]](https://github.com/h2oai/h2o-3/issues/6856) - Fixed broken links on the H2O Release page.\\n- [[#6860]](https://github.com/h2oai/h2o-3/issues/6860) - Added information on how Isolation Forest and Extended Isolation Forest handle missing values.\\n- [[#6862]](https://github.com/h2oai/h2o-3/issues/6862) - Fixed typos and made examples work on performance-and-prediction.html.\\n- [[#6863]](https://github.com/h2oai/h2o-3/issues/6863) - Removed outdated roadmap from Readme file.\\n\\n#### Security\\n- [[#6794]](https://github.com/h2oai/h2o-3/issues/6794) - Addressed CVE-2022-3509 by upgrading `google-cloud-storage`.\\n\\n### Zygmund (3.38.0.3) - 11/23/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/3/index.html</a>\\n\\n#### Bug Fix\\n- [[#6877]](https://github.com/h2oai/h2o-3/issues/6877) - Enforced DkvClassLoader while accessing Python resources through JythonCFuncLoader.\\n- [[#6878]](https://github.com/h2oai/h2o-3/issues/6878) - Closed open file descriptors from H2OConnection.\\n- [[#6871]](https://github.com/h2oai/h2o-3/issues/6871) - Fixed incorrect value indicator for a partial dependence plot for its current row.\\n- [[#6873]](https://github.com/h2oai/h2o-3/issues/6873) - Fixed GBM model with `interaction_constraints` only building single-depth trees.\\n- [[#6897]](https://github.com/h2oai/h2o-3/issues/6897) - Fixed slow estimator validation when training model with wide datasets.\\n- [[#6907]](https://github.com/h2oai/h2o-3/issues/6907) - Fixed GAM failure when `numknots=2` for I-spline.\\n\\n\\n#### Task\\n- [[#6896]](https://github.com/h2oai/h2o-3/issues/6896) - Ensured non-negative will not overwrite `splines_non_negative` for GAM I-spline.\\n- [[#6921]](https://github.com/h2oai/h2o-3/issues/6921) - Implemented p-value calculation for GLM with regularization.\\n- [[#6925]](https://github.com/h2oai/h2o-3/issues/6925) - Verified the minimum number of knots each spline type can support for GAM.\\n- [[#6926]](https://github.com/h2oai/h2o-3/issues/6926) - Implemented normal (non-monotonic) splines that can support any degrees.\\n\\n\\n#### Docs\\n- [[#6874]](https://github.com/h2oai/h2o-3/issues/6874) - Updated `compute_p_value` documentation for GLM and GAM to reflect that p-values and z-values can now be computed with regularization.\\n- [[#6875]](https://github.com/h2oai/h2o-3/issues/6875) - Documented GAM M-splines.\\n- [[#6876]](https://github.com/h2oai/h2o-3/issues/6876) - Updated site logo, favicon, and color scheme to reflect H2Os brand kit.\\n- [[#6870]](https://github.com/h2oai/h2o-3/issues/6870) - Updated booklet links for GBM, GLM, and Deep Learning on their respective algorithm pages.\\n- [[#6881]](https://github.com/h2oai/h2o-3/issues/6881) - Fixed typo in Model Selection for `build_glm_model` parameter.\\n- [[#6885]](https://github.com/h2oai/h2o-3/issues/6885) - Updated links in the provided bibliography in the FAQ.\\n- [[#6894]](https://github.com/h2oai/h2o-3/issues/6894) - Removed Sparkling Water booklet link from the download page.\\n- [[#6904]](https://github.com/h2oai/h2o-3/issues/6904) - Added optional Python plotting requirement `matplotlib` to the download page.\\n\\n### Zygmund (3.38.0.2) - 10/27/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/2/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#6895]](https://github.com/h2oai/h2o-3/issues/6895) - Fixed H2ODeepLearningEstimator `autoencoder` not working without `y` value.\\n- [[#6911]](https://github.com/h2oai/h2o-3/issues/6911) - Added `libgomp` into docker images thus enabling XGBoost multithreading.\\n- [[#6919]](https://github.com/h2oai/h2o-3/issues/6919) - Stopped throwing warning about jobs not having proper model types when models werent even trained.\\n- [[#6928]](https://github.com/h2oai/h2o-3/issues/6928) - Fixed cross validation failure for concurrent sorting.\\n- [[#6930]](https://github.com/h2oai/h2o-3/issues/6930) - Enabled parallelism in cross validation for Isotonic Regression.\\n\\n#### Task\\n\\n- [[#6917]](https://github.com/h2oai/h2o-3/issues/6917) - Enabled GAM I-spline to support increasing and decreasing functions.\\n- [[#6925]](https://github.com/h2oai/h2o-3/issues/6925) - Updated the number of knots required for GAM I-splines to be >=2.\\n- [[#6949]](https://github.com/h2oai/h2o-3/issues/6949) -  Improved ModelSelections `mode=maxrsweep` runtime.\\n\\n#### Docs\\n\\n- [[#6890]](https://github.com/h2oai/h2o-3/issues/6890) - Added information on ModelSelections new `build_glm_model` parameter for `mode=maxrsweep`.\\n- [[#6903]](https://github.com/h2oai/h2o-3/issues/6903) - Fixed incorrect header case on ModelSelection and Cox Proportional Hazards algorithm pages in the user guide.\\n- [[#6913]](https://github.com/h2oai/h2o-3/issues/6913) - Added an example to Variable Inflation Factors in the user guide.\\n- [[#6917]](https://github.com/h2oai/h2o-3/issues/6917) - Fixed broken links on the Welcome to H2O-3 page of the user guide.\\n- [[#6948]](https://github.com/h2oai/h2o-3/issues/6948) - Added model explainability for plotting SHAP to the Performance and Prediction page of the user guide.\\n- [[#7442]](https://github.com/h2oai/h2o-3/issues/7442) - Added examples for `varsplits()` and `feature_frequencies()` to Python documentation.\\n\\n#### Security\\n\\n- [[#6889]](https://github.com/h2oai/h2o-3/issues/6889) - Addressed CVE-2022-42003 and CVE-2022-42889 security issues through Library upgrades.\\n\\n\\n### Zygmund (3.38.0.1) - 9/19/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/1/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#6937]](https://github.com/h2oai/h2o-3/issues/6937) - Fixed the sorting of `h2o.make_leaderboard`.\\n- [[#6940]](https://github.com/h2oai/h2o-3/issues/6940) - Fixed H2O dependencies overriding Jetty implementation.\\n- [[#6951]](https://github.com/h2oai/h2o-3/issues/6951) - Fixed Flows export Frame throwing an NPE because it doesnt provide a file type.\\n- [[#6959]](https://github.com/h2oai/h2o-3/issues/6959) - Fixed GLM ordinal generic metrics to provide missing information in the payload.\\n- [[#6960]](https://github.com/h2oai/h2o-3/issues/6960) - Fixed maxrsweep NPE in ModelSelection thrown when the replacement step stopped too early.\\n- [[#6961]](https://github.com/h2oai/h2o-3/issues/6961) - Fixed maxrsweep replacement bug in ModelSelection by updating the implementation method.\\n- [[#6973]](https://github.com/h2oai/h2o-3/issues/6973) - Fixed  unnecessary transformations in the scikit-learn wrapper by using model performance API. \\n- [[#6979]](https://github.com/h2oai/h2o-3/issues/6979) - Fixed upload of big files in Sparkling Water deployment.\\n- [[#6983]](https://github.com/h2oai/h2o-3/issues/6983) - Changed the error message that GLM does not support contributions.\\n- [[#6985]](https://github.com/h2oai/h2o-3/issues/6985) - Fixed QuantilesGlobal histogram type failing in GBM when all columns were categorial.\\n- [[#7002]](https://github.com/h2oai/h2o-3/issues/7002) - Added support for MapR 6.2 to fix the error caused by updating the cluster.\\n- [[#7006]](https://github.com/h2oai/h2o-3/issues/7006) - Fixed large file upload in Python.\\n- [[#7023]](https://github.com/h2oai/h2o-3/issues/7023) - Fixed inability to stop print out of model information in Python.\\n- [[#7056]](https://github.com/h2oai/h2o-3/issues/7056) - Removed `-seed` variable hiding in GAM.\\n- [[#7104]](https://github.com/h2oai/h2o-3/issues/7104) - Updated `h2o.upload_mojo` to also work for POJO.\\n- [[#7432]](https://github.com/h2oai/h2o-3/issues/7432) - Added unsupported operation exception when trying to use SHAP summary plot when building DRF model with `binomial_double_trees`.\\n- [[#8542]](https://github.com/h2oai/h2o-3/issues/8542) - Refactored the rendering logic in the Python client.\\n- [[#10436]](https://github.com/h2oai/h2o-3/issues/10436) - Added xval argument to `h2o.confusionMatrix` in R.\\n\\n#### Improvement\\n\\n- [[#6933]](https://github.com/h2oai/h2o-3/issues/6933) - Added support for calibrating an already trained model manually.\\n- [[#6941]](https://github.com/h2oai/h2o-3/issues/6941) - Added support for using Isotonic Regression for model calibration.\\n- [[#6942]](https://github.com/h2oai/h2o-3/issues/6942) - Added ability to S3A allowing it to share the built-in AWS credential providers.\\n- [[#6947]](https://github.com/h2oai/h2o-3/issues/6947) - Improved `configure_s3_using_s3a` allowing it to be usable in any deployment.\\n- [[#6982]](https://github.com/h2oai/h2o-3/issues/6982) - Updated `train_segments` function in R to be independent of camel casing in the algorithm name.\\n- [[#6986]](https://github.com/h2oai/h2o-3/issues/6986) -  Improved runtime for QuantilesGlobal histogram by using exact split-points for low-cardinality columns.\\n- [[#6992]](https://github.com/h2oai/h2o-3/issues/6992) - Exposed the Sequential Walker for R/Python and added option to disable early stopping.\\n- [[#7007]](https://github.com/h2oai/h2o-3/issues/7007) - Cleaned up Key API by removing replicas.\\n- [[#7340]](https://github.com/h2oai/h2o-3/issues/7340) - Cleaned up the default output after training a model.\\n- [[#7510]](https://github.com/h2oai/h2o-3/issues/7510) - Exposed calibrated probabilities in `mojo_predict_pandas`.\\n\\n#### New Feature\\n\\n- [[#6950]](https://github.com/h2oai/h2o-3/issues/6950) - Simplified the configuration of S3 for Frame exportation. \\n- [[#6984]](https://github.com/h2oai/h2o-3/issues/6984) - Added `train_segments` test for Isolation Forest.\\n- [[#6991]](https://github.com/h2oai/h2o-3/issues/6991) - Added ability to `h2o.no_progress` in R allowing it to accept expressions.\\n- [[#7011]](https://github.com/h2oai/h2o-3/issues/7011) - Implemented dispersion parameter estimation for GLM.\\n- [[#7016]](https://github.com/h2oai/h2o-3/issues/7016) - Added ability to export H2O Frame to a Parquet.\\n- [[#7076]](https://github.com/h2oai/h2o-3/issues/7076) - Added Pareto front plots to AutoML Explain.\\n- [[#7091]](https://github.com/h2oai/h2o-3/issues/7091) - Added deviance method to dispersion for calculating p-values. \\n- [[#7093]](https://github.com/h2oai/h2o-3/issues/7093) - Implemented variable inflation factors for GLM.\\n- [[#7192]](https://github.com/h2oai/h2o-3/issues/7192) - Implemented in-training checkpoints for GBM.\\n- [[#8005]](https://github.com/h2oai/h2o-3/issues/8005) - Implemented support for interactions to MOJO for CoxPH.\\n- [[#12152]](https://github.com/h2oai/h2o-3/issues/12152) - Added `h2o.make_leaderboard` function which scores and compares a set of models to AutoML.\\n\\n#### Task\\n\\n- [[#6927]](https://github.com/h2oai/h2o-3/issues/6927) - Secured XGBoost connections in multinode environments. \\n- [[#6948]](https://github.com/h2oai/h2o-3/issues/6948) - Added missing added predictor and deleted predictor to the result frame and model summary of ModelSelection.\\n- [[#6952]](https://github.com/h2oai/h2o-3/issues/6952) - Added support allowing you to force GLM to build a null model where the model only returns the coefficients for the intercept.\\n- [[#6953]](https://github.com/h2oai/h2o-3/issues/6953) - Added support allowing GLM `gamma` to fix the dispersion parameter to calculate p-values.\\n- [[#6998]](https://github.com/h2oai/h2o-3/issues/6998) - Implemented maxr speedup for Modelselection by introducting maxrsweep.\\n- [[#7013]](https://github.com/h2oai/h2o-3/issues/7013) - Implemented dispersion factor estimation using maximum likelihood for GLM gamma family.\\n\\n#### Docs\\n\\n- [[#6920]](https://github.com/h2oai/h2o-3/issues/6920) - Added documentation on Isotonic Regression.\\n- [[#6931]](https://github.com/h2oai/h2o-3/issues/6931) - Added variable inflation factors to GLM section of the user guide.\\n- [[#6932]](https://github.com/h2oai/h2o-3/issues/6932) - Added Tweedie dispersion parameter estimation to the GLM section of the user guide.\\n- [[#6934]](https://github.com/h2oai/h2o-3/issues/6934) - Added confusion matrix calculation explanation to performance and prediction.\\n- [[#6939]](https://github.com/h2oai/h2o-3/issues/6939) - Added `get_predictors_removed_per_step()` and `get_predictors_added_per_step()` examples to ModelSelection.\\n- [[#6945]](https://github.com/h2oai/h2o-3/issues/6945) - Added use case section to the welcome page of the user guide.\\n- [[#6987]](https://github.com/h2oai/h2o-3/issues/6987) - Added MOJO import/export information to each algorithm page.\\n- [[#7048]](https://github.com/h2oai/h2o-3/issues/7048) - Added major release blogs to user guide and moved change log to top of the sidebar.\\n\\n### Zumbo (3.36.1.5) - 9/15/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/5/index.html</a>\\n\\n#### Security\\n\\n- Addressed security vulnerability CVE-2021-22569 in the `h2o.jar`.\\n\\n### Zumbo (3.36.1.4) - 8/3/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/4/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#6954]](https://github.com/h2oai/h2o-3/issues/6954) - Disabled `partial_plot` for Uplift DRF temporarily.\\n- [[#6958]](https://github.com/h2oai/h2o-3/issues/6958) - Added support for predicting with Autoencoder when using eigen encoding.\\n- [[#6962]](https://github.com/h2oai/h2o-3/issues/6962) - Fixed XGBoost failure with enabled cross validation in external cluster mode by explicitly starting external XGBoost before cross validation.\\n\\n#### Security\\n\\n- [[#6946]](https://github.com/h2oai/h2o-3/issues/6946) - Addressed security vulnerabilities CVE-2021-22573 and CVE-2019-10172 in Steam assembly.\\n\\n### Zumbo (3.36.1.3) - 7/8/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/3/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#6975]](https://github.com/h2oai/h2o-3/issues/6975) - Fixed CoxPH MOJO ignoring offset column.\\n- [[#6976]](https://github.com/h2oai/h2o-3/issues/6976) - Fixed the incorrect predictions from the CoxPH MOJO on categorical columns. \\n- [[#6977]](https://github.com/h2oai/h2o-3/issues/6977) - Fixed the **View** button not working after completing an AutoML job.\\n- [[#6989]](https://github.com/h2oai/h2o-3/issues/6989) - Fixed `num_of_features` not being used in call for `varimp_heatmap()`.\\n- [[#7015]](https://github.com/h2oai/h2o-3/issues/7015) - Fixed GAMs `fold_column` being treated as a normal column to score for.\\n- [[#7053]](https://github.com/h2oai/h2o-3/issues/7053) - Updated GBM cross validation model summary tables to reflect that some trees are removed due to a better score occurring with fewer trees.\\n- [[#7140]](https://github.com/h2oai/h2o-3/issues/7140) - Fixed `fit_params` passthrough for scikit-learn compatibility.\\n- [[#7718]](https://github.com/h2oai/h2o-3/issues/7718) - Fixed validateWithCheckpoint to work with default parameter settings.\\n- [[#7719]](https://github.com/h2oai/h2o-3/issues/7719) - Fixed validateWithCheckpoint to work with parameters that are arrays.\\n\\n#### Improvement\\n\\n- [[#6990]](https://github.com/h2oai/h2o-3/issues/6990) - Added expert option to force-enable MOJO for CoxPH even when `interactions` are enabled.\\n- [[#7060]](https://github.com/h2oai/h2o-3/issues/7060) - Makes language rules generation on demand and introduced EnumLimited option for categorical encoding.\\n\\n#### New Feature\\n\\n- [[#6968]](https://github.com/h2oai/h2o-3/issues/6968) - Added `transform_frame` for GLRM allowing users to obtain the new X for a new data set.\\n- [[#6974]](https://github.com/h2oai/h2o-3/issues/6974) - Added support for numerical interactions in CoxPH MOJO.\\n\\n#### Docs\\n\\n- [[#6965]](https://github.com/h2oai/h2o-3/issues/6965) - Fixed the `uplift_metric` documentation for Uplift DRF.\\n- [[#6967]](https://github.com/h2oai/h2o-3/issues/6967) - Added `transform_frame` to GLRM documentation. \\n- [[#6970]](https://github.com/h2oai/h2o-3/issues/6970) - Added `mode = maxrsweep` to ModelSelection documentation.\\n- [[#6971]](https://github.com/h2oai/h2o-3/issues/6971) - Corrected the R documentation on R^2.\\n- [[#6988]](https://github.com/h2oai/h2o-3/issues/6988) - Updated supported MOJO list to include GAM MOJO import.\\n\\n#### Security\\n\\n- [[#6978]](https://github.com/h2oai/h2o-3/issues/6978) - Fixed security issue in genmodel (CVE-2022-25647).\\n\\n### Zumbo (3.36.1.2) - 5/26/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/2/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#6999]](https://github.com/h2oai/h2o-3/issues/6999) - Refactored Uplift DRF methods.\\n- [[#7001]](https://github.com/h2oai/h2o-3/issues/7001) - Removed duplicate runs in MaxR.\\n- [[#7014]](https://github.com/h2oai/h2o-3/issues/7014) - Fixed the ambiguity check in Explains consolidate varimp.\\n- [[#7020]](https://github.com/h2oai/h2o-3/issues/7020) - Improved efficiency of ``pd_plot`` and ``ice_plot`` and made ``rug`` optional.\\n- [[#7021]](https://github.com/h2oai/h2o-3/issues/7021) - Fixed H2O failing with null pointer exception when providing an improper ``-network`` to h2o.jar.\\n- [[#7022]](https://github.com/h2oai/h2o-3/issues/7022) - Fixed external XGBoost on K8s.\\n- [[#7042]](https://github.com/h2oai/h2o-3/issues/7042) - Fixed failing concurrent-running GLM training processes.\\n- [[#7283]](https://github.com/h2oai/h2o-3/issues/7283) - Fixed missing time values SHAP summary plot error.\\n- [[#7732]](https://github.com/h2oai/h2o-3/issues/7732) - Fixed Partial Dependence Plots date/time handling from explainability modules.\\n\\n#### New Feature\\n\\n- [[#7004]](https://github.com/h2oai/h2o-3/issues/7004) - Updated Uplift DRF API.\\n\\n#### Docs\\n\\n- [[#7000]](https://github.com/h2oai/h2o-3/issues/7000) - Added ``model_summary`` examples for GLM.\\n- [[#7010]](https://github.com/h2oai/h2o-3/issues/7010) - Updated incorrect formula in GLM booklet.\\n- [[#7219]](https://github.com/h2oai/h2o-3/issues/7219) - Updated Python Module documentation readability.\\n\\n### Zumbo (3.36.1.1) - 4/13/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zumbo/1/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7035]](https://github.com/h2oai/h2o-3/issues/7035) - Fixed Residual Analysis plot flipping the residual calculation.\\n- [[#7040]](https://github.com/h2oai/h2o-3/issues/7040) - Added more detailed exception when disconnected due to error caused by `Rcurl`. \\n- [[#7047]](https://github.com/h2oai/h2o-3/issues/7047) - Made R client attempt to connect to `curl` package  instead of `Rcurl` package first.\\n- [[#7055]](https://github.com/h2oai/h2o-3/issues/7055) - Ensures GLM models fail instead of throwing warnings when `beta_contraints` and `non_negative are used with multinomial or ordinal families.\\n- [[#7057]](https://github.com/h2oai/h2o-3/issues/7057) - Fixed how `cv_computeAndSetOptimalParameters` deals with multiple `alpha` and `lambda` values across different folds.\\n- [[#7058]](https://github.com/h2oai/h2o-3/issues/7058) - Increased MaxR running speed.\\n- [[#7068]](https://github.com/h2oai/h2o-3/issues/7068) - Fixed `getGLMRegularizationPath` erroring out when `standardize = False`.\\n- [[#7194]](https://github.com/h2oai/h2o-3/issues/7194) - Fixed Keystore not generating on Java 16+.\\n- [[#7634]](https://github.com/h2oai/h2o-3/issues/7634) - Added a `num_of_features` argument to `h2o.varimp_heatmap` to limit the number of displayed variables. \\n- [[#12130]](https://github.com/h2oai/h2o-3/issues/12130) - Fixed `cross_validation_metrics_summary` not being accessible for Stacked Ensemble.\\n\\n#### Improvement\\n\\n- [[#7029]](https://github.com/h2oai/h2o-3/issues/7029) - Improved AUUC result information in Uplift DRF by adding information on number of bins.\\n- [[#7038]](https://github.com/h2oai/h2o-3/issues/7038) - Replaced `class()` with `inherits()` in R package. \\n- [[#7039]](https://github.com/h2oai/h2o-3/issues/7039) - Fixed invalid URLs in R Package.\\n- [[#7051]](https://github.com/h2oai/h2o-3/issues/7051) - Added normalized `AUUC` to Uplift DRF. \\n- [[#7059]](https://github.com/h2oai/h2o-3/issues/7059) - Sped-up AutoML by avoiding sleep-waiting.\\n- [[#7061]](https://github.com/h2oai/h2o-3/issues/7061) - Removed Stacked Ensembles with XGB metalearner to increase speed.\\n- [[#7062]](https://github.com/h2oai/h2o-3/issues/7062) - Ensures AutoML reproducibility when `max_models` is used.\\n- [[#7135]](https://github.com/h2oai/h2o-3/issues/7135) - Updated AutoML default leaderboard regression sorting to `RMSE`.\\n\\n#### New Feature\\n\\n- [[#7036]](https://github.com/h2oai/h2o-3/issues/7036) - Bundled several basic datasets with H2O for use in examples.\\n- [[#7050]](https://github.com/h2oai/h2o-3/issues/7050) - Added h2o.jar assembly for secure Steam deployments and excluded PAM authentication from minimal/Steam builds.\\n- [[#7054]](https://github.com/h2oai/h2o-3/issues/7054) - Added ability to ingest data from secured Hive using h2odriver.jar in standalone.\\n- [[#7073]](https://github.com/h2oai/h2o-3/issues/7073) - Bundled KrbStandalone extension in h2odriver.jar. \\n- [[#7085]](https://github.com/h2oai/h2o-3/issues/7085) - Implemented new method for defining histogram split-points in GBM/DRF designed to address outlier issues with default `UniformAdaptive` method.\\n- [[#7092]](https://github.com/h2oai/h2o-3/issues/7092) - Added ability to reorder frame levels based on their frequencies and to relieve only topN levels for GLM.\\n- [[#7094]](https://github.com/h2oai/h2o-3/issues/7094) - Added a function to calculate predicted versus actual response in GLM.\\n- [[#7258]](https://github.com/h2oai/h2o-3/issues/7258) - Implemented MOJO for Extended Isolation Forest.\\n- [[#7261]](https://github.com/h2oai/h2o-3/issues/7261) - Added monotone splines to GAM.\\n- [[#7271]](https://github.com/h2oai/h2o-3/issues/7271) - Added a plot function for gains/lift to R and Python.  \\n- [[#7285]](https://github.com/h2oai/h2o-3/issues/7285) - Added ability to acquire metric builder updates for Sparkling Water calculation without H2O runtime.\\n- [[#7664]](https://github.com/h2oai/h2o-3/issues/7664) - Added support for `interaction_constraints` to GBM.\\n- [[#7785]](https://github.com/h2oai/h2o-3/issues/7785) - Exposed distribution parameter in AutoML\\n\\n#### Task\\n\\n- [[#7078]](https://github.com/h2oai/h2o-3/issues/7078) - Decoupled Infogram and XGBoost removing Infograms reliance on XGBoost to work.\\n- [[#7080]](https://github.com/h2oai/h2o-3/issues/7080) - Verified GLM binomial IRLSM implementation and `p-value` calculation. \\n- [[#7089]](https://github.com/h2oai/h2o-3/issues/7089) - Added private ModelBuilder parameter to AutoML to enforce the time budget on the final model after cross-validation.\\n\\n#### Sub-Task\\n\\n- [[#7069]](https://github.com/h2oai/h2o-3/issues/7069) - Made Ice plot functionalities also available on `pd_plot`.\\n- [[#7160]](https://github.com/h2oai/h2o-3/issues/7160) - Added option to normalize y-axis values. \\n- [[#7163]](https://github.com/h2oai/h2o-3/issues/7163) - Added option to display logodds for binary models for Ice plots.\\n- [[#7164]](https://github.com/h2oai/h2o-3/issues/7164) - Added ability to save final graphing data to a frame for Ice plots.\\n- [[#7165]](https://github.com/h2oai/h2o-3/issues/7165) - Added option to specify a grouping variable for Ice plots.\\n- [[#7166]](https://github.com/h2oai/h2o-3/issues/7166) - Shows original observation values as points on the line for Ice plots.\\n- [[#7167]](https://github.com/h2oai/h2o-3/issues/7167) - Added option to toggle PDP vs Ice lines on or off.\\n\\n#### Docs\\n\\n- [[#7034]](https://github.com/h2oai/h2o-3/issues/7034) - Added documentation on the monotone spline for GAM.\\n- [[#7027]](https://github.com/h2oai/h2o-3/issues/7027) - Added links to the Additional Resources page to the sites where users can ask questions.\\n- [[#7031]](https://github.com/h2oai/h2o-3/issues/7031) - Updated the examples for the Residual Analysis Plot.\\n- [[#7037]](https://github.com/h2oai/h2o-3/issues/7037) - Updated the K8s deployment tutorial.\\n- [[#7043]](https://github.com/h2oai/h2o-3/issues/7043) - Improved Uplift DRF User Guide documentation. \\n- [[#7049]](https://github.com/h2oai/h2o-3/issues/7049) - Shifted the links from the H2O-3 docs page to the User Guide Additional Resources page. \\n- [[#7066]](https://github.com/h2oai/h2o-3/issues/7066) - Fixed MOJO importable/exportable table in User Guide.\\n- [[#7067]](https://github.com/h2oai/h2o-3/issues/7067) - Added a note that MOJOs wont build if `interactions` are specified. \\n- [[#7070]](https://github.com/h2oai/h2o-3/issues/7070) - Added information on how H2O handles date columns. \\n- [[#7075]](https://github.com/h2oai/h2o-3/issues/7075) - Fixed code typos on Admissible ML page in User Guide.\\n- [[#7074]](https://github.com/h2oai/h2o-3/issues/7074) - Added information on the `-hdfs_config` tag.\\n\\n\\n### Zorn (3.36.0.4) - 3/30/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/4/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7046]](https://github.com/h2oai/h2o-3/issues/7046) - Fixed logic operations error in R package. \\n- [[#7052]](https://github.com/h2oai/h2o-3/issues/7052) - Clarified that `enum` and `eigen` `categorical_encoding` values do not work for XGBoost.\\n\\n#### Improvement\\n\\n- [[#7177]](https://github.com/h2oai/h2o-3/issues/7177) - Added the Qini value metric to Uplift DRF.\\n\\n#### Docs\\n\\n- [[#7157]](https://github.com/h2oai/h2o-3/issues/7157) - Added information on the `make_metrics` command to the Performance and Prediction section of the User Guide.\\n\\n\\n### Zorn (3.36.0.3) - 2/16/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/3/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7098]](https://github.com/h2oai/h2o-3/issues/7098) - Fixed S3 file downloads not working by adding `aws_java_sdk_sts` as a dependency of H2O persist S3.\\n- [[#7102]](https://github.com/h2oai/h2o-3/issues/7102) - Added note to GBM, DRF, IF, and EIF that `build_tree_one_node=True` does not work with current release.\\n- [[#7108]](https://github.com/h2oai/h2o-3/issues/7108) - Extended AWS default credential chain instead of replacing it.\\n- [[#7112]](https://github.com/h2oai/h2o-3/issues/7112) - Fixed import failures for URLs longer than 152 characters.\\n- [[#7113]](https://github.com/h2oai/h2o-3/issues/7113) -  Fix AutoML ignoring `verbosity` setting.\\n- [[#7138]](https://github.com/h2oai/h2o-3/issues/7138) - Fixed Huber distribution bug for `deviance`.\\n\\n#### Improvement\\n\\n- [[#7187]](https://github.com/h2oai/h2o-3/issues/7187) - Removed H2O API Extensions from `h2o.init()` output.\\n\\n#### Docs\\n\\n- [[#7096]](https://github.com/h2oai/h2o-3/issues/7096) - Corrected typos and inconsistencies in Admissible ML documentation.\\n- [[#7119]](https://github.com/h2oai/h2o-3/issues/7119) - Updated copyright year in documentation.\\n- [[#7128]](https://github.com/h2oai/h2o-3/issues/7128) - Clarified feasible intervals for tweedie power.\\n- [[#7196]](https://github.com/h2oai/h2o-3/issues/7196) - Clarified Java requirements when running H2O on Hadoop.\\n\\n\\n### Zorn (3.36.0.2) - 1/25/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/2/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7125]](https://github.com/h2oai/h2o-3/issues/7125) - Updated XGBoostMojoModel to only consider the number of built trees, not the value of ``ntrees``.\\n- [[#7126]](https://github.com/h2oai/h2o-3/issues/7126) - Fixed issue in AutoEncoders early stopping automatic selection by setting ``AUTO = MSE`` instead of ``deviance``.\\n- [[#7133]](https://github.com/h2oai/h2o-3/issues/7133) - Fixed MOJO imports to retain information on weights column.\\n- [[#7143]](https://github.com/h2oai/h2o-3/issues/7143) - Fixed XGBoost errors on Infogram by improving support for XGBoost.\\n- [[#7145]](https://github.com/h2oai/h2o-3/issues/7145) - Fixed MOJO import automatically re-using original Model ID for current release cycle.\\n- [[#7149]](https://github.com/h2oai/h2o-3/issues/7149) - Fixed import of Parquet files from S3.\\n- [[#7155]](https://github.com/h2oai/h2o-3/issues/7155) - Fixed `h2o.group_by` warning present in documentation example caused by function only reading the first column when several are provided.\\n- [[#7174]](https://github.com/h2oai/h2o-3/issues/7174) - Added check to ensure that a model supports MOJOs to prevent production of bad MOJOs.\\n- [[#7179]](https://github.com/h2oai/h2o-3/issues/7179) - Fixed Python warnings before model training when training with offset, weights, and fold columns.\\n- [[#7181]](https://github.com/h2oai/h2o-3/issues/7181) - Fixed MOJO upload in Python.\\n- [[#7201]](https://github.com/h2oai/h2o-3/issues/7201) - Fixed error in uploading pandas DataFrame to H2O by enforcing `uft-8` encoding.\\n- [[#7273]](https://github.com/h2oai/h2o-3/issues/7273) - Customized FormAuthenticator to use relative redirects.\\n\\n\\n#### Improvement\\n\\n- [[#7141]](https://github.com/h2oai/h2o-3/issues/7141) - Removed numpy dependency for Infogram.\\n\\n#### New Feature\\n\\n- [[#7232]](https://github.com/h2oai/h2o-3/issues/7232) - Added backward selection method for ModelSelection. \\n\\n#### Task\\n\\n- [[#7123]](https://github.com/h2oai/h2o-3/issues/7123) - Added support to PredictCsv for testing concurrent predictions.\\n\\n#### Docs\\n\\n- [[#7136]](https://github.com/h2oai/h2o-3/issues/7136) -  Added backward mode documentation to ModelSelection.\\n- [[#7194]](https://github.com/h2oai/h2o-3/issues/7194) - Updated Kubernetes Headless Service and StatefulSet documentation.\\n\\n### Zorn (3.36.0.1) - 12/29/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zorn/1/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7214]](https://github.com/h2oai/h2o-3/issues/7214) - Fixed differences in H2Os random behavior across Java versions by disabling Stream API in this task. \\n- [[#7247]](https://github.com/h2oai/h2o-3/issues/7247) - Fixed CoxPH summary method in Python to return H2OTwoDimTable.\\n- [[#7273]](https://github.com/h2oai/h2o-3/issues/7273) - Fixed form authentication not working by enforcing relative redirects in Jetty.\\n- [[#7888]](https://github.com/h2oai/h2o-3/issues/7888) - Fixed exception raised in K-Means when a model is built using `nfolds` by disabling centroid stats for Cross-Validation.\\n\\n\\n#### Improvement\\n\\n- [[#7188]](https://github.com/h2oai/h2o-3/issues/7188) - Removed `ymu` and `rank` visibility from FlowUI.\\n- [[#7209]](https://github.com/h2oai/h2o-3/issues/7209) - Exposed `lambda` in Rulefit to have better control over regularization strength. \\n- [[#7217]](https://github.com/h2oai/h2o-3/issues/7217) - Implemented sequential replacement method with ModelSelection.\\n- [[#7222]](https://github.com/h2oai/h2o-3/issues/7222) - Improved rule extraction from trees in RuleFit.\\n- [[#7240]](https://github.com/h2oai/h2o-3/issues/7240) - Improved exception handling in AutoML and Grids to prevent model failure.\\n- [[#7395]](https://github.com/h2oai/h2o-3/issues/7395) - Ensured Infogram uses validation frame and cross-validation when enabled.\\n- [[#8096]](https://github.com/h2oai/h2o-3/issues/8096) - Added dynamic stacking metalearning strategy for Stacked Ensemble in AutoML.\\n\\n#### New Feature\\n\\n- [[#7246]](https://github.com/h2oai/h2o-3/issues/7246) - Added support and rule coverage to RuleFit.\\n- [[#7268]](https://github.com/h2oai/h2o-3/issues/7268) - Added support for importing GAM MOJO.\\n- [[#7279]](https://github.com/h2oai/h2o-3/issues/7279) - Added a convenience tool that converts MOJO to POJO from the command line.\\n- [[#7280]](https://github.com/h2oai/h2o-3/issues/7280) - Added support allowing users to modify floating point representation in POJO.\\n- [[#7287]](https://github.com/h2oai/h2o-3/issues/7287) - Added experimental support for importing POJO for in-H2O scoring.\\n- [[#7316]](https://github.com/h2oai/h2o-3/issues/7316) - Added official support for Java 16 and 17.\\n- [[#7323]](https://github.com/h2oai/h2o-3/issues/7323) - Added Java 16 and 17 to the cluster.\\n- [[#7333]](https://github.com/h2oai/h2o-3/issues/7333) - Added a compatibility K8s module that allows older versions of H2O to run on K8s.\\n- [[#7447]](https://github.com/h2oai/h2o-3/issues/7447) - Added ability to convert MOJO to POJO for tree models.\\n- [[#7515]](https://github.com/h2oai/h2o-3/issues/7515) - Added support enabling users to configure S3 with S3A configuration.\\n- [[#7574]](https://github.com/h2oai/h2o-3/issues/7574) - Implemented the Infogram model.\\n- [[#11818]](https://github.com/h2oai/h2o-3/issues/11818) - Implemented the Uplift DRF algorithm.\\n\\n#### Task\\n\\n- [[#7322]](https://github.com/h2oai/h2o-3/issues/7322) - Upgraded to Gradle 7 to support Java 16+.\\n- [[#7430]](https://github.com/h2oai/h2o-3/issues/7430) - Added R API for Infogram.\\n\\n#### Docs\\n\\n- [[#7212]](https://github.com/h2oai/h2o-3/issues/7212) - Added documentation on Infogram to the User Guide.\\n- [[#7279]](https://github.com/h2oai/h2o-3/issues/7279) - Added documentation on ModelSelection to the User Guide.\\n- [[#7275]](https://github.com/h2oai/h2o-3/issues/7275) - Added notebook on floating point issue for POJO and FAQ documentation on POJO split points.\\n- [[#7329]](https://github.com/h2oai/h2o-3/issues/7329) - Fixed bullet list formatting issues. \\n- [[#7742]](https://github.com/h2oai/h2o-3/issues/7742) - Updated R Reference Guide list.\\n\\n### Zizler (3.34.0.8) - 1/13/2022\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/8/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/8/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7148]](https://github.com/h2oai/h2o-3/issues/7148) - Fixed MOJO import automatically re-using original Model ID.\\n\\n#### Security\\n\\n- [[#7147]](https://github.com/h2oai/h2o-3/issues/7147) - Upgraded to log4j 2.17.1.\\n\\n### Zizler (3.34.0.7) - 12/21/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/7/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/7/index.html</a>\\n\\n#### Security\\n\\n- Fixed CVE-2021-45105 log4j vulnerability.\\n\\n### Zizler (3.34.0.6) - 12/15/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/6/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/6/index.html</a>\\n\\n#### Security\\n\\n- Fixed CVE-2021-45046 log4j vulnerability.\\n\\n### Zizler (3.34.0.5) - 12/13/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/5/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7213]](https://github.com/h2oai/h2o-3/issues/7213) - Fixed permutation variable importance to correctly work with weights.\\n- [[#7234]](https://github.com/h2oai/h2o-3/issues/7234) - Fixed data removal issue in GAM caused by fitting two different models on the same DataFrame.\\n\\n#### Improvement\\n\\n- [[#7233]](https://github.com/h2oai/h2o-3/issues/7233) - Added `coef()` and `coef_norm()` functions to MaxRGLM.\\n- [[#7251]](https://github.com/h2oai/h2o-3/issues/7251) - Added ability that labels observations that match rules in Rulefit.\\n- [[#7262]](https://github.com/h2oai/h2o-3/issues/7262) - Updated parquet parser to handle dates allowing H2O `import_file()` to import date columns from Spark DataFrame.\\n- [[#7276]](https://github.com/h2oai/h2o-3/issues/7276) - Consolidated Rulefit rules to remove unnecessary splits.\\n- [[#7439]](https://github.com/h2oai/h2o-3/issues/7439) -  Improved the efficiency of job polling in AutoML.\\n- [[#7474]](https://github.com/h2oai/h2o-3/issues/7474) - Deduplicated Rulefit rules in post-processing step.\\n\\n#### New Feature\\n\\n- [[#7235]](https://github.com/h2oai/h2o-3/issues/7235) - Added option to mimic the ActiveProcessorCount for older JVMs.\\n\\n#### Task\\n\\n- [[#7227]](https://github.com/h2oai/h2o-3/issues/7227) - Added warning in GLRM for when users set `model_id` and `representation_name` to the same string to help avoid a collision of model and frame using the same key.\\n- [[#7239]](https://github.com/h2oai/h2o-3/issues/7239) - Added `rank` and `ymu` model outputs to GLM.\\n\\n#### Docs\\n- [[#7210]](https://github.com/h2oai/h2o-3/issues/7210) - Added link to the Change Log in the User Guide index.\\n- [[#7218]](https://github.com/h2oai/h2o-3/issues/7218) - Updated parameter list for MaxRGLM and outlined that MaxRGLM only support regression.\\n- [[#7228]](https://github.com/h2oai/h2o-3/issues/7228) -  Updated MaxRGLM examples to use new functions `coef()`, `coef_norm()`, and `result()`.\\n- [[#7236]](https://github.com/h2oai/h2o-3/issues/7236) - Added examples in R/Python on how to get reproducibility information.\\n- [[#7296]](https://github.com/h2oai/h2o-3/issues/7296) - Fixed local build warnings for Python Module documentation.\\n\\n#### Security\\n- Upgraded to log4j 2.15.0 to address vulnerability CVE-2021-44228.\\n\\n\\n### Zizler (3.34.0.4) - 11/17/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/4/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7252]](https://github.com/h2oai/h2o-3/issues/7252) -  Fixed broken `weights_column` in GAM.\\n- [[#7256]](https://github.com/h2oai/h2o-3/issues/7256) - Fixed printing a DRF model when there are no out-of-bag samples.\\n- [[#7259]](https://github.com/h2oai/h2o-3/issues/7259) - Fixed the `pyunit_PUBDEV_5008_5386_glm_ordinal_large.py` test from failing.\\n- [[#7263]](https://github.com/h2oai/h2o-3/issues/7263) - Fixed AutoML XGBoost `learn_rate` search step.\\n- [[#7265]](https://github.com/h2oai/h2o-3/issues/7265) - Ensured that jobs are rendered correctly in Flow and that AutoML internal jobs can be monitored without crashing on the backend.\\n- [[#7269]](https://github.com/h2oai/h2o-3/issues/7269) - Fixed `gam_columns` failure in the `pyunit_PUBDEV_7185_GAM_mojo_ordinal.py` test. \\n- [[#7290]](https://github.com/h2oai/h2o-3/issues/7290) - Outlined that `tree_method=approx` is not supported with `col_sample_rate` or `col_sample_by_level` in XGBoost.\\n- [[#7517]](https://github.com/h2oai/h2o-3/issues/7517) - Fixed multinomial classification in Rulefit.\\n- [[#7681]](https://github.com/h2oai/h2o-3/issues/7681) - Fixed inconsistencies in GLM `beta_constraints`.\\n- [[#7738]](https://github.com/h2oai/h2o-3/issues/7738) - Enabled ability to provide metalearner parameters for NaiveBayes and XGBoost.\\n\\n#### Improvement\\n\\n- [[#7358]](https://github.com/h2oai/h2o-3/issues/7358) - Added a custom model ID parameter to MOJO importing/uploading methods through R/Python API and if a custom model ID is not specified, the default model ID is propagated as the models name from the MOJO path.\\n- [[#7361]](https://github.com/h2oai/h2o-3/issues/7361) - Added warning for users who accidentally build a regression model when attempting building a binary classification model because they forgot to convert their target to categorical.\\n- [[#7372]](https://github.com/h2oai/h2o-3/issues/7372) - Tuned `scale_pos_weight` parameter for XGBooost in AutoML for imbalanced data.\\n\\n#### New Feature\\n\\n- [[#7274]](https://github.com/h2oai/h2o-3/issues/7274) - Added saving parameters to plot functions.\\n\\n#### Task\\n\\n- [[#7250]](https://github.com/h2oai/h2o-3/issues/7250) - Added GAM training/validation metrics.\\n- [[#7264]](https://github.com/h2oai/h2o-3/issues/7264) - Ensured H2O-3 builds with pip version >= 21.3.\\n- [[#7311]](https://github.com/h2oai/h2o-3/issues/7311) - Added result frame to MAXRGLM.\\n\\n#### Docs\\n\\n- [[#7267]](https://github.com/h2oai/h2o-3/issues/7267) - Localized MOJO support list for all the H2O-3 algorithms.\\n- [[#7278]](https://github.com/h2oai/h2o-3/issues/7278) - Added Gains/Lift documentation to the Performance and Prediction section of the User Guide.\\n- [[#7288]](https://github.com/h2oai/h2o-3/issues/7288) - Corrected metric in the Performance and Prediction Sensitive to Outliers section of the User Guide.\\n- [[#7377]](https://github.com/h2oai/h2o-3/issues/7377) - Clarified that `asnumeric()` converted enum columns to underlying factor values and highlighted correct transformation approach.\\n\\n### Zizler (3.34.0.3) - 10/7/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/3/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7291]](https://github.com/h2oai/h2o-3/issues/7291) - Fixed user login from key tab in standalone on Kerberos.\\n- [[#7300]](https://github.com/h2oai/h2o-3/issues/7300) - Improved error messages in Explain module by making the errors clearer.\\n- [[#7307]](https://github.com/h2oai/h2o-3/issues/7307) - Fixed H2OTable colTypes in Grids summary table.\\n- [[#7308]](https://github.com/h2oai/h2o-3/issues/7308) - Fixed infinite loop in hex.grid.HyperSpaceWalker.RandomDiscreteValueWalker.\\n- [[#7317]](https://github.com/h2oai/h2o-3/issues/7317) - Fixed AutoML ignoring optional Stacked Ensembles.\\n- [[#7319]](https://github.com/h2oai/h2o-3/issues/7319) - Fixed NPE thrown in AutoML when XGBoost is disabled/not available.\\n- [[#7320]](https://github.com/h2oai/h2o-3/issues/7320) - Fixed CRAN install.\\n- [[#8458]](https://github.com/h2oai/h2o-3/issues/8458) - Improved XGBoost API to ensure both `col_sample_rate` and `colsample_bylevel` (and other XGBoost parameters aliases) are set correctly. \\n- [[#7375]](https://github.com/h2oai/h2o-3/issues/7375) - Fixed NPE thrown for `ModelJsonReader.findINJson` for cases when path does not exist.\\n\\n#### Improvement\\n\\n- [[#7314]](https://github.com/h2oai/h2o-3/issues/7314) - Exposed AutoML `get_leaderboard` as a method in Python.\\n- [[#7315]](https://github.com/h2oai/h2o-3/issues/7315) - Improved Python client by printing the stacktrace in case of ServerError allowing users to report informative issues for investigation.\\n- [[#7350]](https://github.com/h2oai/h2o-3/issues/7350) - Enhanced tests by testing the case through all encodings.\\n\\n#### Task\\n\\n- [[#7312]](https://github.com/h2oai/h2o-3/issues/7312) - Updated ANOVA GLM to save model summary as a frame.\\n- [[#7326]](https://github.com/h2oai/h2o-3/issues/7326) - Added GLM offset column support to GLM MOJO.\\n\\n#### Docs\\n\\n- [[#7302]](https://github.com/h2oai/h2o-3/issues/7302) - Updated the R/Python AutoML documentation parameters to match the descriptions in the User Guide.\\n- [[#7304]](https://github.com/h2oai/h2o-3/issues/7304) - Removed GLM from `balance_classes` parameter appendix page in the User Guide.\\n- [[#7309]](https://github.com/h2oai/h2o-3/issues/7309) - Updated the `asfactor` procedure documentation to show multiple column usage.\\n\\n\\n\\n### Zizler (3.34.0.1) - 9/14/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zizler/1/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7330]](https://github.com/h2oai/h2o-3/issues/7330) - Fixed matplotlib 3.4 compatibility issues with `partial_plot`.\\n- [[#7339]](https://github.com/h2oai/h2o-3/issues/7339) - Deprecated `is_supervised` parameter for h2o.grid method in R.\\n- [[#7341]](https://github.com/h2oai/h2o-3/issues/7341) - Fixed AutoML NPE by ensuring that models without metrics are not added to the leaderboard.\\n- [[#7360]](https://github.com/h2oai/h2o-3/issues/7360) - Redistributed the time budget for AutoML.\\n- [[#7365]](https://github.com/h2oai/h2o-3/issues/7365) - Fixed and reorganized the H2O Explain leaderboard and fixed the confusion matrix.\\n- [[#7366]](https://github.com/h2oai/h2o-3/issues/7366) - Decreased the number of displayed features in the heatmap for AutoML inside H2O Explain.\\n- [[#7378]](https://github.com/h2oai/h2o-3/issues/7378) - Fixed NPE raised from ``weight_column`` not being in the training model.\\n- [[#7380]](https://github.com/h2oai/h2o-3/issues/7380) - Fixed the ``weight=0`` documentation change error.\\n- [[#7383]](https://github.com/h2oai/h2o-3/issues/7383) - Fixed failing rotterdam tests.\\n- [[#7387]](https://github.com/h2oai/h2o-3/issues/7387) - Fixed GAM NPE from multiple runs with knots specified in a frame.\\n- [[#8458]](https://github.com/h2oai/h2o-3/issues/8458) - Fixed ``col_sample_rate`` not sampling for XGBoost when set to a value lower than 1.0.\\n- [[#7396]](https://github.com/h2oai/h2o-3/issues/7396) - Fixed wrong column type on MOJO models for Cross-Validation Metrics Summary. \\n- [[#7408]](https://github.com/h2oai/h2o-3/issues/7408) - Prevented R connect from starting H2O locally.\\n- [[#7420]](https://github.com/h2oai/h2o-3/issues/7420) - Added StackedEnsembles to AutoMLs time budget to prevent unexpected training times.\\n- [[#7441]](https://github.com/h2oai/h2o-3/issues/7441) - Fixed the failing `pyunit_scale_pca_rf.py` test.\\n- [[#7475]](https://github.com/h2oai/h2o-3/issues/7475) - Improved AutoML behavior when multiple instances are created in parallel.\\n- [[#7787]](https://github.com/h2oai/h2o-3/issues/7787) - Solved corner cases involving mapping between encoded varimps and predictor columns for H2O Explain by making the varimp feature consolidation more robust.\\n\\n#### Improvement\\n\\n- [[#7381]](https://github.com/h2oai/h2o-3/issues/7381) - Ensured that AutoML uses the entire time budget for `max_runtime`.\\n- [[#7455]](https://github.com/h2oai/h2o-3/issues/7455) - Implemented custom progress widgets for Wave apps using H2O-3.\\n- [[#7461]](https://github.com/h2oai/h2o-3/issues/7461) - Allowed users to convert floats to doubles with PrintMojo to prevent possible parsing issues.\\n- [[#7465]](https://github.com/h2oai/h2o-3/issues/7465) - Updated GBM cross validation with ``early_stopping`` to use ``ntrees`` that produce the best score.\\n- [[#7466]](https://github.com/h2oai/h2o-3/issues/7466) - Enabled ``print_mojo`` to produce .png outputs.\\n- [[#7470]](https://github.com/h2oai/h2o-3/issues/7470) - Updated Python API for all algorithms and AutoML to retrieve the trained model or leader.\\n- [[#7476]](https://github.com/h2oai/h2o-3/issues/7476) - Removed algorithm-specific logic from base classes.\\n- [[#7478]](https://github.com/h2oai/h2o-3/issues/7478) - Added support for scoreContributions for imported MOJOs in Java.\\n- [[#7480]](https://github.com/h2oai/h2o-3/issues/7480) - Exposed AutoML args as writeable properties until first called to train.\\n- [[#7482]](https://github.com/h2oai/h2o-3/issues/7482) - Updated XGBoost ``print_mojo`` to now output weights.\\n- [[#7498]](https://github.com/h2oai/h2o-3/issues/7498) - Removed the Python client dependency on colorama.\\n- [[#7504]](https://github.com/h2oai/h2o-3/issues/7504) - Added the parameters and their default values to the ``_init_`` function of the Py code generator.\\n- [[#7535]](https://github.com/h2oai/h2o-3/issues/7535) - Reduced the workspace of the validation frame in GBM by sharing it with the training frame in cross validation.\\n- [[#7564]](https://github.com/h2oai/h2o-3/issues/7564) - Slightly reduced precision of predictions stored in holdout frames to significantly save on memory.\\n- [[#7633]](https://github.com/h2oai/h2o-3/issues/7633) - Removed warning in the Stacked Ensemble prediction function about missing ``fold_column`` frame.\\n- [[#7690]](https://github.com/h2oai/h2o-3/issues/7690) - Enabled returning data from Explains `varimp_heatmap` and `model_correlation_matrix`.\\n- [[#7708]](https://github.com/h2oai/h2o-3/issues/7708) - Exposed the ``top n`` and ``bottom n`` reason codes in Python/R and MOJO.\\n- [[#12171]](https://github.com/h2oai/h2o-3/issues/12171) - Fixed nightly build version mismatch that prevented the H2OCluster timezone being set to America/Denver.\\n\\n\\n#### New Feature\\n\\n- [[#7336]](https://github.com/h2oai/h2o-3/issues/7336) - Implemented a java-self-check to allow users to run on latest Java.\\n- [[#7343]](https://github.com/h2oai/h2o-3/issues/7343) - Sped up GBM by optimizing the building of histograms.\\n- [[#7368]](https://github.com/h2oai/h2o-3/issues/7368) - Added a warning to the TreeSHAP reweighting feature if there are 0 weights and updated the API.\\n- [[#7418]](https://github.com/h2oai/h2o-3/issues/7418) - Added Maximum R Square Improvement (MAXR) algorithm to GLM.\\n- [[#7424]](https://github.com/h2oai/h2o-3/issues/7424) - Added warning for when H2O doesnt have enough memory to run XGBoost.\\n- [[#7431]](https://github.com/h2oai/h2o-3/issues/7431) - Added the ability to specify a custom file name when saving a MOJO.\\n- [[#7448]](https://github.com/h2oai/h2o-3/issues/7448) - Added output version number of genmodel.jar when printing usage for PrintMojo.\\n- [[#7536]](https://github.com/h2oai/h2o-3/issues/7536) - Added MOJO to Rulefit.\\n- [[#7550]](https://github.com/h2oai/h2o-3/issues/7550) - Implemented ability to calculate Shapley values on a re-weighted tree.\\n- [[#7561]](https://github.com/h2oai/h2o-3/issues/7561) - Implemented H2O ANOVA GLM algorithm for GLM.\\n- [[#8283]](https://github.com/h2oai/h2o-3/issues/8283) - Improved and consolidated the handling of version mismatch between Python and Backend.\\n- [[#8500]](https://github.com/h2oai/h2o-3/issues/8500) -  Implemented permutation feature importance for black-box models.\\n- [[#8501]](https://github.com/h2oai/h2o-3/issues/8501) - Implemented Extended Isolation Forest algorithm.\\n- [[#9260]](https://github.com/h2oai/h2o-3/issues/9260) - Added support for saving a model directly to S3.\\n\\n#### Task\\n\\n- [[#7363]](https://github.com/h2oai/h2o-3/issues/7363) - Fixed the time limits for the Merge/Sort benchmark.\\n- [[#7454]](https://github.com/h2oai/h2o-3/issues/7454) - Switched removed pandas ``as_matrix`` method to ``.values`` and exposed the interim `pandas.DataFrame` object.\\n- [[#7533]](https://github.com/h2oai/h2o-3/issues/7533) - Fixed S3 credential for `pyunit_s3_model_save.py` test. \\n- [[#7565]](https://github.com/h2oai/h2o-3/issues/7565) - Connected XGBoost aggregation functionality with sorting functionality.\\n\\n#### Technical task\\n\\n- [[#7449]](https://github.com/h2oai/h2o-3/issues/7449) - Replaced subsampling in Extended Isolation Forest.\\n\\n#### Docs\\n\\n- [[#7348]](https://github.com/h2oai/h2o-3/issues/7348) - Updated the AutoML FAQ.\\n- [[#7351]](https://github.com/h2oai/h2o-3/issues/7351) - Corrected the ``ignored_columns`` example.\\n- [[#7356]](https://github.com/h2oai/h2o-3/issues/7356) - Added RMarkdown, Jupyter Notebook, and HTML output example files to H2O Explain documentation.\\n- [[#7373]](https://github.com/h2oai/h2o-3/issues/7373) - Added Maximum R Improvements (MAXR) GLM documentation.\\n- [[#7392]](https://github.com/h2oai/h2o-3/issues/7392) - Added the loss function equations for each distribution and link type.\\n- [[#7405]](https://github.com/h2oai/h2o-3/issues/7405) - Updated the documentation about StackedEnsembles time constraints in AutoML.\\n- [[#7446]](https://github.com/h2oai/h2o-3/issues/7446) - Clarified that the Explain function only works for supervised models.\\n- [[#7471]](https://github.com/h2oai/h2o-3/issues/7471) - Added Examine Models section to AutoML documentation.\\n- [[#7484]](https://github.com/h2oai/h2o-3/issues/7484) - Added documentation for H2O ANOVA GLM algorithm.\\n- [[#7526]](https://github.com/h2oai/h2o-3/issues/7526) - Fixed the H2O Explain example in the documentation.\\n- [[#7596]](https://github.com/h2oai/h2o-3/issues/7596) - Updated and gathered Java links to a singular place in the User Guide.\\n\\n### Zipf (3.32.1.7) - 8/31/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/7/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/7/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7419]](https://github.com/h2oai/h2o-3/issues/7419) - Fixed predicting issues with imported MOJOs trained with an offset-column.\\n- [[#7406]](https://github.com/h2oai/h2o-3/issues/7406) - Fixed slow tree building by implementing a switch to turn off the generation of plain language rules.\\n- [[#7357]](https://github.com/h2oai/h2o-3/issues/7357) - Fixed potential NPE thrown by setting `_orig_projection_array=[]`.\\n- [[#7346]](https://github.com/h2oai/h2o-3/issues/7346) - Fixed generic model deserialization.\\n- [[#7345]](https://github.com/h2oai/h2o-3/issues/7345) - Fixed predictions for splits NA vs REST with monotone constraints.\\n\\n#### New Feature\\n\\n- [[#7362]](https://github.com/h2oai/h2o-3/issues/7362) - H2O Standalone now uses log4j2 as the logger implementation.\\n\\n\\n### Zipf (3.32.1.6) - 8/19/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/6/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/6/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7390]](https://github.com/h2oai/h2o-3/issues/7390) - Fixed the POJO mismatch from MOJO and in-H2O scoring for an unseen categorical value.\\n- [[#7393]](https://github.com/h2oai/h2o-3/issues/7393) - Simplified duplicated XGBoost parameters in Flow.\\n- [[#7414]](https://github.com/h2oai/h2o-3/issues/7414) - Fixed broken data frame conversion behavior.\\n\\n#### Improvement\\n\\n- Added security updates.\\n\\n#### New Feature\\n\\n- [[#7371]](https://github.com/h2oai/h2o-3/issues/7371) - Exposed the ``scale_pos_weight`` parameter in XGBoost.\\n\\n#### Task\\n\\n- [[#7412]](https://github.com/h2oai/h2o-3/issues/7412) - Clarified the anomaly score formula used for score calculation within Isolation Forest and Extended Isolation Forest.\\n\\n#### Docs\\n\\n- [[#7553]](https://github.com/h2oai/h2o-3/issues/7553) - Added a note on memory usage when using XGBoost to User Guide.\\n\\n### Zipf (3.32.1.5) - 8/4/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/5/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7399]](https://github.com/h2oai/h2o-3/issues/7399) - Modified legacy Dockerfile to add a non-root user.\\n- [[#7400]](https://github.com/h2oai/h2o-3/issues/7400) - Fixed an issue where running `java -jar h2o.jar -version` failed.\\n- [[#7403]](https://github.com/h2oai/h2o-3/issues/7403) - Fixed an issue where monotone constraints in GBM caused issues when reproducing the model.\\n- [[#7407]](https://github.com/h2oai/h2o-3/issues/7407) - Fixed an issue that caused DRF to create incorrect leaf nodes due to rounding errors.\\n- [[#7409]](https://github.com/h2oai/h2o-3/issues/7409) - Fixed an issue that caused CoxPH MOJO import to fail.\\n- [[#7411]](https://github.com/h2oai/h2o-3/issues/7411) - Fixed an issue where categorical splits NAvsREST were not represented correctly.\\n- [[#7413]](https://github.com/h2oai/h2o-3/issues/7413) - Fixed GBM reproducibility for correlated columns with NAs.\\n- [[#7416]](https://github.com/h2oai/h2o-3/issues/7416) - Fixed h2odriver so that it no longer uses invalid GC options.\\n- [[#7423]](https://github.com/h2oai/h2o-3/issues/7423) - Fixed GenericModel predictions for non-AUTO categorical encodings.\\n- [[#7434]](https://github.com/h2oai/h2o-3/issues/7434) - Fixed H2O interaction outcomes.\\n- [[#7460]](https://github.com/h2oai/h2o-3/issues/7460) - When `remove_collinear_columns=True`, fixed an issue where the dimension of gradient and coefficients changed when predictors were removed.\\n\\n#### Docs\\n\\n- [[#7415]](https://github.com/h2oai/h2o-3/issues/7415) - Updated changelog format.\\n\\n### Zipf (3.32.1.4) - 7/8/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/4/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7427]](https://github.com/h2oai/h2o-3/issues/7427) - Fixed h2odriver invalid argument error on Java 11.\\n- [[#7429]](https://github.com/h2oai/h2o-3/issues/7429) - Fixed GLM `GRADIENT_DESCENT_SQERR` Solver validation.\\n- [[#7433]](https://github.com/h2oai/h2o-3/issues/7433) - Upgraded to latest version of Javassist (3.28).\\n- [[#7444]](https://github.com/h2oai/h2o-3/issues/7444) - Fixed H statistic gpu assertion error.\\n- [[#7456]](https://github.com/h2oai/h2o-3/issues/7456) - Fixed predict contributions failure in multi-MOJO environments.\\n- [[#7457]](https://github.com/h2oai/h2o-3/issues/7457) - Fixed bug in ordinal GLM class predictions.\\n- [[#7462]](https://github.com/h2oai/h2o-3/issues/7462) - Fixed Partial Dependent Plot not working with Flow.\\n- [[#7469]](https://github.com/h2oai/h2o-3/issues/7469) - Updated to current Python syntax.\\n- [[#7483]](https://github.com/h2oai/h2o-3/issues/7483) - Fixed bug in ordinal GLM class predictions.\\n\\n#### Improvement\\n\\n- [[#7509]](https://github.com/h2oai/h2o-3/issues/7509) - Added support for refreshing HDFS delegation tokens for standalone H2O.\\n\\n\\n#### New Feature\\n\\n- [[#7540]](https://github.com/h2oai/h2o-3/issues/7540) - Obtained Friedmans H statistic for XGBoost and GBM.\\n\\n#### Task\\n\\n- [[#7500]](https://github.com/h2oai/h2o-3/issues/7500) - Added a warning message when using `alpha` as a hyperparameter for GLM\\n\\n#### Docs\\n\\n- [[#7492]](https://github.com/h2oai/h2o-3/issues/7492) - Added section on how to delete objects in Flow.\\n- [[#7499]](https://github.com/h2oai/h2o-3/issues/7499) - Added a note to the productionizing docs that C++ is only available with additional support.\\n\\n\\n### Zipf (3.32.1.3) - 5/19/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/3/index.html</a>\\n\\n#### Bug Fix\\n\\n- [[#7514]](https://github.com/h2oai/h2o-3/issues/7514) - Fixed the printing for `auc_pr` and `pr_auc` in cross-validation summaries.\\n\\n#### New Feature\\n\\n- [[#7519]](https://github.com/h2oai/h2o-3/issues/7519) - Added parameter `auc_type` to performance method to compute multiclass AUC.\\n\\n#### Task\\n\\n- [[#7503]](https://github.com/h2oai/h2o-3/issues/7503) - Upgraded XGBoost predictor to 0.3.18.\\n- [[#7505]](https://github.com/h2oai/h2o-3/issues/7505) - Increased the timeout duration on the R package jar download.\\n\\n#### Docs\\n\\n- [[#7530]](https://github.com/h2oai/h2o-3/issues/7530) - Fixed formatting errors for local builds.\\n- [[#7558]](https://github.com/h2oai/h2o-3/issues/7558) - Updated docs examples for baseline hazard, baseline survival, and concordance.\\n\\n### Zipf (3.32.1.2) - 4/29/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/2/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7800\\'>#7800</a>] - Stacked Ensemble will no longer ignore a column if any base model uses it.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7791\\'>#7791</a>] - Added a user-friendly reminder that the new explainability functions require newer versions of `ggplot2` in R.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7698\\'>#7698</a>] - NullPointerException error no longer thrown when used a saved and reloaded RuleFit model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7693\\'>#7693</a>] - Can now extract metrics from the validation dataset with a Rulefit Model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7573\\'>#7573</a>] - Fixed failures from Stacked Ensemble with Multinomial GLM within tests.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7572\\'>#7572</a>] - Fixed AutoML error when an alpha array is used for GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7570\\'>#7570</a>] - Fixed Rollup not possible\" stats failure in GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7552\\'>#7552</a>] - H2O will now still start despite system properties that begin with ai.h2o..\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7551\\'>#7551</a>] - H2O exits without logging any buffered messages instead of throwing a NullPointerException when starting H2O with an invalid argument.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7549\\'>#7549</a>] - ModelDescriptor field in MOJO is now Serializable.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7547\\'>#7547</a>] - AutoML no longer crashes if model builder produces H2OIllegalArgumentException in the parameter validation phase.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7543\\'>#7543</a>] - Weights in GLM grid search is no longer used as features.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7529\\'>#7529</a>] - Fixed Stacked Ensemble MOJO for cases when sub-model doesnt have the same columns as the metalearner.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7524\\'>#7524</a>] - Efron-method now fully deterministic in CoxPH.\\n</li>\\n</ul>\\n\\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7562\\'>#7562</a>] - User now allowed to specify the escape character for parsing CSVs.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7557\\'>#7557</a>] - Added H2O reconnection script for intermittent 401 errors to R.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7548\\'>#7548</a>] - Added ice_root error documented in FAQ.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7531\\'>#7531</a>] - Added further regularization to the GLM metalearner.\\n</li>\\n</ul>\\n\\n#### New Feature\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9372\\'>#9372</a>] - Warning now issued against irreproducible model when early stopping is enabled but neither `score_tree_interva`l or `score_each_iteration` are defined.\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7625\\'>#7625</a>] - Encrypted files that contain CSVs can now be imported.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7577\\'>#7577</a>] - Added guidelines for correct use of `remove_collinear_columns` for GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7538\\'>#7538</a>] - Support added for CDP 7.2.\\n</li>\\n</ul>\\n\\n#### Docs\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7582\\'>#7582</a>] - Added information about the `path` argument for exporting .xlsx files.\\n</li>\\n</ul>\\n\\n### Zipf (3.32.1.1) - 3/25/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zipf/1/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9268\\'>#9268</a>] -         GBM histograms now ignore rows with NA responses.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8032\\'>#8032</a>] -         Variable Importances added to GLM Generic model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7859\\'>#7859</a>] -         Fixed the ArrayIndexOutOfBoundsException issue with GLM CV.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7816\\'>#7816</a>] -         CoxPH performance no longer fails when a factor is used for the `event_column`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7801\\'>#7801</a>] -         Existing frame no longer overwritten when data with the same query is loaded.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7736\\'>#7736</a>] -         Fixed how `gain` is calculated in XGBFI for GBM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7711\\'>#7711</a>] -         Improved the error messages for `save_to_hive_table`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7685\\'>#7685</a>] -         Added missing argument test for `h2o.explain_row()`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7665\\'>#7665</a>] -         All trees now supported for XGBoost Print MOJO in Java.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7657\\'>#7657</a>] -         CoxPH `prediction` no longer fails when `offset_column` is specified.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7678\\'>#7678</a>] -         Added keys for Individual Conditional Expectation (ICE) plot in H2OExplanation class.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7635\\'>#7635</a>] -         `model@model$parameters$x` now reports actual feature names instead of `names`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7632\\'>#7632</a>] -         `h2o.explain` no longer errors when AutoML object is trained with a `fold_column`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7603\\'>#7603</a>] -         Fixed issues with pythons explanation plots not displaying fully.\\n</li>\\n</ul>\\n    \\n#### New Feature\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7933\\'>#7933</a>] -         Ignored columns that are actually used for model training are unignored and no longer prevent model training to start in Flow.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7904\\'>#7904</a>] -         Added baseline hazard function estimate to CoxPH model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7891\\'>#7891</a>] -         Target Encoding now supports feature interactions.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7837\\'>#7837</a>] -         Added CoxPH concordance to both Flow and R/Python CoxPH summaries.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7821\\'>#7821</a>] -         Added a `topbasemodel` attribute to AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7811\\'>#7811</a>] -         Added new learning curve plotting function to R/Python.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7788\\'>#7788</a>] -         Added script for estimating the memory usage of a dataset.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7784\\'>#7784</a>] -         Added fault protections to grid search allowing saving of data and parameters, model checkpointing, and auto-recovery.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7761\\'>#7761</a>] -         Added support for Java 15.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7673\\'>#7673</a>] -         Added CDP7.1 support.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7666\\'>#7666</a>] -         Added support for XGBoost to Print MOJO as JSON.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7627\\'>#7627</a>] -         Added support for refreshing HDFS delegation tokens.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7613\\'>#7613</a>] -         Reverted XGBoost categorical encodings for contributions.\\n</li>\\n</ul>\\n    \\n#### Task\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8002\\'>#8002</a>] -         `max_hit_ratio_k` deprecated and removed.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7751\\'>#7751</a>] -         Added upper bound cap to supported Java version in H2O CRAN package requirements.\\n</li>\\n</ul>\\n    \\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8165\\'>#8165</a>] -         Users now allowed to include categorical column name in beta constraints.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8059\\'>#8059</a>] -         Multinomial PDP can now be plotted for more than one target class in Flow.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7903\\'>#7903</a>] -          Sped up CoxPH concordance score by using tree instead of the direct approach.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7822\\'>#7822</a>] -         XGBoost no longer fails when specifying custom `fold_column`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7799\\'>#7799</a>] -         XGBoost CV models now built on multiple GPUs in parallel.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8459\\'>#8459</a>] -         Missing metrics added to GLM scoring history.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7631\\'>#7631</a>] -         Added validation checks for sampling rates for XGBoost for the R/Python clients.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7624\\'>#7624</a>] -         \\nNo longer errors when trying to use a fold column where not all folds are represented.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7616\\'>#7616</a>] -         Added the `metalearner_transform` option to Stacked Ensemble. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7592\\'>#7592</a>] -         GBM main model now built in parallel to the CV models.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7589\\'>#7589</a>] -         Removed redundant extraction weights from GBM/DRF histogram.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7588\\'>#7588</a>] -         GBM now avoids scoring the last iteration twice when early stopping is enabled.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7586\\'>#7586</a>] -         POJO predictions for XGBoost now even closer to in-H2O predictions.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7585\\'>#7585</a>] -         Double-scoring of CV models in AutoML now avoided thus speeding up AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7579\\'>#7579</a>] -         AutoML now uses fewer neurons in DL grids and has improved the metalearner for Stacked Ensemble.\\n</li>\\n</ul>\\n    \\n####Technical task\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7783\\'>#7783</a>] -         Thin plate regression splines added to GAM.\\n</li>\\n</ul>\\n    \\n#### Docs\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7728\\'>#7728</a>] -         Added checkpoint description to GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7680\\'>#7680</a>] -         Added thin plate regression spline documentation to GAM algorithm page.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7656\\'>#7656</a>] -         Added missing parameters to XGBoost algorithm page.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7652\\'>#7652</a>] -         Added more information about log files to User Guide.\\n</li>\\n</ul>\\n\\n\\n### Zermelo (3.32.0.5) - 3/16/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/5/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7844\\'>#7844</a>] -         GAM no longer creates multiple knots at the same coordinates when the cardinality of the `gam_columns` is less than the number of `knots` specified by the user.\\n</li>\\n</ul>\\n    \\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7694\\'>#7694</a>] -         \\nFeature interactions can now be save as .xlxs files. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7614\\'>#7614</a>] -         Job polling will retry connecting to h2o nodes if connection fails.\\n</li>\\n</ul>\\n\\n### Zermelo (3.32.0.4) - 2/1/2021\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/4/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7697\\'>#7697</a>] -         Partial Dependence Plot no longer failing for High Cardinality even when `user_splits` is defined.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7695\\'>#7695</a>] -         Fixed failing Delta Lake import for Python API.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7686\\'>#7686</a>] -         Fix Stacked Ensembles incorrect handling of fold column.\\n</li>\\n</ul>\\n\\n    \\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7902\\'>#7902</a>] -         Added MOJO support for CoxPH.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7675\\'>#7675</a>] -         Escape all quotes by default when writing CSV.\\n</li>\\n</ul>\\n    \\n#### Docs\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7701\\'>#7701</a>] -         Added to docs that AUCPR can be plotted.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7684\\'>#7684</a>] -         Updated the Customer Algorithm graphic for the Architecture section of the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7661\\'>#7661</a>] -         Updated the copyright year to 2021.\\n</li>\\n</ul>\\n\\n\\n### Zermelo (3.32.0.3) - 12/24/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/3/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7868\\'>#7868</a>] -         The `pca_impl` parameter is no longer passed to PCA MOJO.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7749\\'>#7749</a>] -         Objects to be retained no longer removed during the `h2o.removeAll()` command.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7743\\'>#7743</a>] -         Starting GridSearch in a fresh cluster with new hyperparameters that overlap old ones will no longer cause the old models to be trained again.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7731\\'>#7731</a>] -         GridSearch no longer hangs indefinitely when not using the default value for paralellism.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7724\\'>#7724</a>] -         Fixed the parent dir lookup for HDFS grid imports.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7717\\'>#7717</a>] -         Fixed the CustomDistribution test error.\\n</li>\\n</ul>\\n    \\n#### New Feature\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12775\\'>#12775</a>] -         Cross-Validation predictions can now be saved alongside the model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8366\\'>#8366</a>] -         Added multinomial and grid search support for AUC/PR AUC metrics.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7782\\'>#7782</a>] -         Now offers a standalone R client that doesnt include the h2o jar.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7773\\'>#7773</a>] -         Created a Red Hat certification for H2O Docker Image.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7764\\'>#7764</a>] -         Fixed randomized split points for `histogram_type=Random` when nbins=2.       \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7729\\'>#7729</a>] -         Single quote regime for CSV parser exposed for importing & uploading files.\\n</li>\\n</ul>\\n    \\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7887\\'>#7887</a>] -         REST API disabled on non-leader Kubernetes nodes.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7769\\'>#7769</a>] -         GLM now uses proper logging instead of printlines.\\n</li>\\n</ul>\\n    \\n#### Docs\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7819\\'>#7819</a>] -         \\nAdded non-tree-based models to the variable importance page in the user guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7775\\'>#7775</a>] -         Updated the AutoML citation in the User Guide to point to the H2O AutoML ICML AutoML workshop paper.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7762\\'>#7762</a>] -         Updated Python docstring examples about cross-validation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7740\\'>#7740</a>] -         Corrected `k` parameter description for PCA.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7723\\'>#7723</a>] -         Corrected the RuleFit Python example.\\n</li>\\n</ul>\\n\\n\\n### Zermelo (3.32.0.2) - 11/17/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/2/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7849\\'>#7849</a>] -         Implemented deserialization of monotone constraints.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7798\\'>#7798</a>] -         Updated required version of ggplot2 in R package to 3.3.0.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7778\\'>#7778</a>] -         Fixed the parsing of GLMs `rand_family` params in MOJO JSON.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7768\\'>#7768</a>] -         Fixed NPE that resulted when starting a grid with SequentialWalker in AutoML exploitation phase.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7765\\'>#7765</a>] -         Fixed MOJO version check message.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7759\\'>#7759</a>] -         When grid search has parallelism enabled, it now includes CV models.\\n</li>\\n</ul>\\n    \\n#### New Feature\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7900\\'>#7900</a>] -         Added feature interactions and importance for XGBoost and GBM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7867\\'>#7867</a>] -         Added new `interaction_constraints` parameter to XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7804\\'>#7804</a>] -         Added an option to not have quotes in the header during exportFile.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7758\\'>#7758</a>] -         Added ability to retrieve a list of all the models in an H2O cluster.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7745\\'>#7745</a>] -         Added custom pod labels for HELM charts.\\n</li>\\n</ul>\\n    \\n#### Task\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7807\\'>#7807</a>] -         Added `lambda_min` & `lambda_max` parameters to GLMModelOutputs.\\n</li>\\n</ul>\\n    \\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7894\\'>#7894</a>] -         Added default values to all algorithm parameters in the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7890\\'>#7890</a>] -         Fixed the discrepancies between the Target Encoding User Guide page and Client.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7808\\'>#7808</a>] -         Added ONNX support to the documentation.\\n</li>\\n</ul>\\n    \\n#### Engineering Story\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7796\\'>#7796</a>] -         Added a new method which properly locks H2O Frames during conversion from Spark Data Frames to H2O Frames in Sparkling Water.\\n</li>\\n</ul>\\n    \\n#### Docs\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7806\\'>#7806</a>] -         On the Grid Search User Guide page, fixed the missing syntax highlight in the Python example of the Random Grid Search section.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7805\\'>#7805</a>] -         Added `rule_generation_ntrees` parameter to the RuleFit page.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7767\\'>#7767</a>] -         Added documentation for GBM and XGBoost on feature interactions and importance.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7757\\'>#7757</a>] -         Added a Python example to the `stratify_by` parameter. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7747\\'>#7747</a>] -         Added a Feature Engineering section to the Data Manipulation page in the User Guide.\\n</li>\\n</ul>\\n\\n\\n### Zermelo (3.32.0.1) - 10/8/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zermelo/1/index.html</a>\\n\\n#### Bug Fix\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7972\\'>#7972</a>] -         Fixed StackedEnsembles retrieval of the seed parameter value.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7893\\'>#7893</a>] -         Deserialization values of MOJO ModelParameter now work when the Value Type is int[].\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7881\\'>#7881</a>] -         H2O no longer uses lazy-loading for sequential zip parse.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7879\\'>#7879</a>] -         Updated model_type argument names for Rulefit in R.\\n</li>\\n</ul>\\n    \\n#### New Feature\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8393\\'>#8393</a>] -         Quantile distributions added to monotone constraints.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8318\\'>#8318</a>] -          TargetEncoder integrated into ModelBuilder.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7885\\'>#7885</a>] -         Python client no longer instructs the user to declare a root handler in library mode.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7851\\'>#7851</a>] -         Hostname used as certificate alias to lookup machine-specific certificate allowing Hadoop users to connect to Flow over HTTPS.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7846\\'>#7846</a>] -         Added the model explainability interface for H2O models and AutoML objects in both R & Python.\\n</li>\\n<li>\\n[<a href=\\'https://github.com/h2oai/h2o-3/issues/7919\\'>#7919</a>] -         Added the RuleFit algorithm for interpretability.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7834\\'>#7834</a>] -         Implemented a basic HELM chart.\\n</li>\\n</ul>\\n    \\n#### Task\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7878\\'>#7878</a>] -         Rulefit model added to algorithm section of UserGuide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7855\\'>#7855</a>] -         Added an Explainability page to the User Guide outlining the new `h2o.explain()` and `h2o.explain_row()` functions.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7838\\'>#7838</a>] -         Updated the AutoML User Guide page to include the new Explainability and Preprocessing sections.\\n</li>\\n</ul>\\n    \\n#### Improvement\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12783\\'>#12783</a>] -         Added support for Python 3.7+.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7922\\'>#7922</a>] -         Exposes names of score0 output values in MOJO.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7909\\'>#7909</a>] -         Added function to plot a Precision Recall Curve.\\n</li>\\n<li>\\n[<a href=\\'https://github.com/h2oai/h2o-3/issues/7899\\'>#7899</a>] -         RuleFit model represented by the set of rules obtained from trees during training.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7876\\'>#7876</a>] -         Performance improved for exporting a Frame to CSV.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7872\\'>#7872</a>] -         GPU backend allowed in XGBoost when running multinode even when `build_tree_one_node` is enabled.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7863\\'>#7863</a>] -         Updated all URLs in R package to use HTTPS.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7852\\'>#7852</a>] -         Upgraded to XGBoost 1.2.0.\\n</li>\\n</ul>\\n    \\n#### Technical task\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8271\\'>#8271</a>] -         Added cross-validation to GAM allowing users to find the best alpha/lambda values when building a GAM model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7967\\'>#7967</a>] -         Added TargetEncoder support for multiclass problems.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7896\\'>#7896</a>] -         Added new TargetEncoder parameter that allows users to remove original features automatically.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7862\\'>#7862</a>] -          Implemented minimal support for TargetEncoding in AutoML.\\n</li>\\n</ul>\\n    \\n#### Docs\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8097\\'>#8097</a>] -          Updated the descriptions of AutoML in R & Python packages.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7860\\'>#7860</a>] -         Made the default for `categorical_encoding` in XGBoost explicit in the documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7831\\'>#7831</a>] -         Updated the import datatype section of the Python FAQ in the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7826\\'>#7826</a>] -         Updated the default values for `min_rule_length` and `max_rule_length` on the RuleFit page of the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7825\\'>#7825</a>] -         Updated the `validation_frame` definition for unsupervised algorithms in the User Guide.\\n</li>\\n</ul>\\n\\n### Zeno (3.30.1.3) - 9/28/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/3/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7861\\'>#7861</a>] -         CRAN - Use HTTPS for all downloads within the R package.\\n</li>\\n</ul>\\n\\n\\n### Zeno (3.30.1.2) - 9/3/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/2/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/11601\\'>#11601</a>] -         The h2o.unique() command will now only return the unique values within a column.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8012\\'>#8012</a>] -         k-LIME easy predict wrapper now uses Regression or KLime as a model category instead of just KLime.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7982\\'>#7982</a>] -         Fixed the CRAN check warnings on r-devel for cross-references in the R documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7935\\'>#7935</a>] -         Documentation added detailing the supported encodings for CSV files.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7931\\'>#7931</a>] -         GLM parameters integrated into GAM parameters.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7901\\'>#7901</a>] -         Fixed broken URLs in R documentation that caused CRAN failures.\\n</li>\\n</ul>\\n    \\n<h4>New Feature</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8017\\'>#8017</a>] -         Added the concordance statistic for CoxPH models.\\n</li>\\n</ul>\\n    \\n<h4>Task</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8157\\'>#8157</a>] -         When using multiple alpha/lambda values for calling GLM from GAM, GLM now returns the best results across all alpha/lambda values. Also added the cold_start parameter added to GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7918\\'>#7918</a>] -         Added documentation for new GAM hyperparameter subspaces.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7913\\'>#7913</a>] -         GLM new parameter cold_start added to User Guide and GLM booklet.\\n</li>\\n</ul>\\n    \\n<h4>Improvement</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7985\\'>#7985</a>] -         Reduced the memory cost of the `drop_duplicate` operation by cleaning up data early.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7916\\'>#7916</a>] -          When calculating unique() values on a column that is the result of an AstRowSlice operation, the domain is now collected in-place and no longer results in an error.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7908\\'>#7908</a>] -         Categorical encoding documentation updated by adding EnumLimited & SortByReponse to KMeans and removing Eigen from XGBoost.\\n</li>\\n</ul>\\n    \\n<h4>Technical task</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8270\\'>#8270</a>] -         Tests added to verify grid search functionality for GAM and allows the user to create more complex hyper spaces for grid search by adding subspaces key and functionality to grid search backend.\\n</li>\\n</ul>\\n    \\n<h4>Docs</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7906\\'>#7906</a>] -         Added documentation on how to retrieve reproducibility information.\\n</li>\\n</ul>\\n\\n\\n### Zeno (3.30.1.1) - 8/10/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zeno/1/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8521\\'>#8521</a>] -         H2OFrames with fields containing double quotes/line breaks can now be converted to Pandas dataframe. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8149\\'>#8149</a>] -         Impossible to set Max_depth to unlimited on DRF classifer\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8004\\'>#8004</a>] -         Model generation for MOJO/POJO are disabled when interaction columns are used in GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7993\\'>#7993</a>] -         Reproducibility Information Table now hidden in H2O-Flow.\\n</li>\\n</ul>\\n    \\n<h4>New Feature</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/11793\\'>#11793</a>] -         Added support for `offset_column` in the Stacked Ensemble metalearner.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/11794\\'>#11794</a>] -         Added support for `weights_column` in the Stacked Ensemble metalearner.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8826\\'>#8826</a>] -         Added continued support to Generalized Additive Models for H2O.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8397\\'>#8397</a>] -         The value of model parameters can be retrieved at the end of training, allowing users to retrieve an automatically chosen value when a parameter is set to AUTO.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8353\\'>#8353</a>] -         H2O Frame is now able to be saved into a Hive table.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8171\\'>#8171</a>] -         XGBoost can now be executed on an external Hadoop cluster.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7999\\'>#7999</a>] -         Added the `contamination` parameter to Isolation Forest which is used to mark anomalous observations.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7998\\'>#7998</a>] -         Introduced the `validation_response_column` parameter for Isolation Forest which allows users to name the response column in the validation frame.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7992\\'>#7992</a>] -         Added official support for Java 14 in H2O.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7942\\'>#7942</a>] -         Added external cluster startup timeout for XGBoost.\\n</li>\\n</ul>\\n    \\n<h4>Task</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7990\\'>#7990</a>] -         Hadoop Docker image run independent of S3.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7966\\'>#7966</a>] -         Upgraded the build/test environment to support R 4.0 and Roxygen2.7.1.1.\\n</li>\\n</ul>\\n    \\n<h4>Improvement</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8698\\'>#8698</a>] -         Implemented TF-IDF algorithm to reflect how important a word is to a document or collection of documents.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8691\\'>#8691</a>] -         GridSearch R API test added for Isolation Forest.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8193\\'>#8193</a>] -         AUTO option added for GLM & GAM family parameter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8142\\'>#8142</a>] -         XGBoost Variable Importances now computed using a Java predictor.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8091\\'>#8091</a>] -         StackedEnsemble can now be created using only monotone models if user specifies `monotone_constraints` in AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8071\\'>#8071</a>] -         Enabled using imported models as base models in Stacked Ensembles.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7988\\'>#7988</a>] -         Removed deprecated H2O-Scala module.\\n</li>\\n</ul>\\n    \\n<h4>Technical Task</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8447\\'>#8447</a>] -          Added Java backend to support MOJO in GAM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8028\\'>#8028</a>] -         Added support for `early_stopping` parameter in GAM and GLM.\\n</li>\\n</ul>\\n    \\n<h4>        Engineering Story\\n</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7938\\'>#7938</a>] -          Sparkling Water Booklet removed from the H2O-3 repository.\\n</li>\\n</ul>\\n    \\n<h4>Docs</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8082\\'>#8082</a>] -         Added H2O Client chapter to the User Guide which includes section on Sklearn integration.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8000\\'>#8000</a>] -         Added documentation in the Isolation Forest section for the `contamination` parameter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7991\\'>#7991</a>] -          Added documentation in GLM & GAM, and the `family` & `link` algorithm parameters to include how `family` can now be set equal to AUTO.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7984\\'>#7984</a>] -         Added `gains lift_bins` to the parameter appendix and added and example to the parameter in the Python documentation. Added an example for the Kolmogorov-Smirnov metric to the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7983\\'>#7983</a>] -         Updated GAM and GLM documentation to include support for `early_stopping`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7978\\'>#7978</a>] -         Added the Kolmogorov-Smirnov metric formula to the Performance and Prediction chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7960\\'>#7960</a>] -         Added the `negativebinomial` value to the `family` parameter page.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7959\\'>#7959</a>] -         Added the `ordinal` and `modified_huber` values to the `distribution` parameter page.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7957\\'>#7957</a>] -         Updated deprecated parameter `loading_name` to `representation_name` and fixed the broken init link in the GLRM section of the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7955\\'>#7955</a>] -         Added a note in the User Guide Stacked Ensemble section about building a monotonic Stacked Ensemble.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7940\\'>#7940</a>] -         Added documentation for how `balance_classes` is triggered.\\n</li>\\n</ul>\\n\\n\\n### Zahradnik (3.30.0.7) - 7/21/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/7/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/7/index.html</a>\\n\\n<h4>New Feature</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8207\\'>#8207</a>] -         Added support for partitionBy column in partitioned parquet or CSV files. \\n</li>\\n</ul>\\n    \\n<h4>Task</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7994\\'>#7994</a>] -         Warning added for user if both a lamba value and lambda search are provided in GLM.\\n</li>\\n</ul>\\n    \\n<h4>Improvement</h4>\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12662\\'>#12662</a>] - Added `max_runtime_secs` parameter to Stacked Ensemble.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/private-h2o-3/issues/28\\'>private-#28</a>] - Upgraded Jetty 9 and switched default webserver to Jetty 9.\\n</li>\\n</ul>\\n\\n\\n### Zahradnik (3.30.0.6) - 6/30/2020 \\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/6/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/6/index.html</a>\\n            \\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8009\\'>#8009</a>] - GLM Plug values are now propagated to MOJOs/POJOs.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8008\\'>#8008</a>] - In the Python documentation, the HGLM example now references `random_columns` by indices rather than by column name.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/7997\\'>#7997</a>] - Fixed a link to H2O blogs in the R documentation.\\n</li>\\n</ul>\\n            \\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8233\\'>#8233</a>] - Added support for the Kolmogorov-Smirnov metric for binary classification models.\\n</li>\\n</ul>\\n                                                                                                                                                                                                                                                                                                \\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8014\\'>#8014</a>] - Added documentation in the Performance and Prediction chapter for the Kolmogorov-Smirnov metric.\\n</li>\\n</ul>\\n\\n### Zahradnik (3.30.0.5) - 6/18/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/5/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/5/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8308\\'>#8308</a>] - Fixed an issue that denied all requests to display H2O Flow in an iframe.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8075\\'>#8075</a>] - Importing with `use_temp_table=False` now works correctly on Teradata.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8050\\'>#8050</a>] - Building a GLM model with `interactions` and `lambda = 0` no longer produces a \"Categorical value out of bounds\" error. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8048\\'>#8048</a>] - Fixed an inconsistency that occurred when using `predict_leaf_node_assignment` with a path and with a terminal node. For trees with a max_depth of up to 63, the results now match. For max_depth of 64 or higher (for path and nodes that are \"too deep\"), H2O will no longer produce incorrect results. Instead it will return \"NA\" for tree paths and \"-1\" for node IDs. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8042\\'>#8042</a>] - Leaf node assignment now works correctly for trees with a depth >= 31. Note that for trees with a max_depth of 64 or higher, H2O will return \"NA\" for tree paths and \"-1\" for node IDs. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8039\\'>#8039</a>] - `allow_insecure_xgboost` now works correctly on Hadoop.\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8206\\'>#8206</a>] - HTML documentation is now available as a downloadable zip file.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8037\\'>#8037</a>] - Users can now retrieve the prediction contributions when running `mojo_predict_pandas` in Python. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8025\\'>#8025</a>] - H2O documentation is now available in an h2odriver distribution zip file. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8024\\'>#8024</a>] - Quantiles models during the training of other models are now recognized as a regular model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8018\\'>#8018</a>] - The H2O-SCALA module is deprecated and will be removed in a future release.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9202\\'>#9202</a>] - Added support for models built with any `family` when running makeGLMModel.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8052\\'>#8052</a>] - K8S Docker images for h2o-3 are now available. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8023\\'>#8023</a>] - Warnings are now produced during model building when using the Python client.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8495\\'>#8495</a>] - Added examples for saving and loading grids in the User Guide. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8051\\'>#8051</a>] - Improved the examples in the Performance and Prediction chapter. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8049\\'>#8049</a>] - In the AutoML Random Grid Search Parameters topic, removed the no-longer-supported `min_sum_hessian_in_leaf` parameter from the XGBoost table. Also added clarification on how GHL models are handled in an AutoML random grid search run.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8035\\'>#8035</a>] - In the Python documentation, add examples for Grid Metrics.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8013\\'>#8013</a>] - The value of T as described in the description for `categorical_encoding=\"enum_limited\"` is 10, not 1024.\\n</li>\\n</ul>\\n\\n\\n### Zahradnik (3.30.0.4) - 6/1/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/4/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8275\\'>#8275</a>] - h2o.merge() now works correctly when you joining an H2O frame where the join is on a <dbl> column to another frame.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8184\\'>#8184</a>] - Fixed an issue that caused h2o.get_leaderboard to fail after creating an AutoML object, disconnecting the client, starting a new session, and then reconecting to the running H2O cluster for the re-attached H2OAutoML object.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8147\\'>#8147</a>] - Stacked Ensemble now inherits distributions/families supported by the metalearner. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8137\\'>#8137</a>] - Fixed an issue that caused AutoML to fail when the target included special characters.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8073\\'>#8073</a>] - CAcert is now supported with the Python API.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8069\\'>#8069</a>] - Water Meter and Form Login now work correctly.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8066\\'>#8066</a>] - In Aggregator, added support for retrieving the Mappings Frame.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8056\\'>#8056</a>] - Added support for using monotone constraints with Tweedie distribution in GBM.\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/10207\\'>#10207</a>] - Added a new drop_duplicates function to drop duplicate observations from an H2O frame.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9371\\'>#9371</a>] - Partial dependence plots are now available for multiclass problems.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8134\\'>#8134</a>] - Users now receive a warning if they try to get variable importances in Stacked Ensemble.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8111\\'>#8111</a>] - In XGBoost, removed the min_sum_hessian_in_leaf and min_data_in_leaf options, which are no longer supported by XGBoost. Also added the `colsample_bynode` option.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8089\\'>#8089</a>] - data.table warning messages are now suppressed inside h2o.automl() in R.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8120\\'>#8120</a>] - Added a \"Training Models\" section to the User Guide, which describes train() and train_segments(). \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8113\\'>#8113</a>] - Updated XGBoost to indicate that this version requires CUDA 9, and included information showing users how to check their CUDA version.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8112\\'>#8112</a>] - Added information about GAM support to the missing_values_handling parameter appendix entry.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8107\\'>#8107</a>] - Updated the Minio Instance topic.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8064\\'>#8064</a>] - `monotone_constraints` can now be used with `distribution=tweedie`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8062\\'>#8062</a>] - Updated the PDP topic to include support for multinomial problems and updated the examples. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8053\\'>#8053</a>] - In the API-related Changes topic, noted that `min_sum_hessian_in_leaf` and `min_data_in_leaf`  are no longer supported in XGBoost. \\n</li>\\n</ul>\\n\\n\\n### Zahradnik (3.30.0.3) - 5/12/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/3/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8146\\'>#8146</a>] - Improved validation and error messages for CoxPH.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8140\\'>#8140</a>] - In XGBoost, the `predict_leaf_node_assignment` parameter now works correctly with multiclass.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8121\\'>#8121</a>] - Fixed an issue that caused GBM to fail when it encountered a bin that included a single value and the rest NAs.\\n</li>\\n</ul>\\n    \\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8537\\'>#8537</a>] - Updated the AutoML example in the R package.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8199\\'>#8199</a>] - PDPs now allow y-axis scaling options.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8175\\'>#8175</a>] - Improved speed for training and prediction of Stacked Ensembles.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12851\\'>#12851</a>] - Added tables showing parameter values and random grid space ranges to the AutoML chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8294\\'>#8294</a>] - Improved the Hive import documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8138\\'>#8138</a>] - Improved documentation for Quantiles in the User Guide. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8133\\'>#8133</a>] - Fixed the documented default value for `min_split_improvement` parameter in XGBoost.\\n</li>\\n</ul>\\n\\n\\n\\n### Zahradnik (3.30.0.2) - 4/28/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/2/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8237\\'>#8237</a>] - Fixed an issue that caused H2O to crash while debugging Python code using intellij/pycharm.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8211\\'>#8211</a>] - Fixed an issue that caused an assertion error while running Grid Search.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8203\\'>#8203</a>] - Training of a model based on a data frame that includes Target Encodings no longer fails due to a locked frame.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8198\\'>#8198</a>] - Added train_segments() to the R html documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8196\\'>#8196</a>] - Target Encoder now unlocks the output frame.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8182\\'>#8182</a>] - Fixed the BiasTerm in XGBoost Contributions after upgrading to XGBoost 1.0.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8152\\'>#8152</a>] - GBM and XGBoost no longer ignore a column that includes a constant and NAs.\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8284\\'>#8284</a>] - Added the following options for customizing and retrieving threshold values.\\n<ul>\\n<li>`threshold` allows you to specify the threshold value used for calculating the confusion matrix.\\n<li>`default_threshold` allows you to change the threshold that is used to binarise the predicted class probabilities.</li>\\n<li>`reset_model_threshold` allows you to reset the model threshold.</li>\\n</ul>\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8261\\'>#8261</a>] - Introduced Kubernetes integration. Docker image tests are now available on K8S and published to Docker Hug.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8229\\'>#8229</a>] - A progress bar is now available during Shap Contributions calculations.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9209\\'>#9209</a>] - An H2O Frame containing weights can now be specified when running `make_metrics`. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8361\\'>#8361</a>] - Added POJO and MOJO support for all encodings in GBM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8192\\'>#8192</a>] - Users will now receive an error if they attempt to run https in h2o.init() when starting a local cluster. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8173\\'>#8173</a>] - Added an `-allow_insecure_xgboost` option to h2o and h2odriver that allows XGBoost multinode to run in a secured cluster.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8169\\'>#8169</a>] - Only the leader node is exposed on K8S.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8620\\'>#8620</a>] - Updated the Target Encoding topic and examples based on the improved API. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8293\\'>#8293</a>] - Added a new \"Supported Data Types\" topic to the Algorithms chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8195\\'>#8195</a>] - Added a new \"Kubernetes Integration\" topic to the Welcome chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8194\\'>#8194</a>] - Fixed the links for the constrained k-means Python demos.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8191\\'>#8191</a>] - Fixed the R example in the GAM chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8187\\'>#8187</a>] - Added clarification for when `min_mem_size` and `max_mem_size`` are set to NULL/None in h2o.init(). \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8179\\'>#8179</a>] - The link to the slideshare in the DRF chapter now points to https instead of http.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8177\\'>#8177</a>] - Added information about the h2o.get_leaderboard() function to the AutoML chapter of the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8176\\'>#8176</a>] - Updated the MOJO Quickstart showing how to use PrintMojo to visualize MOJOs without requiring Graphviz.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8168\\'>#8168</a>] - The import_mojo() function now uses \"path\" instead of \"dir\" when downloading, importing, uploading, and saving models. Updated the examples in the documentation.\\n</li>\\n</ul>\\n\\n\\n### Zahradnik (3.30.0.1) - 4/3/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-zahradnik/1/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8638\\'>#8638</a>] - Fixed an issue that caused performing multiple h2o.init() to fails with R on Windows.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8545\\'>#8545</a>] - Increased the default clouding time to avoid times out that resulted in a Cloud 1 under 4 error. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8296\\'>#8296</a>] - Removed obsolete exactLambdas parameter from GLM.\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12884\\'>#12884</a>] - Added support for a fractional response in GLM.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8826\\'>#8826</a>] - Added support for Generalized Additive Models (GAMs) in H2O. The documentation for this newly added algorithm can be found <a href=\\'http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gam.html\\'>here</a>.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8730\\'>#8730</a>] - Added support for parallel training (e.g. spark_apply in rsparkling or Python/R).\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8404\\'>#8404</a>] - Added support for Continuous Bag of Words (CBOW) models in Word2Vec.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8369\\'>#8369</a>] - H2O can now predict OOME during parsing and stop the job if OOME is imminent. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8333\\'>#8333</a>] - Add GBM POJO support for SortByResponse and enumlimited.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8290\\'>#8290</a>] - Added support for Leaf Node Assignments in XGBoost and Isolation Forest MOJOs.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8285\\'>#8285</a>] - Added support for importing Stacked Ensemble MOJO models for scoring. (Note that this only applies to Stacked Ensembles that include algos with MOJO support.)\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8232\\'>#8232</a>] - Added support for the `single_node_mode` parameter in CoxPH.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8228\\'>#8228</a>] - H2O now provides the original algorithm name for MOJO import.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8215\\'>#8215</a>] - Created a segmented model training interface in R. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8214\\'>#8214</a>] - Added a print method for the H2OSegmentModel object type in R.\\n</li>\\n</ul>\\n\\n<h4>Task</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8401\\'>#8401</a>] - Removed the previously deprecated DeepWater Estimator function.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8252\\'>#8252</a>] - Now using Java-based scoring for XGBoostModels.\\n</li>\\n</ul>\\n    \\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/11521\\'>#11521</a>] - In the H2O R package, `data.table` is now enabled by default (if installed). \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9327\\'>#9327</a>] - In AutoML, users can try tuning the learning rate for the best model found during exploration in XGBoost and GBM. Note that the new `exploitation_ratio` parameter is still experimental. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8781\\'>#8781</a>] - Added out-of-the-box support for starting an h2o cluster on Kubernetes. Refer to this <a href=\\'https://github.com/h2oai/h2o-3/blob/master/h2o-k8s/README.md\\'>README</a> for more information. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8553\\'>#8553</a>] - Improved the way AUC-PR is calculated.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8430\\'>#8430</a>] - Added an option to upload binary models from Python and R.\\n</li>\\n</ul>\\n                                                                                                                                                                                                                                                    \\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8564\\'>#8564</a>] - Added examples for Grid Search in the Python Module documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8481\\'>#8481</a>] - Added examples to the R Reference Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8287\\'>#8287</a>] - Added documentation for the fractional binomial family in the GLM section.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8286\\'>#8286</a>] - Added documentation for the new GAM algorithm. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8249\\'>#8249</a>] - Updated tab formatting for the `cluster_size_constraints` parameter appendix entry.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8231\\'>#8231</a>] - Updated the Target Encoding R example.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8230\\'>#8230</a>] - Included confusion matrix threshold details for binary and multiclass classification in the Performance and Prediction chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8227\\'>#8227</a>] - Added documentation for new `upload_model` function.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8221\\'>#8221</a>] - Improved documentation around citing H2O in publications. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8209\\'>#8209</a>] - Added documentation for `single_node_mode` in CoxPH. \\n</li>\\n</ul>\\n\\n\\n### Yule (3.28.1.3) - 4/2/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yule/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yule/3/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8300\\'>#8300</a>] - Fixed an issue that occurred during Hive SQL import with `fetch_mode=SINGLE`; improved Hive SQL import speed; added an option to specify the number of chunks to parse.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8251\\'>#8251</a>] - Hive delegation token refresh now recognizes `-runAsUser`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8243\\'>#8243</a>] - Fixed `base_model` selection for Stacked Ensembles in Flow.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8241\\'>#8241</a>] - The Parquet parser now supports arbitrary precision decimal types.\\n</li>\\n</ul>\\n\\n<h4>Story</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8246\\'>#8246</a>] - The H2O Hive parser now recognizes varchar column types.\\n</li>\\n</ul>\\n\\n<h4>Task</h4>\\n\\n<ul>\\n\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8223\\'>#8223</a>] - Hive tokens are now refreshed without distributing the Steam keytab.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8469\\'>#8469</a>] - Users can now specify the `max_log_file_size` when starting H2O. The log file size currently defaults to 3MB.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8279\\'>#8279</a>] - Fixed the of parameters for TargetEncoder in Flow.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8247\\'>#8247</a>] -  HostnameGuesser.isInetAddressOnNetwork is now public.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8235\\'>#8235</a>] - Improved mapper-side Hive delegation token acquisition. Now when H2O is started from Steam, the Hive delegation token will already be acquired when the cluster is up.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8257\\'>#8257</a>] - Added to docs that `transform` only works on numerical columns.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8218\\'>#8218</a>] - Added documentation for the new num_chunks_hint option that can be specified with `import_sql_table`.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8217\\'>#8217</a>] - Added documentation for the new `max_log_file_size` H2O starting parameter.\\n</li>\\n</ul>\\n\\n\\n\\n\\n### Yule (3.28.1.2) - 3/17/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yule/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yule/2/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8847\\'>#8847</a>] - The `base_models` attribute in Stacked Ensembles is now populated in both Python and R. \\n<br/>\\nNote that in Python, if there are no `base_models` in `_parms`, then `actual_params` is used to retrieve base_models, and it contains the names of the models. In R, `ensemble@model$base_models` is populated with a vector of base model names.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8344\\'>#8344</a>] - Fixed an issue that caused the leader node to be overloaded when parsing 30k+ Parquet files.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8332\\'>#8332</a>] - Fixed an issue that caused `model end_time` and `run_time` properties to return a value of 0 in client mode.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8280\\'>#8280</a>] - TargetEncoderModel\\'s summary no longer prints the fold column as a column that is going to be encoded by this model.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8273\\'>#8273</a>] - When h2omapper fails before discovering SELF (ip & port), the log messages are no longer lost. \\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8506\\'>#8506</a>] - Added DeepLearning MOJO support in Generic Models. \\n</li>\\n</ul>\\n\\n<h4>Improvement\\\\</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9031\\'>#9031</a>] - Changed the output format of `get_automl` in Python from a dictionary to an object.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8266\\'>#8266</a>] - Users can now specify `-hdfs_config` multiple times to specify multiple Hadoop config files.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8264\\'>#8264</a>] - Fixed an issue that caused the clouding process to time out for the Target Encoding module and resulted in a `Cloud 1 under 4` error.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8445\\'>#8445</a>] - Improved FAQ describing how to use the H2O-3 REST API from Java.\\n</li>\\n</ul>\\n\\n### Yule (3.28.1.1) - 3/5/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yule/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yule/1/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8314\\'>#8314</a>] - Added missing AutoML global functions to the Python and R documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8312\\'>#8312</a>] - In the Python client, improved the H2OFrame documentation and properly labeled deprecated functions.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8303\\'>#8303</a>] - Fixed an issue that caused imported MOJOs to produce different predictions than the original model.\\n</li>\\n</ul>\\n\\n<h4>Engineering Story</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8310\\'>#8310</a>] - Removed Sparling Water external backend code from H2O.\\n</li>\\n</ul>\\n\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8309\\'>#8309</a>] - In the R client docs for h2o.head() and h2o.tail(), added an example showing how to control the number of columns to display in dataframe when using a Jupyter notebook with the R kernel.\\n</li>\\n</ul>\\n\\n### Yu (3.28.0.4) - 2/23/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yu/4/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yu/4/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9016\\'>#9016</a>] - DeepLearning MOJOs are now thread-safe. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8406\\'>#8406</a>] - Fixed an issue that caused h2oframe.apply to fail when run in Python 3.7. Note that Python 3.7 is still not officially supported, but support is a WIP.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8375\\'>#8375</a>] - XGBoost now correctly respects monotonicity constraints for all tree_methods.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8373\\'>#8373</a>] - Decision Tree descriptions no longer include more descriptions than `max_depth` splits.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8365\\'>#8365</a>] - Fixed an issue that caused `import_hive_table` to fail with a JDBC source and a partitioned table. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8364\\'>#8364</a>] - Improved the DKVManager sequential removal mechanism.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8356\\'>#8356</a>] - In XGBoost, added a message indicating that the `exact` tree method is not supported in multinode. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8329\\'>#8329</a>] - XGBoost ContributionsPredictor is now serializable.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8328\\'>#8328</a>] - Fixed a CRAN warning related to ellipsis within arguments in the R package.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8325\\'>#8325</a>] - Added support for specifying AWS session tokens.\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9179\\'>#9179</a>] - Added support for Constrained K-Means clustering. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8674\\'>#8674</a>] - In Stacked Ensembles, added support for \"xgboost\" and \"naivebayes\" in the `metalearner_algorithm` parameter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8334\\'>#8334</a>] - Added support for `build_tree_one_node` in XGBoost.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8507\\'>#8507</a>] - In the R client, users can now optionally specify the number of columns to display in `h2o.frame`, `h2o.head`, and `h2o.tail`. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8443\\'>#8443</a>] - Fixed an issue that caused AutoML to fail to run if XGBoost was disabled.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8382\\'>#8382</a>] - Stacktraces are no longer returned in  `h2o.getGrid` when failed models are present.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8327\\'>#8327</a>] - Added `createNewChunks` with a \"sparse\" parameter in ChunkUtils. \\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8675\\'>#8675</a>] - Added an FAQ to the MOJO and POJO quick starts noting that MOJOs and POJOs are thread safe for all supported algorithms.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8420\\'>#8420</a>] - Added the new `cluster_size_constraints` parameter to the KMeans chapter. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8350\\'>#8350</a>] - Updated docs to specify that `mtries=-2` gives all features.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8323\\'>#8323</a>] - Updated EC2 and S3 Storage topic to include the new, optional AWS session token.\\n</li>\\n</ul>\\n\\n\\n### Yu (3.28.0.3) - 2/5/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yu/3/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yu/3/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8888\\'>#8888</a>] - In the R client, fixed a parsing bug that occurred when using quotes with .csv files in as.data.frame().\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8815\\'>#8815</a>] - Fixed an Unsupported Operation Exception in UDP-TCP-SEND. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8522\\'>#8522</a>] - GLM now supports coefficients on variable importance when model standardization is disabled.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8446\\'>#8446</a>] - In the Python client, rbind() can now be used on all numerical types.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8440\\'>#8440</a>] - In XGBoost, fixed an error that occurred during model prediction when OneHotExplicit was specified during model training. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8429\\'>#8429</a>] - Performing grid search over Target Encoding parameters now works correctly.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8391\\'>#8391</a>] - Fixed an issue that caused import_hive_table to not classload the JDBC driver.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8389\\'>#8389</a>] - MOJOs can now be built from XGBoost models built with an offset column.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8363\\'>#8363</a>] - Fixed an issue that cause the R and Python clients to return the wrong sensitivity metric value.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8362\\'>#8362</a>] - Fixed an incorrect sender port calculation in TimestampSnapshot.\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9128\\'>#9128</a>] - In AutoML, multinode XGBoost is now enabled by default.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8410\\'>#8410</a>] - Users can now specify a custom JDBC URL to retrieve the Hive Delegation token using hiveJdbcUrlPattern.\\n</li>\\n</ul>\\n\\n<h4>Task</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8385\\'>#8385</a>] - In XGBoost fixed a deprecation warning for reg:linear.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8442\\'>#8442</a>] - import_folder() can now be used when running H2O in GCS.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8407\\'>#8407</a>] - Added support for registering custom servlets.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8377\\'>#8377</a>] - In XGBoost, when a parameter with a synonym is updated, the synonymous parameter is now also updated. \\n</li>\\n</ul>\\n\\n<h4>Engineering Story</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8388\\'>#8388</a>] - AutoBuffer.getInt() is now public.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8412\\'>#8412</a>] - Python examples for plot method on binomial models now use the correct method signature.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8411\\'>#8411</a>] - Updated custom_metric_func description to indicate that it is not supported in GLM. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8395\\'>#8395</a>] - Updated the AutoML documentation to indicate that multinode XGBoost is now turned on by default.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8379\\'>#8379</a>] - Fixed the description for the Hadoop -nthreads parameter.\\n</li>\\n</ul>\\n\\n\\n### Yu (3.28.0.2) - 1/20/2020\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yu/2/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yu/2/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8571\\'>#8571</a>] - Fixed an issue that resulted in a \"DistributedException java.lang.ClassNotFoundException: BAD\" message.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8499\\'>#8499</a>] - Users can now specify either a model or a model key when checkpointing.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8490\\'>#8490</a>] - Fixed an issue that resulted in an endless loop when CsvParser parser $ sign was enclosed in quotes.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8480\\'>#8480</a>] - In GBM and DRF, fixed an AIOOBE error that occurred when the dataset included negative zeros (-0.0).\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8467\\'>#8467</a>] - Fixed a race condition in the addWarningP method on Model class.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8455\\'>#8455</a>] - h2odriver now gets correct version of Hadoop dependencies.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8439\\'>#8439</a>] - Fixed a race condition in addVec.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8435\\'>#8435</a>] - Parallel Grid Search threads now call the Hyperspace iterator one at a time.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8431\\'>#8431</a>] - sklearn wrappers now expose wrapped estimator as a public property.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8428\\'>#8428</a>] - Fixed an issue in reading user_splits in Java.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8421\\'>#8421</a>] - Fixed an issue that caused rank vectors of Spearman correlation to have different chunk layouts.\\n</li>\\n</ul>\\n\\n<h4>Task</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8583\\'>#8583</a>] - Added a JSON option of PrintMojo.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8520\\'>#8520</a>] - Improved the error message that displays when a user attempts to import data from an HDFS directory that is empty.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8456\\'>#8456</a>] - H2O can now read Hive table metadata two ways: either via direct Metastore access or via JDBC.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9167\\'>#9167</a>] - Improved heuristics used for finding IP addresses on Hadoop in order to select the right subnet automatically. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8605\\'>#8605</a>] - Added support for `offset_column in XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8551\\'>#8551</a>] - Users can now create tree visualizations without installing additional packages.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8503\\'>#8503</a>] - Added a new `download_model` function for downloading binary models in the R and Python clients. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8475\\'>#8475</a>] - Improved XGBoost performance.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8474\\'>#8474</a>] - When computing the correlation matrix of one or two H2OFrames (using `cor()`), users can now specify a method of either Pearson (default) or Spearman.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8438\\'>#8438</a>] - Users are now warned when they attempt to run AutoML with a validation frame and with nfolds > 0.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8436\\'>#8436</a>] - AutoML no longer trains a \"Best of Family Stacked Ensemble\" when only one family is specified.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12973\\'>#12973</a>] - Removed `ignored_columns` from the list of available paramters in AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8647\\'>#8647</a>] - Fixed a broken link in the JAVA FAQ.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8552\\'>#8552</a>] - Improved the documentation for Tree Class in the Python Client docs.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8484\\'>#8484</a>] - Clarified the difference between h2o.performance() and h2o.predict() in the Performance and Prediction chapter of the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8478\\'>#8478</a>] - Incorporated HGLM documentation updates into the GLM booklet.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8441\\'>#8441</a>] - Added an FAQ for GC allocation failure in the FAQ > Clusters section.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8434\\'>#8434</a>] - In the Stacked Ensembles chapter, improved the metalearner support FAQ.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8419\\'>#8419</a>] - Added `offset_column` to the list of supported parameters in XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8418\\'>#8418</a>] - Added information about recent API changes in AutoML to the <a href=\"http://docs.h2o.ai/h2o/latest-stable/h2o-docs/api-changes.html\">API-Related Changes</a> section in the User Guide.\\n</li>\\n</ul>\\n\\n\\n### Yu (3.28.0.1) - 12/16/2019\\n\\nDownload at: <a href=\\'http://h2o-release.s3.amazonaws.com/h2o/rel-yu/1/index.html\\'>http://h2o-release.s3.amazonaws.com/h2o/rel-yu/1/index.html</a>\\n\\n<h4>Bug Fix</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12823\\'>#12823</a>] - AutoML reruns using, for example, the same project name, no project name, etc., now produce consistent results.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8924\\'>#8924</a>] - Fixed an issue that occcurred when running an AutoML instance twice using the same project_name. AutoML no longer appends new models to the existing leaderboard, which caused the models for the first run to attempt to get rescored against the new learderboard_frame.  \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8696\\'>#8696</a>] - Updated the list of stopping metric options for AutoML in Flow. Also added support for the aucpr stopping metric in AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8673\\'>#8673</a>] - When training a K-Means model, the framename is no longer missing in the training metrics.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8642\\'>#8642</a>] - In AutoML, the `project_name` is now restricted to the same constraints as h2o frames. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8576\\'>#8576</a>] - In GBM, fixed an NPE that occurred when sample rate < 1.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8575\\'>#8575</a>] - The AutoML backend no longer accepts `ignored_columns` that contain one of response column, fold column, or weights column.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8505\\'>#8505</a>] - XGBoost MOJO now works correctly in Spark.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8504\\'>#8504</a>] - The REST API ping thread now starts after the cluster is up.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8502\\'>#8502</a>] - Fixed an NPE at hex.tree.TreeHandler.fillNodeCategoricalSplitDescription(TreeHandler.java:272)\\n</li>\\n</ul>\\n\\n<h4>New Feature</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/12219\\'>#12219</a>] - Extended MOJO support for PCA\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9121\\'>#9121</a>] - We are very excited to add HGLM (Hierarchical GLM) to our open source offering. As this is the first release, we only implemented the Gaussian family. However, stay tuned or better yet, tell us what distributions you want to see next. Try it out and send us your feedback!\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9117\\'>#9117</a>] - MOJO Import is now available for XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8917\\'>#8917</a>] - Improved integration of the H2O Python client with Sklearn.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8896\\'>#8896</a>] - Users can now specify monotonicity constraints in AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8884\\'>#8884</a>] - Users can now save and load grids to continue a Grid Search after a cluster restart.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8860\\'>#8860</a>] - Users can now specify a `parallelism` parameter when running grid search. A value of 1 indicagtes sequential building (default); a value of 0 is used for adapative parallelism; and any value greater than 1 sets the exact number of models built in parallel.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8837\\'>#8837</a>] - Added a function to calculate Spearman Correlation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8793\\'>#8793</a>] - Users can now specify the order in which training steps will be executed during an AutoML run. This is done using the new `modeling_plan` option. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8745\\'>#8745</a>] - The `calibration_frame` and `calibrate_model` options can now be spcified in XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8707\\'>#8707</a>] - Added support for OneHotExplicit categorical encoding in EasyPredictModelWrapper.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8568\\'>#8568</a>] - Added aucpr to the AutoML leaderboard, stopping_metric, and sort_metric.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8566\\'>#8566</a>] - An AutoML leaderboard extension is now available that includes model training time and model scoring time.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8558\\'>#8558</a>] - Exposed the location of Chunks in the REST API.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8544\\'>#8544</a>] - Added a `rest_api_ping_timeout` option, which can stop a cluster if nothing has touched the REST API for the specified timeout.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8535\\'>#8535</a>] - Added support for Java 13.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8513\\'>#8513</a>] - H2O no longer performs an internal self-check when converting trees in H2O. \\n</li>\\n</ul>\\n\\n<h4>Task</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8840\\'>#8840</a>] - Fixed an XGBoost error on multinode with AutoML.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8818\\'>#8818</a>] - Added checkpointing to XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8664\\'>#8664</a>] - Users can now perform random grid search over target encoding hyperparameters\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8582\\'>#8582</a>] - Improved Grid Search testing in Flow.\\n</li>\\n</ul>\\n\\n<h4>Improvement</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/11862\\'>#11862</a>] - When specifying a `stopping_metric`, H2O now supports lowercase and uppercase characters.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9424\\'>#9424</a>] - Added a warning message to AutoML if the leaderboard is empty due to too little time for training.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/9019\\'>#9019</a>] - In AutoML, blending frame details were added to event_log.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8879\\'>#8879</a>] - If early stopping is enabled, GBM can reset the ntree value. In these cases, added an `ntrees_actual` (Python)/`get_ntrees_actual` (R) method to provide the actual ntree value (whether CV is enabled or not) rather than the original ntree value set by the user before building a model. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8808\\'>#8808</a>] - Refactored AutoML to improve integration with Target Encoding.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8708\\'>#8708</a>] - Exposed `get_automl` from `h2o.automl` in the Python client.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8701\\'>#8701</a>] - In GBM POJOs, one hot explicit  EasyPredictModelWrapper now takes care of the encoding, and the user does not need to explicitly apply it.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8670\\'>#8670</a>] - Added support for numeric arrays to IcedHashMap.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8581\\'>#8581</a>] - Improved the AutoML Flow UI.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8574\\'>#8574</a>] - The `mae`, `rmsle`, and `aucpr` stopping metrics are now available in Grid Search.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8567\\'>#8567</a>] - When creating a hex.genmodel.easy.EasyPredictModelWrapper with contributions enabled, H2O now uses slf4j in the library, giving more control to users about when/where warnings will be printed.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8491\\'>#8491</a>] - Moved the order of AUCPR in the list of values for `stopping_metric` to right after AUC.\\n</li>\\n</ul>\\n\\n<h4>Engineering Story</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8541\\'>#8541</a>] - Removed unused code in UDPClientEvent.\\n</li>\\n</ul>\\n\\n<h4>Docs</h4>\\n\\n<ul>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8957\\'>#8957</a>] - Added examples to the Python Module documentation DRF chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8920\\'>#8920</a>] - Added examples to the Binomial Models section in the Python Python Module documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8905\\'>#8905</a>] - Added examples to the Multimonial Models section in the Python Python Module documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8903\\'>#8903</a>] - Added examples to the Clustering Methods section in the Python Module documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8902\\'>#8902</a>] - Added examples to the Regression section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8892\\'>#8892</a>] - Added examples to the Autoencoder section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8891\\'>#8891</a>] - Added examples to the Tree Class section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8872\\'>#8872</a>] - Added examples to the Assembly section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8867\\'>#8867</a>] - Added examples to the Node, Leaf Node, and Split Leaf Node sections in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8864\\'>#8864</a>] - Added examples to the H2O Module section in the Python documentation\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8821\\'>#8821</a>] - Added examples to the H2OFrame section in the Python documentation\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8804\\'>#8804</a>] - Documented support for `checkpointing` in XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8802\\'>#8802</a>] - Added examples to the GroupBy section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8792\\'>#8792</a>] - Update to the supported platform table in the XGBoost chapter.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8784\\'>#8784</a>] - Added R/Python examples to the metrics in Performance and Prediction section of the User Guide.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8782\\'>#8782</a>] - Added Parameter Appendix entries for CoxPH parameters.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8744\\'>#8744</a>] - Added examples to the GBM section in the Python documentation\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8728\\'>#8728</a>] - Added a new Reference entry to the Target Encoding documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8721\\'>#8721</a>] - Added examples to the KMeans section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8712\\'>#8712</a>] - Added examples to the CoxPH section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8697\\'>#8697</a>] - Added examples to the Deep Learning section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8667\\'>#8667</a>] -  Added examples to the Stacked Ensembles section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8661\\'>#8661</a>] - Added new `use_spnego` option to the Starting H2O in R topic.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8654\\'>#8654</a>] - Added examples to the Target Encoding section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8652\\'>#8652</a>] - Added examples to the Aggregator section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8651\\'>#8651</a>] - Updated the XGBoost extramempercent FAQ.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8636\\'>#8636</a>] - Added examples to the PCA section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8621\\'>#8621</a>] - Added a new section for Installing and Starting H2O in the Python Client documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8615\\'>#8615</a>] - Added examples to the SVD section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8604\\'>#8604</a>] - Improve the R and Python documentation for `search_criteria` in Grid Search.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8546\\'>#8546</a>] - Added an example using `predict_contributions` to the MOJO quick start.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8524\\'>#8524</a>] - Added examples to the PSVM section in the Python documentation.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8512\\'>#8512</a>] - Added documentation for HGLM in the GLM chapter. \\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8498\\'>#8498</a>] - Improved AutoML documentation: \\n<ul>\\n<li>aucpr is now an available stopping metric and sort metric for AutoML.</li>\\n<li>monotone_constraints can now be specified in AutoML.</li>\\n<li>Added modeling_plan option to list of AutoML parameters.</li>\\n</ul>\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8497\\'>#8497</a>] - MOJOs are now available for PCA.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8496\\'>#8496</a>] - MOJO models are now available for XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8494\\'>#8494</a>] - calibration_frame and calibrate_model are now available in XGBoost.\\n</li>\\n<li>[<a href=\\'https://github.com/h2oai/h2o-3/issues/8493\\'>#8493</a>] - Added Java 13 to list of supported Java versions.\\n</li>\\n</ul>\\n\\n### [Older Releases](Changes-prior-3.28.0.1.md)'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 1. Downloading H2O-3\\n\\nWhile most of this README is written for developers who do their own builds, most H2O users just download and use a pre-built version.  If you are a Python or R user, the easiest way to install H2O is via [PyPI](https://pypi.python.org/pypi/h2o) or [Anaconda](https://anaconda.org/h2oai/h2o) (for Python) or [CRAN](https://CRAN.R-project.org/package=h2o) (for R):  \\n\\n### Python\\n\\n```bash\\npip install h2o\\n```\\n\\n### R\\n\\n```r\\ninstall.packages(\"h2o\")\\n```\\n\\nFor the latest stable, nightly, Hadoop (or Spark / Sparkling Water) releases, or the stand-alone H2O jar, please visit: [https://h2o.ai/download](https://h2o.ai/download)\\n\\nMore info on downloading & installing H2O is available in the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html).\\n\\n<a name=\"Resources\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 2. Open Source Resources\\n\\nMost people interact with three or four primary open source resources:  **GitHub** (which you\\'ve already found), **GitHub issues** (for bug reports and issue tracking), **Stack Overflow** for H2O code/software-specific questions, and **h2ostream** (a Google Group / email discussion forum) for questions not suitable for Stack Overflow.  There is also a **Gitter** H2O developer chat group, however for archival purposes & to maximize accessibility, we\\'d prefer that standard H2O Q&A be conducted on Stack Overflow.\\n\\n<a name=\"IssueTracking\"></a>\\n### 2.1 Issue Tracking and Feature Requests\\n\\nYou can browse and create new issues in our GitHub repository:  <https://github.com/h2oai/h2o-3>\\n\\n*  You can **browse** and search for **issues** without logging in to Github:\\n    1. Click the `Issues` tab on the top of the page\\n    2. Apply filter to search for particular issues \\n*  To **create** an **issue** (either a bug or a feature request):\\n    *  Create H2O-3 issues on the page [https://github.com/h2oai/h2o-3/issues/new/choose](https://github.com/h2oai/h2o-3/issues/new/choose).  (Note: Sparkling Water questions should be addressed under the [Sparkling Water](https://github.com/h2oai/sparkling-water/issues) repository.)\\n\\n\\n<a name=\"OpenSourceResources\"></a>\\n### 2.2 List of H2O Resources\\n\\n*  GitHub\\n    * <https://github.com/h2oai/h2o-3>\\n*  GitHub issues -- file bug reports / track issues here\\n    * The <https://github.com/h2oai/h2o-3/issues> page contains issues for the current H2O-3 project)\\n*  Stack Overflow -- ask all code/software questions here\\n    * <http://stackoverflow.com/questions/tagged/h2o>\\n*  Cross Validated (Stack Exchange) -- ask algorithm/theory questions here\\n    * <https://stats.stackexchange.com/questions/tagged/h2o>\\n*  h2ostream Google Group -- ask non-code related questions here\\n    * Web: <https://groups.google.com/d/forum/h2ostream>\\n    * Mail to: [h2ostream@googlegroups.com](mailto:h2ostream@googlegroups.com)\\n* Gitter H2O Developer Chat\\n    * <https://gitter.im/h2oai/h2o-3>    \\n*  Documentation\\n    * H2O User Guide (main docs): <http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html>\\n    * All H2O documenation links: <http://docs.h2o.ai>\\n    * Nightly build page (nightly docs linked in page): <https://s3.amazonaws.com/h2o-release/h2o/master/latest.html>\\n*  Download (pre-built packages)\\n    * <http://h2o.ai/download>\\n*  Website\\n    * <http://h2o.ai>\\n*  Twitter -- follow us for updates and H2O news!\\n    * <https://twitter.com/h2oai>\\n\\n*  Awesome H2O -- share your H2O-powered creations with us\\n   * <https://github.com/h2oai/awesome-h2o>\\n\\n\\n<a name=\"Artifacts\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 3. Using H2O-3 Artifacts\\n\\nEvery nightly build publishes R, Python, Java, and Scala artifacts to a build-specific repository.  In particular, you can find Java artifacts in the maven/repo directory.\\n\\nHere is an example snippet of a gradle build file using h2o-3 as a dependency.  Replace x, y, z, and nnnn with valid numbers.\\n\\n```\\n// h2o-3 dependency information\\ndef h2oBranch = \\'master\\'\\ndef h2oBuildNumber = \\'nnnn\\'\\ndef h2oProjectVersion = \"x.y.z.${h2oBuildNumber}\"\\n\\nrepositories {\\n  // h2o-3 dependencies\\n  maven {\\n    url \"https://s3.amazonaws.com/h2o-release/h2o-3/${h2oBranch}/${h2oBuildNumber}/maven/repo/\"\\n  }\\n}\\n\\ndependencies {\\n  compile \"ai.h2o:h2o-core:${h2oProjectVersion}\"\\n  compile \"ai.h2o:h2o-algos:${h2oProjectVersion}\"\\n  compile \"ai.h2o:h2o-web:${h2oProjectVersion}\"\\n  compile \"ai.h2o:h2o-app:${h2oProjectVersion}\"\\n}\\n```\\n\\nRefer to the latest H2O-3 bleeding edge [nightly build page](http://s3.amazonaws.com/h2o-release/h2o-3/master/latest.html) for information about installing nightly build artifacts.\\n\\nRefer to the [h2o-droplets GitHub repository](https://github.com/h2oai/h2o-droplets) for a working example of how to use Java artifacts with gradle.\\n\\n> Note: Stable H2O-3 artifacts are periodically published to Maven Central ([click here to search](http://search.maven.org/#search%7Cga%7C1%7Cai.h2o)) but may substantially lag behind H2O-3 Bleeding Edge nightly builds.\\n\\n\\n<a name=\"Building\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 4. Building H2O-3\\n\\nGetting started with H2O development requires [JDK 1.8+](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements), [Node.js](https://nodejs.org/), [Gradle](https://gradle.org/), [Python](https://www.python.org/) and [R](http://www.r-project.org/).  We use the Gradle wrapper (called `gradlew`) to ensure up-to-date local versions of Gradle and other dependencies are installed in your development directory.\\n\\n### 4.1. Before building\\n\\nBuilding `h2o` requires a properly set up R environment with [required packages](#InstallRPackagesInUnix) and Python environment with the following packages:\\n\\n```\\ngrip\\ntabulate\\nrequests\\nwheel\\n```\\n\\nTo install these packages you can use [pip](https://pip.pypa.io/en/stable/installing/) or [conda](https://conda.io/).\\nIf you have troubles installing these packages on *Windows*, please follow section [Setup on Windows](#SetupWin) of this guide.\\n> (Note: It is recommended to use some virtual environment such as [VirtualEnv](https://virtualenv.pypa.io/), to install all packages. )\\n\\n\\n### 4.2. Building from the command line (Quick Start)\\n\\nTo build H2O from the repository, perform the following steps.\\n\\n\\n#### Recipe 1: Clone fresh, build, skip tests, and run H2O\\n\\n```\\n# Build H2O\\ngit clone https://github.com/h2oai/h2o-3.git\\ncd h2o-3\\n./gradlew build -x test\\n\\nYou may encounter problems: e.g. npm missing. Install it:\\nbrew install npm\\n\\n# Start H2O\\njava -jar build/h2o.jar\\n\\n# Point browser to http://localhost:54321\\n\\n```\\n\\n#### Recipe 2: Clone fresh, build, and run tests (requires a working install of R)\\n\\n```\\ngit clone https://github.com/h2oai/h2o-3.git\\ncd h2o-3\\n./gradlew syncSmalldata\\n./gradlew syncRPackages\\n./gradlew build\\n```\\n\\n>**Notes**:\\n>\\n> - Running tests starts five test JVMs that form an H2O cluster and requires at least 8GB of RAM (preferably 16GB of RAM).\\n> - Running `./gradlew syncRPackages` is supported on Windows, OS X, and Linux, and is strongly recommended but not required. `./gradlew syncRPackages` ensures a complete and consistent environment with pre-approved versions of the packages required for tests and builds. The packages can be installed manually, but we recommend setting an ENV variable and using `./gradlew syncRPackages`. To set the ENV variable, use the following format (where `${WORKSPACE} can be any path):\\n>  \\n> ```\\n> mkdir -p ${WORKSPACE}/Rlibrary\\n> export R_LIBS_USER=${WORKSPACE}/Rlibrary\\n> ```\\n\\n#### Recipe 3:  Pull, clean, build, and run tests\\n\\n```\\ngit pull\\n./gradlew syncSmalldata\\n./gradlew syncRPackages\\n./gradlew clean\\n./gradlew build\\n```\\n\\n#### Notes\\n\\n - We recommend using `./gradlew clean` after each `git pull`.\\n\\n- Skip tests by adding `-x test` at the end the gradle build command line.  Tests typically run for 7-10 minutes on a Macbook Pro laptop with 4 CPUs (8 hyperthreads) and 16 GB of RAM.\\n\\n- Syncing smalldata is not required after each pull, but if tests fail due to missing data files, then try `./gradlew syncSmalldata` as the first troubleshooting step.  Syncing smalldata downloads data files from AWS S3 to the smalldata directory in your workspace.  The sync is incremental.  Do not check in these files.  The smalldata directory is in .gitignore.  If you do not run any tests, you do not need the smalldata directory.\\n- Running `./gradlew syncRPackages` is supported on Windows, OS X, and Linux, and is strongly recommended but not required. `./gradlew syncRPackages` ensures a complete and consistent environment with pre-approved versions of the packages required for tests and builds. The packages can be installed manually, but we recommend setting an ENV variable and using `./gradlew syncRPackages`. To set the ENV variable, use the following format (where `${WORKSPACE}` can be any path):\\n\\n  ```\\n  mkdir -p ${WORKSPACE}/Rlibrary\\n  export R_LIBS_USER=${WORKSPACE}/Rlibrary\\n  ```\\n\\n#### Recipe 4:  Just building the docs\\n\\n```\\n./gradlew clean && ./gradlew build -x test && (export DO_FAST=1; ./gradlew dist)\\nopen target/docs-website/h2o-docs/index.html\\n```\\n\\n#### Recipe 5:  Building using a Makefile\\n\\nRoot of the git repository contains a Makefile with convenient shortcuts for frequent build targets used in development.\\nTo build `h2o.jar` while skipping tests and also the building of alternative assemblies, execute \\n\\n```\\nmake\\n```\\n\\nTo build `h2o.jar` using the minimal assembly, run\\n```\\nmake minimal\\n```\\n\\nThe minimal assembly is well suited for developement of H2O machine learning algorithms. It doesn\\'t bundle some heavyweight\\ndependencies (like Hadoop) and using it saves build time as well as need to download large libraries from Maven repositories.\\n\\n<a name=\"SetupWin\"></a>\\n### 4.3. Setup on Windows\\n\\n##### Step 1: Download and install [WinPython](https://winpython.github.io).\\n  From the command line, validate `python` is using the newly installed package by using `which python` (or `sudo which python`). [Update the Environment variable](https://github.com/winpython/winpython/wiki/Environment) with the WinPython path.\\n\\n##### Step 2: Install required Python packages:\\n\\n    pip install grip tabulate wheel\\n\\n##### Step 3: Install JDK\\n\\nInstall [Java 1.8+](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements) and add the appropriate directory `C:\\\\Program Files\\\\Java\\\\jdk1.7.0_65\\\\bin` with java.exe to PATH in Environment Variables. To make sure the command prompt is detecting the correct Java version, run:\\n\\n    javac -version\\n\\nThe CLASSPATH variable also needs to be set to the lib subfolder of the JDK:\\n\\n    CLASSPATH=/<path>/<to>/<jdk>/lib\\n\\n##### Step 4. Install Node.js\\n\\nInstall [Node.js](http://nodejs.org/download/) and add the installed directory `C:\\\\Program Files\\\\nodejs`, which must include node.exe and npm.cmd to PATH if not already prepended.\\n\\n##### Step 5. Install R, the required packages, and Rtools:\\n\\nInstall [R](http://www.r-project.org/) and add the bin directory to your PATH if not already included.\\n\\n<a name=\"InstallRPackagesInUnix\"></a>\\nInstall the following R packages:\\n\\n- [RCurl](http://cran.r-project.org/package=RCurl)\\n- [jsonlite](http://cran.r-project.org/package=jsonlite)\\n- [statmod](http://cran.r-project.org/package=statmod)\\n- [devtools](http://cran.r-project.org/package=devtools)\\n- [roxygen2](http://cran.r-project.org/package=roxygen2)\\n- [testthat](http://cran.r-project.org/package=testthat)\\n\\nTo install these packages from within an R session:\\n\\n```r\\npkgs <- c(\"RCurl\", \"jsonlite\", \"statmod\", \"devtools\", \"roxygen2\", \"testthat\")\\nfor (pkg in pkgs) {\\n  if (! (pkg %in% rownames(installed.packages()))) install.packages(pkg)\\n}\\n```\\nNote that [libcurl](http://curl.haxx.se) is required for installation of the **RCurl** R package.\\n\\nNote that this packages don\\'t cover running tests, they for building H2O only.\\n\\nFinally, install [Rtools](http://cran.r-project.org/bin/windows/Rtools/), which is a collection of command line tools to facilitate R development on Windows.\\n>**NOTE**: During Rtools installation, do **not** install Cygwin.dll.\\n\\n##### Step 6. Install [Cygwin](https://cygwin.com/setup-x86_64.exe)\\n**NOTE**: During installation of Cygwin, deselect the Python packages to avoid a conflict with the Python.org package.\\n\\n###### Step 6b. Validate Cygwin\\nIf Cygwin is already installed, remove the Python packages or ensure that Native Python is before Cygwin in the PATH variable.\\n\\n##### Step 7. Update or validate the Windows PATH variable to include R, Java JDK, Cygwin.\\n\\n##### Step 8. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)\\n\\nIf you don\\'t already have a Git client, please install one.  The default one can be found here http://git-scm.com/downloads.  Make sure that command prompt support is enabled before the installation.\\n\\nDownload and update h2o-3 source codes:\\n\\n    git clone https://github.com/h2oai/h2o-3\\n\\n##### Step 9. Run the top-level gradle build:\\n\\n    cd h2o-3\\n    ./gradlew.bat build\\n\\n> If you encounter errors run again with `--stacktrace` for more instructions on missing dependencies.\\n\\n\\n### 4.4. Setup on OS X\\n\\nIf you don\\'t have [Homebrew](http://brew.sh/), we recommend installing it.  It makes package management for OS X easy.\\n\\n##### Step 1. Install JDK\\n\\nInstall [Java 1.8+](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirementsl). To make sure the command prompt is detecting the correct Java version, run:\\n\\n    javac -version\\n\\n##### Step 2. Install Node.js:\\n\\nUsing Homebrew:\\n\\n    brew install node\\n\\nOtherwise, install from the [NodeJS website](http://nodejs.org/download/).\\n\\n##### Step 3. Install R and the required packages:\\n\\nInstall [R](http://www.r-project.org/) and add the bin directory to your PATH if not already included.\\n\\n<a name=\"InstallRPackagesInUnix\"></a>\\nInstall the following R packages:\\n\\n- [RCurl](http://cran.r-project.org/package=RCurl)\\n- [jsonlite](http://cran.r-project.org/package=jsonlite)\\n- [statmod](http://cran.r-project.org/package=statmod)\\n- [devtools](http://cran.r-project.org/package=devtools)\\n- [roxygen2](http://cran.r-project.org/package=roxygen2)\\n- [testthat](http://cran.r-project.org/package=testthat)\\n\\nTo install these packages from within an R session:\\n\\n```r\\npkgs <- c(\"RCurl\", \"jsonlite\", \"statmod\", \"devtools\", \"roxygen2\", \"testthat\")\\nfor (pkg in pkgs) {\\n  if (! (pkg %in% rownames(installed.packages()))) install.packages(pkg)\\n}\\n```\\nNote that [libcurl](http://curl.haxx.se) is required for installation of the **RCurl** R package.\\n\\nNote that this packages don\\'t cover running tests, they for building H2O only.\\n\\n##### Step 4. Install python and the required packages:\\n\\nInstall python:\\n\\n    brew install python\\n\\nInstall pip package manager:\\n\\n    sudo easy_install pip\\n\\nNext install required packages:\\n\\n    sudo pip install wheel requests tabulate  \\n\\n##### Step 5. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)\\n\\nOS X should already have Git installed. To download and update h2o-3 source codes:\\n\\n    git clone https://github.com/h2oai/h2o-3\\n\\n##### Step 6. Run the top-level gradle build:\\n\\n    cd h2o-3\\n    ./gradlew build\\n\\nNote: on a regular machine it may take very long time (about an hour) to run all the tests.\\n\\n> If you encounter errors run again with `--stacktrace` for more instructions on missing dependencies.\\n\\n### 4.5. Setup on Ubuntu 14.04\\n\\n##### Step 1. Install Node.js\\n\\n    curl -sL https://deb.nodesource.com/setup_0.12 | sudo bash -\\n    sudo apt-get install -y nodejs\\n\\n##### Step 2. Install JDK:\\n\\nInstall [Java 8](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements). Installation instructions can be found here [JDK installation](http://askubuntu.com/questions/56104/how-can-i-install-sun-oracles-proprietary-java-jdk-6-7-8-or-jre). To make sure the command prompt is detecting the correct Java version, run:\\n\\n    javac -version\\n\\n##### Step 3. Install R and the required packages:\\n\\nInstallation instructions can be found here [R installation](http://cran.r-project.org).  Click Download R for Linux.  Click ubuntu.  Follow the given instructions.\\n\\nTo install the required packages, follow the [same instructions as for OS X above](#InstallRPackagesInUnix).\\n\\n>**Note**: If the process fails to install RStudio Server on Linux, run one of the following:\\n>\\n>`sudo apt-get install libcurl4-openssl-dev`\\n>\\n>or\\n>\\n>`sudo apt-get install libcurl4-gnutls-dev`\\n\\n##### Step 4. Git Clone [h2o-3](https://github.com/h2oai/h2o-3.git)\\n\\nIf you don\\'t already have a Git client:\\n\\n    sudo apt-get install git\\n\\nDownload and update h2o-3 source codes:\\n\\n    git clone https://github.com/h2oai/h2o-3\\n\\n##### Step 5. Run the top-level gradle build:\\n\\n    cd h2o-3\\n    ./gradlew build\\n\\n> If you encounter errors, run again using `--stacktrace` for more instructions on missing dependencies.\\n\\n> Make sure that you are not running as root, since `bower` will reject such a run.\\n\\n### 4.6. Setup on Ubuntu 13.10\\n\\n##### Step 1. Install Node.js\\n\\n    curl -sL https://deb.nodesource.com/setup_16.x | sudo bash -\\n    sudo apt-get install -y nodejs\\n\\n##### Steps 2-4. Follow steps 2-4 for Ubuntu 14.04 (above)\\n\\n### 4.7. Setup on CentOS 7\\n\\n```\\ncd /opt\\nsudo wget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/7u79-b15/jdk-7u79-linux-x64.tar.gz\"\\n\\nsudo tar xzf jdk-7u79-linux-x64.tar.gz\\ncd jdk1.7.0_79\\n\\nsudo alternatives --install /usr/bin/java java /opt/jdk1.7.0_79/bin/java 2\\n\\nsudo alternatives --install /usr/bin/jar jar /opt/jdk1.7.0_79/bin/jar 2\\nsudo alternatives --install /usr/bin/javac javac /opt/jdk1.7.0_79/bin/javac 2\\nsudo alternatives --set jar /opt/jdk1.7.0_79/bin/jar\\nsudo alternatives --set javac /opt/jdk1.7.0_79/bin/javac\\n\\ncd /opt\\n\\nsudo wget http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm\\nsudo rpm -ivh epel-release-7-5.noarch.rpm\\n\\nsudo echo \"multilib_policy=best\" >> /etc/yum.conf\\nsudo yum -y update\\n\\nsudo yum -y install R R-devel git python-pip openssl-devel libxml2-devel libcurl-devel gcc gcc-c++ make openssl-devel kernel-devel texlive texinfo texlive-latex-fonts libX11-devel mesa-libGL-devel mesa-libGL nodejs npm python-devel numpy scipy python-pandas\\n\\nsudo pip install scikit-learn grip tabulate statsmodels wheel\\n\\nmkdir ~/Rlibrary\\nexport JAVA_HOME=/opt/jdk1.7.0_79\\nexport JRE_HOME=/opt/jdk1.7.0_79/jre\\nexport PATH=$PATH:/opt/jdk1.7.0_79/bin:/opt/jdk1.7.0_79/jre/bin\\nexport R_LIBS_USER=~/Rlibrary\\n\\n# install local R packages\\nR -e \\'install.packages(c(\"RCurl\",\"jsonlite\",\"statmod\",\"devtools\",\"roxygen2\",\"testthat\"), dependencies=TRUE, repos=\"http://cran.rstudio.com/\")\\'\\n\\ncd\\ngit clone https://github.com/h2oai/h2o-3.git\\ncd h2o-3\\n\\n# Build H2O\\n./gradlew syncSmalldata\\n./gradlew syncRPackages\\n./gradlew build -x test\\n\\n```\\n\\n\\n<a name=\"Launching\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 5. Launching H2O after Building\\n\\nTo start the H2O cluster locally, execute the following on the command line:\\n\\n    java -jar build/h2o.jar\\n\\nA list of available start-up JVM and H2O options (e.g. `-Xmx`, `-nthreads`, `-ip`), is available in the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/starting-h2o.html#from-the-command-line).\\n\\n<a name=\"BuildingHadoop\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 6. Building H2O on Hadoop\\n\\nPre-built H2O-on-Hadoop zip files are available on the [download page](http://h2o.ai/download).  Each Hadoop distribution version has a separate zip file in h2o-3.\\n\\nTo build H2O with Hadoop support yourself, first install sphinx for python: `pip install sphinx`\\nThen start the build by entering  the following from the top-level h2o-3 directory:\\n\\n    export BUILD_HADOOP=1;\\n    ./gradlew build -x test;\\n    ./gradlew dist;\\n\\nThis will create a directory called \\'target\\' and generate zip files there.  Note that `BUILD_HADOOP` is the default behavior when the username is `jenkins` (refer to `settings.gradle`); otherwise you have to request it, as shown above.\\n\\nTo build the zip files only for selected distributions use the `H2O_TARGET` env variable together with `BUILD_HADOOP`, for example:\\n\\n    export BUILD_HADOOP=1;\\n    export H2O_TARGET=hdp2.5,hdp2.6\\n    ./gradlew build -x test;\\n    ./gradlew dist;\\n\\n### Adding support for a new version of Hadoop\\n\\nIn the `h2o-hadoop` directory, each Hadoop version has a build directory for the driver and an assembly directory for the fatjar.\\n\\nYou need to:\\n\\n1.  Add a new driver directory and assembly directory (each with a `build.gradle` file) in `h2o-hadoop`\\n2.  Add these new projects to `h2o-3/settings.gradle`\\n3.  Add the new Hadoop version to `HADOOP_VERSIONS` in `make-dist.sh`\\n4.  Add the new Hadoop version to the list in `h2o-dist/buildinfo.json`\\n\\n\\n### Secure user impersonation\\n\\nHadoop supports [secure user impersonation](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/Superusers.html) through its Java API.  A kerberos-authenticated user can be allowed to proxy any username that meets specified criteria entered in the NameNode\\'s core-site.xml file.  This impersonation only applies to interactions with the Hadoop API or the APIs of Hadoop-related services that support it (this is not the same as switching to that user on the machine of origin).\\n\\nSetting up secure user impersonation (for h2o):\\n\\n1.  Create or find an id to use as proxy which has limited-to-no access to HDFS or related services; the proxy user need only be used to impersonate a user\\n2.  (Required if not using h2odriver) If you are not using the driver (e.g. you wrote your own code against h2o\\'s API using Hadoop), make the necessary code changes to impersonate users (see [org.apache.hadoop.security.UserGroupInformation](http://hadoop.apache.org/docs/r2.8.0/api/org/apache/hadoop/security/UserGroupInformation.html))\\n3.  In either of Ambari/Cloudera Manager or directly on the NameNode\\'s core-site.xml file, add 2/3 properties for the user we wish to use as a proxy (replace <proxyusername> with the simple user name - not the fully-qualified principal name).\\n    * `hadoop.proxyuser.<proxyusername>.hosts`: the hosts the proxy user is allowed to perform impersonated actions on behalf of a valid user from\\n    * `hadoop.proxyuser.<proxyusername>.groups`: the groups an impersonated user must belong to for impersonation to work with that proxy user\\n    * `hadoop.proxyuser.<proxyusername>.users`: the users a proxy user is allowed to impersonate\\n    * Example: ```\\n               <property>\\n                 <name>hadoop.proxyuser.myproxyuser.hosts</name>\\n                 <value>host1,host2</value>\\n               </property>\\n               <property>\\n                 <name>hadoop.proxyuser.myproxyuser.groups</name>\\n                 <value>group1,group2</value>\\n               </property>\\n               <property>\\n                 <name>hadoop.proxyuser.myproxyuser.users</name>\\n                 <value>user1,user2</value>\\n               </property>\\n               ```\\n4.  Restart core services such as HDFS & YARN for the changes to take effect\\n\\nImpersonated HDFS actions can be viewed in the hdfs audit log (\\'auth:PROXY\\' should appear in the `ugi=` field in entries where this is applicable).  YARN similarly should show \\'auth:PROXY\\' somewhere in the Resource Manager UI.\\n\\n\\nTo use secure impersonation with h2o\\'s Hadoop driver:\\n\\n*Before this is attempted, see Risks with impersonation, below*\\n\\nWhen using the h2odriver (e.g. when running with `hadoop jar ...`), specify `-principal <proxy user kerberos principal>`, `-keytab <proxy user keytab path>`, and `-run_as_user <hadoop username to impersonate>`, in addition to any other arguments needed.  If the configuration was successful, the proxy user will log in and impersonate the `-run_as_user` as long as that user is allowed by either the users or groups configuration property (configured above); this is enforced by HDFS & YARN, not h2o\\'s code.  The driver effectively sets its security context as the impersonated user so all supported Hadoop actions will be performed as that user (e.g. YARN, HDFS APIs support securely impersonated users, but others may not).\\n\\n#### Precautions to take when leveraging secure impersonation\\n\\n*  The target use case for secure impersonation is applications or services that pre-authenticate a user and then use (in this case) the h2odriver on behalf of that user.  H2O\\'s Steam is a perfect example: auth user in web app over SSL, impersonate that user when creating the h2o YARN container.\\n*  The proxy user should have limited permissions in the Hadoop cluster; this means no permissions to access data or make API calls.  In this way, if it\\'s compromised it would only have the power to impersonate a specific subset of the users in the cluster and only from specific machines.\\n*  Use the `hadoop.proxyuser.<proxyusername>.hosts` property whenever possible or practical.\\n*  Don\\'t give the proxyusername\\'s password or keytab to any user you don\\'t want to impersonate another user (this is generally *any* user).  The point of impersonation is not to allow users to impersonate each other.  See the first bullet for the typical use case.\\n*  Limit user logon to the machine the proxying is occurring from whenever practical.\\n*  Make sure the keytab used to login the proxy user is properly secured and that users can\\'t login as that id (via `su`, for instance)\\n*  Never set hadoop.proxyuser.<proxyusername>.{users,groups} to \\'*\\' or \\'hdfs\\', \\'yarn\\', etc.  Allowing any user to impersonate hdfs, yarn, or any other important user/group should be done with extreme caution and *strongly* analyzed before it\\'s allowed.\\n\\n#### Risks with secure impersonation\\n\\n*  The id performing the impersonation can be compromised like any other user id.\\n*  Setting any `hadoop.proxyuser.<proxyusername>.{hosts,groups,users}` property to \\'*\\' can greatly increase exposure to security risk.\\n*  When users aren\\'t authenticated before being used with the driver (e.g. like Steam does via a secure web app/API), auditability of the process/system is difficult.\\n\\n\\n```\\n$ git diff\\ndiff --git a/h2o-app/build.gradle b/h2o-app/build.gradle\\nindex af3b929..097af85 100644\\n--- a/h2o-app/build.gradle\\n+++ b/h2o-app/build.gradle\\n@@ -8,5 +8,6 @@ dependencies {\\n   compile project(\":h2o-algos\")\\n   compile project(\":h2o-core\")\\n   compile project(\":h2o-genmodel\")\\n+  compile project(\":h2o-persist-hdfs\")\\n }\\n\\ndiff --git a/h2o-persist-hdfs/build.gradle b/h2o-persist-hdfs/build.gradle\\nindex 41b96b2..6368ea9 100644\\n--- a/h2o-persist-hdfs/build.gradle\\n+++ b/h2o-persist-hdfs/build.gradle\\n@@ -2,5 +2,6 @@ description = \"H2O Persist HDFS\"\\n\\n dependencies {\\n   compile project(\":h2o-core\")\\n-  compile(\"org.apache.hadoop:hadoop-client:2.0.0-cdh4.3.0\")\\n+  compile(\"org.apache.hadoop:hadoop-client:2.4.1-mapr-1408\")\\n+  compile(\"org.json:org.json:chargebee-1.0\")\\n }\\n```\\n\\n<a name=\"Sparkling\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 7. Sparkling Water\\n\\nSparkling Water combines two open-source technologies: Apache Spark and the H2O Machine Learning platform.  It makes H2Os library of advanced algorithms, including Deep Learning, GLM, GBM, K-Means, and Distributed Random Forest, accessible from Spark workflows. Spark users can select the best features from either platform to meet their Machine Learning needs.  Users can combine Spark\\'s RDD API and Spark MLLib with H2Os machine learning algorithms, or use H2O independently of Spark for the model building process and post-process the results in Spark.\\n\\n**Sparkling Water Resources**:\\n\\n* [Download page for pre-built packages](http://h2o.ai/download/)\\n* [Sparkling Water GitHub repository](https://github.com/h2oai/sparkling-water)  \\n* [README](https://github.com/h2oai/sparkling-water/blob/master/README.md)\\n* [Developer documentation](https://github.com/h2oai/sparkling-water/blob/master/DEVEL.md)\\n\\n<a name=\"Documentation\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 8. Documentation\\n\\n### Documenation Homepage\\n\\nThe main H2O documentation is the [H2O User Guide](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html).  Visit <http://docs.h2o.ai> for the top-level introduction to documentation on H2O projects.\\n\\n\\n### Generate REST API documentation\\n\\nTo generate the REST API documentation, use the following commands:\\n\\n    cd ~/h2o-3\\n    cd py\\n    python ./generate_rest_api_docs.py  # to generate Markdown only\\n    python ./generate_rest_api_docs.py --generate_html  --github_user GITHUB_USER --github_password GITHUB_PASSWORD # to generate Markdown and HTML\\n\\nThe default location for the generated documentation is `build/docs/REST`.\\n\\nIf the build fails, try `gradlew clean`, then `git clean -f`.\\n\\n### Bleeding edge build documentation\\n\\nDocumentation for each bleeding edge nightly build is available on the [nightly build page](http://s3.amazonaws.com/h2o-release/h2o/master/latest.html).\\n\\n\\n<a name=\"Citing\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 9. Citing H2O\\n\\nIf you use H2O as part of your workflow in a publication, please cite your H2O resource(s) using the following BibTex entry:\\n\\n### H2O Software\\n\\n\\t@Manual{h2o_package_or_module,\\n\\t    title = {package_or_module_title},\\n\\t    author = {H2O.ai},\\n\\t    year = {year},\\n\\t    month = {month},\\n\\t    note = {version_information},\\n\\t    url = {resource_url},\\n\\t}\\n\\n**Formatted H2O Software citation examples**:\\n\\n- H2O.ai (Oct. 2016). _Python Interface for H2O_, Python module version 3.10.0.8. [https://github.com/h2oai/h2o-3](https://github.com/h2oai/h2o-3).\\n- H2O.ai (Oct. 2016). _R Interface for H2O_, R package version 3.10.0.8. [https://github.com/h2oai/h2o-3](https://github.com/h2oai/h2o-3).\\n- H2O.ai (Oct. 2016). _H2O_, H2O version 3.10.0.8. [https://github.com/h2oai/h2o-3](https://github.com/h2oai/h2o-3).\\n\\n### H2O Booklets\\n\\nH2O algorithm booklets are available at the [Documentation Homepage](http://docs.h2o.ai/h2o/latest-stable/index.html).\\n\\n\\t@Manual{h2o_booklet_name,\\n\\t    title = {booklet_title},\\n\\t    author = {list_of_authors},\\n\\t    year = {year},\\n\\t    month = {month},\\n\\t    url = {link_url},\\n\\t}\\n\\n**Formatted booklet citation examples**:\\n\\nArora, A., Candel, A., Lanford, J., LeDell, E., and Parmar, V. (Oct. 2016). _Deep Learning with H2O_. <http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepLearningBooklet.pdf>.\\n\\nClick, C., Lanford, J., Malohlava, M., Parmar, V., and Roark, H. (Oct. 2016). _Gradient Boosted Models with H2O_. <http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/GBMBooklet.pdf>.\\n\\n<a name=\"Community\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## 10. Community\\n\\nH2O has been built by a great many number of contributors over the years both within H2O.ai (the company) and the greater open source community.  You can begin to contribute to H2O by answering [Stack Overflow](http://stackoverflow.com/questions/tagged/h2o) questions or [filing bug reports](https://github.com/h2oai/h2o-3/issues).  Please join us!  \\n\\n\\n### Team & Committers\\n\\n```\\nSriSatish Ambati\\nCliff Click\\nTom Kraljevic\\nTomas Nykodym\\nMichal Malohlava\\nKevin Normoyle\\nSpencer Aiello\\nAnqi Fu\\nNidhi Mehta\\nArno Candel\\nJosephine Wang\\nAmy Wang\\nMax Schloemer\\nRay Peck\\nPrithvi Prabhu\\nBrandon Hill\\nJeff Gambera\\nAriel Rao\\nViraj Parmar\\nKendall Harris\\nAnand Avati\\nJessica Lanford\\nAlex Tellez\\nAllison Washburn\\nAmy Wang\\nErik Eckstrand\\nNeeraja Madabhushi\\nSebastian Vidrio\\nBen Sabrin\\nMatt Dowle\\nMark Landry\\nErin LeDell\\nAndrey Spiridonov\\nOleg Rogynskyy\\nNick Martin\\nNancy Jordan\\nNishant Kalonia\\nNadine Hussami\\nJeff Cramer\\nStacie Spreitzer\\nVinod Iyengar\\nCharlene Windom\\nParag Sanghavi\\nNavdeep Gill\\nLauren DiPerna\\nAnmol Bal\\nMark Chan\\nNick Karpov\\nAvni Wadhwa\\nAshrith Barthur\\nKaren Hayrapetyan\\nJo-fai Chow\\nDmitry Larko\\nBranden Murray\\nJakub Hava\\nWen Phan\\nMagnus Stensmo\\nPasha Stetsenko\\nAngela Bartz\\nMateusz Dymczyk\\nMicah Stubbs\\nIvy Wang\\nTerone Ward\\nLeland Wilkinson\\nWendy Wong\\nNikhil Shekhar\\nPavel Pscheidl\\nMichal Kurka\\nVeronika Maurerova\\nJan Sterba\\nJan Jendrusak\\nSebastien Poirier\\nTom Frda\\nArd Kelmendi\\nYuliia Syzon\\nAdam Valenta\\nMarek Novotny\\nZuzana Olajcova\\n```\\n\\n<a name=\"Advisors\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## Advisors\\n\\nScientific Advisory Council\\n\\n```\\nStephen Boyd\\nRob Tibshirani\\nTrevor Hastie\\n```\\n\\nSystems, Data, FileSystems and Hadoop\\n\\n```\\nDoug Lea\\nChris Pouliot\\nDhruba Borthakur\\n```\\n\\n<a name=\"Investors\"></a>'}\n",
      "{'filename': 'h2o-3-master/README.md', 'section': '## Investors\\n\\n```\\nJishnu Bhattacharjee, Nexus Venture Partners\\nAnand Babu Periasamy\\nAnand Rajaraman\\nAsh Bhardwaj\\nRakesh Mathur\\nMichael Marks\\nEgbert Bierman\\nRajesh Ambati\\n```'}\n",
      "{'filename': 'h2o-3-master/README_DATA.md', 'section': '## Use cases\\n\\n### Pre-push tests\\n\\nPre-push tests are intended to be fast and run often.\\nRunning time for all tests should be a few minutes.\\n\\n#### Hardware assumptions:\\n\\n* 8GB or better RAM.\\n* Five 1GB JVMs get started.\\n\\n#### Data assumptions:\\n\\n* All test data is either generated or exists in the s3://h2o-public-test-data/smalldata/ directory.\\n\\n#### Test environment:\\n\\n* Driven by java unit test runners\\n\\n#### How to get the data:\\n\\n`$ ./gradlew syncSmalldata`\\n\\n#### How to run tests:\\n\\n`$ ./gradlew test`\\n\\n\\n### Laptop (or small server)\\n\\nLaptop tests are meant to run stressful workloads on a powerful laptop.\\nRunning time for all tests should be less than an hour.\\n\\n#### Hardware assumptions:\\n\\n* At least 12 GB of RAM.  At least 2 CPUs.  (Most H2O developers use a 16 GB Macbook Pro with 4 CPUs and 8 hardware threads.)\\n\\n#### Data assumptions:\\n\\n* Max 5 GB of data to download.\\n\\n* Search for data in the following order:\\n\\n\\t1.  path specified by environment variable H2O_BIGDATA + \"/laptop\"\\n\\t1.  ./bigdata/laptop (A \"magic\" directory in your git workspace)\\n\\t1.  /home/h2opublictestdata/bigdata/laptop\\n\\t1.  /mnt/h2o-public-test-data/bigdata/laptop\\n\\t\\n#### Test environment:\\n\\n* RUnit tests\\n* Python tests\\n\\n#### How to get the data:\\n\\n`$ ./gradlew syncBigdataLaptop`  \\n\\n\\n#### How to run tests:\\n\\n`./gradlew testLaptop`\\n\\n\\n### Big server\\n\\nBig server tests are meant to run stressful workloads on modern server hardware with lots of resources.  Many servers may be used at once to reduce running time.\\n\\n#### Hardware assumptions:\\n\\n* At least 256 GB of RAM.  Lots of CPUs.\\n\\n#### Data assumptions:\\n\\n* Max 50 GB of data to download.\\n* Take advantage of soft and hard links to make bigger datasets.\\n\\n* Search for data in the following order:\\n\\n\\t1.  path specified by environment variable H2O_BIGDATA + \"/bigserver\"\\n\\t1.  ./bigdata/bigserver (A \"magic\" directory in your git workspace)\\n\\t1.  /home/h2opublictestdata/bigdata/bigserver\\n\\t1.  /mnt/h2o-public-test-data/bigdata/bigserver\\n\\n#### Test environment:\\n\\n* RUnit tests\\n* Python tests\\n\\n#### How to get the data:  \\n\\nCAUTION: Don\\'t do this at home.\\n\\n`$ ./gradlew syncBigdataBigserver`\\n\\n#### How to run tests:\\n\\n`./gradlew testBigserver`\\n\\n\\n### Cluster in EC2\\n\\n#### Environment assumptions:\\n\\n* Infinite RAM.  Lots of cores.\\n\\n#### Data assumptions:\\n\\n* Data lives in S3.  Huge. \\n\\n* Search for data in the following order:\\n\\n\\t1.  s3://h2o-public-test-data/bigdata\\n\\n#### Test environment:\\n\\n* JVMs running directly in EC2 instances\\n\\n#### How to get the data:\\n\\nYou don\\'t.  Just point your test directly to s3://h2o-public-test-data/bigdata.  Definitely do not copy the data to EBS disks.\\n\\n#### How to run tests:\\n\\nJenkins launches them nightly or on-demand.  (Need instructions for how to do this.)'}\n",
      "{'filename': 'h2o-3-master/SECURITY.md', 'section': '## Reporting a Vulnerability\\n\\nPlease report (suspected) security vulnerabilities to support@h2o.ai. You will receive a response from us within 48 hours. \\nIf the issue is confirmed, we will release a patch as soon as possible depending on complexity but historically within a few days.'}\n",
      "{'filename': 'h2o-3-master/SECURITY.md', 'section': '## Known Vulnerabilities\\n\\nWe located these vulnerabilites from our security scans. The following list shows the vulnerabilities and the libraries they were found in:\\n\\n- CVE-2024-9143: `libcrypto3`, `libssl3`\\n- CVE-2021-22569: `com.google.protobuf:protobuf-java (main-3.46.0.jar)`, `com.google.protobuf:protobuf-java (main.jar)`\\n- CVE-2021-22570: `com.google.protobuf:protobuf-java (main-3.46.0.jar)`, `com.google.protobuf:protobuf-java (main.jar)`\\n- CVE-2022-3509: `com.google.protobuf:protobuf-java (main-3.46.0.jar)`, `com.google.protobuf:protobuf-java (main.jar)`\\n- CVE-2022-3510: `com.google.protobuf:protobuf-java (main-3.46.0.jar)`, `com.google.protobuf:protobuf-java (main.jar)`\\n- CVE-2024-7254: `com.google.protobuf:protobuf-java (main-3.46.0.jar)`, `com.google.protobuf:protobuf-java (main.jar)`\\n- CVE-2022-3171: `com.google.protobuf:protobuf-java (main-3.46.0.jar)`, `com.google.protobuf:protobuf-java (main.jar)`\\n- CVE-2024-23454: `org.apache.hadoop:hadoop-common (main-3.46.0.jar)`, `org.apache.hadoop:hadoop-common (main.jar)`\\n- CVE-2024-6763: `org.eclipse.jetty:jetty-http (main-3.46.0.jar)`, `org.eclipse.jetty:jetty-http (main.jar)`\\n- CVE-2024-8184: `org.eclipse.jetty:jetty-http (main-3.46.0.jar)`, `org.eclipse.jetty:jetty-http (main.jar)`\\n- CVE-2024-9823: `org.eclipse.jetty:jetty-http (main-3.46.0.jar)`, `org.eclipse.jetty:jetty-http (main.jar)`\\n- CVE-2024-23454: `org.apache.hadoop:hadoop-common (steam-3.46.0.jar)`, `org.apache.hadoop:hadoop-common (steam.jar)`\\n- CVE-2024-6763: `org.eclipse.jetty:jetty-http (steam-3.46.0.jar)`, `org.eclipse.jetty:jetty-http (steam.jar)`\\n- CVE-2024-8184: `org.eclipse.jetty:jetty-http (steam-3.46.0.jar)`, `org.eclipse.jetty:jetty-http (steam.jar)`'}\n",
      "{'filename': 'h2o-3-master/examples/deeplearning/notebooks/README.md', 'section': '## Table of Contents'}\n",
      "{'filename': 'h2o-3-master/examples/deeplearning/notebooks/README.md', 'section': '## CPU Deep Learning using H2O Deep Learning\\n\\n### Python Jupyter notebooks\\n- [Anomaly detection with H2O Deep Learning (CPU)](./deeplearning_anomaly_detection.ipynb)\\n- [Image reconstruction and clustering with H2O Deep Learning (CPU)](./deeplearning_image_reconstruction_and_clustering.ipynb)'}\n",
      "{'filename': 'h2o-3-master/examples/deeplearning/notebooks/README.md', 'section': '## GPU Deep Learning using H2O Deep Water\\n\\n### Python Jupyter notebooks\\n- [Cat/Dog/Mouse image classification with Inception Resnet V2 convolutional neural network (GPU)](./deeplearning_cat_dog_mouse_inception_resnetv2.ipynb)\\n\\n### R Jupyter notebooks'}\n",
      "{'filename': 'h2o-3-master/gradle/README.md', 'section': '## Artifacts\\n\\nH2O artifacts are published in [Maven Central](http://search.maven.org).\\nThe artifacts are available via [this query](http://search.maven.org/#search%7Cga%7C1%7Cai.h2o).\\n\\n  * `h2o-core` - core functionaly of H2O platform including K/V store, M/R framework, networking\\n  * `h2o-algos` - basic set of algorithms (GLM, GBM, DeepLeaning,...)\\n  * `h2o-app` - H2O standalone application launcher\\n  * `h2o-web` - H2O web UI called _Steam_\\n  \\n### Versioning of Artifacts\\nAll published artifacts share the same version enforced by\\nparent project. See file `gradle.properties` which contains version definition.'}\n",
      "{'filename': 'h2o-3-master/gradle/README.md', 'section': '## Artifacts Building'}\n",
      "{'filename': 'h2o-3-master/gradle/README.md', 'section': '## Artifacts Assembling'}\n",
      "{'filename': 'h2o-3-master/gradle/README.md', 'section': '## Artifacts Publishing\\n\\nFor publishing gradle nexus pluging is used (see\\nhttps://github.com/bmuschko/gradle-nexus-plugin).\\n\\nIt publishes artifacts into:\\n\\n  * Local maven repository\\n  * Sonatype snapshot repository\\n  * Sonatype release repository\\n\\n### Local Maven repository\\nTo publish artifacts into **local Maven** repository type:\\n\\n```\\ngradle install\\n```\\n\\n### Local build repository\\nTo publish artifacts into **local Maven** repository stored under `build/repo`:\\n\\n```\\ngradle publish\\n```\\n\\n### Sonatype & Public H2O Release Repository\\nTo publish artifacts into remote **Sonatype Release** and **Public H2O Release** repository please type:\\n```\\ngradle -PdoRelease publish\\n```\\n\\n#### Public release repository configuration\\n\\nTo upload artifacts into remote repository the file `~/.gradle/gradle.properties` has to contains your Sonatype credentials\\n```\\noss-releases.username=<your Sonatype username>\\noss-releases.password=<your Sonatype password>\\n```\\nand your Public H2O repository credentials\\n```\\npublic-release-maven.username=<your Public H2O repository username>\\npublic-release-maven.password=<your Public H2O repository password>\\n```\\n\\nPublishing to public repositories requires signed artifacts.\\nHence it is necessary to provide GNUPG key reference into`~/.gradle/gradle.properties` or via command line correspond `-P` options:\\n\\n```\\nsigning.keyId=<Your Key Id>\\nsigning.password=<Your Public Key Password>\\nsigning.secretKeyRingFile=<Path To Your Key Ring File>\\n```\\n\\nTo import H2O public key, please use:\\n```\\ngpg2 --keyserver hkp://pool.sks-keyservers.net --recv-keys 539BAEFF\\n```\\n\\nThe published artifacts are available at https://oss.sonatype.org. \\nThey need manual approval and propagation to maven central. \\nPlease use OSS credentials to log into OSS (http://oss.sonatype.org/), select _Staging Repositories_ item, select `ai.h2o` repository (verify open date), then close the repository and propagate into Maven Cenral.\\n\\nThe artifacts should be available via Maven central in a few minutes.\\nTo check them please use the following search link: http://search.maven.org/#search%7Cga%7C1%7Cai.h2o\\n\\n#### Automatic release and propagation into Maven Central\\nTo release, approve and propagate H2O artifacts into Maven central automatically please type:\\n```\\ngradle -PdoRelease release\\n``` \\n\\n> The `release` task has the same requirements as `publish` task described above'}\n",
      "{'filename': 'h2o-3-master/gradle/README.md', 'section': '## Jenkins Pipelines\\n\\nIn release Jenkins job, please setup environment and call the `make-dist.sh` script.'}\n",
      "{'filename': 'h2o-3-master/gradle/README.md', 'section': '## Gradle FAQ\\n\\n* Should i use `gradle` command from my machine or `gradlew` command provided by the project?\\n  * Use `gradlew` command since it will download expected (and supported) gradle version for you.\\n\\n* How can I run a specific task (i.e., test)?\\n  * `./gradlew test`\\n  \\n* How can I run a specific task on a particular project?\\n  * `./gradlew :h2o-algos:test`\\n  \\n* How can I run gradle daemon by default?\\n  * Put `org.gradle.daemon=true` into your `~/.gradle/gradle.properties`\\n  \\n* How can I run gradle without daemon?\\n  * `./gradlew --no-daemon`\\n  \\n* How can I open h2o-dev project in Idea?\\n  * Open project via selecting top-level `build.gradle` file.\\n   \\n* How can I work offline with gradle?\\n  * Run gradle with `--offline` command line parameter, for example: `./gradlew --offline\\n    test`\\n\\n* How can I pass a parameter to gradle build?\\n  * Specify parameter with `-P` option on gradle command line, for example:\\n    `./gradlew -PdoFindbugs=true install`'}\n",
      "{'filename': 'h2o-3-master/h2o-algos/src/main/java/hex/deeplearning/README.md', 'section': \"## Results\\n\\nWhile H2O Deep Learning is written in pure Java, it is *competitive* with other CPU and GPU based solutions for common multi-layer feed-forward neural networks. H2O is distributed and can handle large datasets that don't fit on any single node's memory. Dedicated GPU-based solutions can be faster for large neural networks (millions of weights) and for convolutional/LSTM-based architectures (which are not currently supported by H2O), especially for image or NLP applications. H2O Deep Learning is particularly well suited for structured data and use cases include fraud, churn, insurance, finance, marketing, sciences and many more.\\n\\n### MNIST\\nThe [MNIST](http://yann.lecun.com/exdb/mnist/) database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. Each digit is represented by 28x28=784 gray-scale pixel values (features).\\n\\nThe following benchmark is available as an [example Flow pack](https://github.com/h2oai/h2o-3/tree/master/h2o-docs/src/product/flow/packs/examples) and is also part of the distribution. In Flow, on the right-hand-side, click on the 'HELP' tab, then 'view example Flows', then select 'DeepLearning_MNIST.flow'.\\n\\n**Model parameters**: For illustration only, not tuned. 2 hidden layers (128,64), Rectifier with Dropout, L1/L2 regularization, mini-batch size 1. Auto-tuning for the number of training images per Map/Reduce iteration (`train_samples_per_iteration=-2`). The model is trained until convergence of the test set accuracy, and scored on the training and test sets every 5 seconds, with full confusion matrices and variable importances.\\n\\n**Hardware**: Dual Xeon E5-2650 2.6GHz, Ubuntu 12.04, Java 7, 10GbE interconnect\\n\\n| | test set error | speed | \\n| --- | ---: | ---: |\\n| H2O 1 node | 2.1% | 80K images/sec |\\n| H2O 2 nodes | 2.1% | 140K images/sec |\\n| H2O 4 nodes | 2.1% | 280K images/sec | \\n| H2O 8 nodes | 2.1% | 550K images/sec |\\n| 1 GPU GTX980 (pick your tool)| | ~100K images/sec |\"}\n",
      "{'filename': 'h2o-3-master/h2o-assemblies/main/README.md', 'section': '## Building\\n\\n```\\n./gradlew :h2o-assemblies:main:build\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-assemblies/minimal/README.md', 'section': '## Building\\n\\n```\\n./gradlew :h2o-assemblies:minimal:build\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-bindings/bin/readme.md', 'section': '## Custom code generation\\n\\nFor our main client APIs in `R` and `Python`, various customizations had to be applied for several algorithms.\\n\\nTherefore, for various reasons  readability/reviews, maintainability, documentation, clear separation of logic for each algo  those customizations are loaded by the code generator through a template mechanism.\\n   \\nFor each algorithm `ALGO`, we can then find a Python template script `gen_ALGO.py` under `custom/python` or `custom/R`\\n                  \\nTo avoid some code duplications (hooks that are common to many algorithms but not to all of them), it\\'s also possible to put the commonly shared logic in a file named `gen_defaults.py`.  \\nFor example, in `R` the validation of `x, y, training_frame` args is shared by the majority of algorithms, so it resides in `custom/R/gen_defaults.py` template.\\nThis also means that for algorithms not using those defaults, there could be some small code duplication, but it\\'s at the benefit of a clearer structure and readability.\\nThis `gen_defaults.py` has the same structure as other `gen_ALGO.py`, it is just a fallback used if and only if its properties haven\\'t been overridden in the dedicated `gen_ALGO.py` file.\\n\\nFile structure:\\n```text\\nh2o-bindings/bin\\n custom\\n    R\\n       gen_aggregator.py\\n   |    ...\\n       gen_defaults.py\\n   |    ...\\n       gen_xgboost.py\\n    python\\n        gen_aggregator.py\\n        ...\\n        gen_defaults.py\\n        ...\\n        gen_xgboost.py\\n gen_R.py\\n gen_python.py\\n```\\n\\n### Templates structure\\n\\nExcept for Python as we\\'ll see below, the `gen_ALGO.py` files are mainly template files, so we can\\'t directly write interpretable code there.\\nInstead, custom code is just wrapped in a `Python` triple-quoted string.\\nFor readability, it is recommended to start the code block in the line following the first `\"\"\"`, and to left-align the code without additional indentation: the code generator will ensure that the block is correctly indented relatively to previous code.\\n\\nThe same applies for documentation.\\n\\nExample in R:\\n```python\\nextensions = dict(\\n    required_params=[\\'x\\', \\'y\\', \\'training_frame\\'],\\n    frame_params=[\\'training_frame\\', \\'validation_frame\\'],\\n    validate_required_params=\"\"\"\\n# If x is missing, then assume user wants to use all columns as features.\\nif (missing(x)) {\\n   if (is.numeric(y)) {\\n       x <- setdiff(col(training_frame), y)\\n   } else {\\n       x <- setdiff(colnames(training_frame), y)\\n   }\\n}\\n\"\"\",\\n)\\n\\ndoc = dict(\\n    params=dict(\\n        x=\"\"\"\\n(Optional) A vector containing the names or indices of the predictor variables to use in building the model.\\nIf x is missing, then all columns except y are used.\\n\"\"\",\\n        y=\"\"\"\\nThe name or column index of the response variable in the data. \\nThe response must be either a numeric or a categorical/factor variable. \\nIf the response is numeric, then a regression model will be trained, otherwise it will train a classification model.\\n\"\"\",\\n    )\\n)\\n```\\n\\n#### Python\\n\\nfor Python the `gen_ALGO.py` files have the following possible entries (none of them being required):\\n\\n```python\\nrest_api_version = 99   # if algo doesn\\'t use the default 3\\n\\ndef update_params(name, param):\\n    # a hook to modify schema params on the fly: mainly used to remove some enum values...\\n    pass\\n\\nextensions = dict(\\n    __imports__=\"a string with additional imports\",\\n    __class__=\"a string or a function defining additional methods for the default algo class\",\\n    __module__=\"a string or a function defining additional methods for the algo module\",\\n    __init__validation=\"a string defining additional code for validation phase of __init__: used only for generic\",\\n    __init__setparams=\"a string defining additional code for set_params phase of __init__: used only for word2vec\",\\n)\\n\\noverrides = dict(\\n    foo = dict(\\n         getter=\"string defining code overriding the getter for property foo\",\\n         setter=\"string defining code overriding the setter for property foo\",\\n    ),\\n)\\n\\ndoc = dict(\\n    __class__ = \"additional documentation added at class level\",\\n    foo = \"additional documentation added to the getter of property foo\"\\n)\\n\\nexamples = dict(\\n    __class__ = \"doc examples added at class level\",\\n    foo = \"doc examples added to the getter of property foo\"\\n)\\n```\\n\\nGiven that `Python` = `Python`, we can directly write custom in Python functions instead of strings.\\nThis provides better color syntax and code readability in general, but those functions are still usually not directly runnable due to difficulties in importing the right dependencies from the template file (playing with sys.path doesn\\'t help in IDEs either). \\n \\nHere is an example of custom code in `gen_deeplearning.py`:\\n\\n```python\\ndef module_extensions():\\n    class H2OAutoEncoderEstimator(H2ODeepLearningEstimator):\\n        \"\"\"\\n        :examples:\\n\\n        >>> import h2o as ml\\n        >>> from h2o.estimators.deeplearning import H2OAutoEncoderEstimator\\n        >>> ml.init()\\n        >>> rows = [[1,2,3,4,0]*50, [2,1,2,4,1]*50, [2,1,4,2,1]*50, [0,1,2,34,1]*50, [2,3,4,1,0]*50]\\n        >>> fr = ml.H2OFrame(rows)\\n        >>> fr[4] = fr[4].asfactor()\\n        >>> model = H2OAutoEncoderEstimator()\\n        >>> model.train(x=range(4), training_frame=fr)\\n        \"\"\"\\n        def __init__(self, **kwargs):\\n            super(H2OAutoEncoderEstimator, self).__init__(**kwargs)\\n            self._parms[\\'autoencoder\\'] = True\\n\\n\\nextensions = dict(\\n    __module__=module_extensions\\n)\\n\\ndoc = dict(\\n    __class__=\"\"\"\\nBuild a Deep Neural Network model using CPUs\\nBuilds a feed-forward multilayer artificial neural network on an H2OFrame\\n\"\"\"\\n)\\n\\nexamples = dict(\\n    __class__=\"\"\"\\n>>> import h2o\\n>>> from h2o.estimators.deeplearning import H2ODeepLearningEstimator\\n>>> h2o.connect()\\n>>> rows = [[1,2,3,4,0], [2,1,2,4,1], [2,1,4,2,1], [0,1,2,34,1], [2,3,4,1,0]] * 50\\n>>> fr = h2o.H2OFrame(rows)\\n>>> fr[4] = fr[4].asfactor()\\n>>> model = H2ODeepLearningEstimator()\\n>>> model.train(x=range(4), y=4, training_frame=fr)\\n\"\"\",\\n    training_frame=\"\"\"\\n>>> cars = h2o.import_file(\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/junit/cars_20mpg.csv\")\\n>>> cars[\"economy_20mpg\"] = cars[\"economy_20mpg\"].asfactor()\\n>>> predictors = [\"displacement\",\"power\",\"weight\",\"acceleration\",\"year\"]\\n>>> response = \"economy_20mpg\"\\n>>> train, valid = cars.split_frame(ratios = [.8], seed = 1234)\\n>>> cars_gbm = H2ODeepLearningEstimator(seed = 1234)\\n>>> cars_gbm.train(x = predictors,\\n...                y = response,\\n...                training_frame = train,\\n...                validation_frame = valid)\\n>>> cars_gbm.auc(valid=True)\\n\"\"\",\\n)\\n\\n\\n```\\nthe name of the function doesn\\'t matter, it is just loaded by the `gen_python.py` generator, which extracts it\\'s source code, and removes the first line, which is the function signature.\\n\\n\\n#### R\\n\\nfor R the `gen_ALGO.py` template files have the following possible entries (none of them being required):\\n\\n```python\\nrest_api_version = 99   # if algo doesn\\'t use the default 3\\n\\ndef update_params(name, param):\\n    # a hook to modify schema params on the fly: mainly used to remove some enum values...\\n    pass\\n\\nextensions = dict(\\n    required_params=[\\'x\\', \\'y\\', \\'training_frame\\'], #required params on top of signature: list of names or tuple(name, default_value)\\n    frame_params=[\\'training_frame\\', \\'validation_frame\\'], #by default (have special validation handling)\\n    extra_params=[(\\'verbose\\', \\'FALSE\\')],  #additional params not in schema: list of tuple(name, default_value)\\n    validate_required_params=\"specific code to validated required params\",\\n    validate_params=\"specific code to validate params\",\\n    set_required_params=\"specific code to set required params to `parms` object\",\\n    skip_default_set_params_for=[\\'foo\\'], # list of params for which we need to remove default set param mechanism because it will be customized below\\n    set_params=\"specific code to set other params to `parms` object\",\\n    with_model=\"code to postprocess the model\",\\n    module=\"additional R functions added to the module\",\\n)\\n\\ndoc = dict(\\n    preamble=\"doc preamble\",\\n    params=dict(\\n        foo=\"doc for param foo (without tag): used for params not in schema, otherwise overrides help doc from schema\",\\n        bar=\"doc for param bar\",\\n    ),\\n    returns=\"@return doc (without tag)\",\\n    seealso=\"@seealso doc (without tag)\",\\n    references=\"@references doc (without tag)\",\\n    examples=\"@examples doc (code only)\",\\n)\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-clustering/README.md', 'section': '## Flatfile format\\n\\nThis module itself does not make any assumptions about the flatfile formatting, besides the basic ones like\\nan empty or missing request body. The flatfile is handed over to H2O and parsed by `NetworkInit.java` - which contains all the details.\\nIPv4 and IPv6 formats are supported (not the shortened representation of IPv6). Both hostname/host ip and port in a format\\n`hostname:port` must be present.\\n\\n### Example\\n\\nA flatfile with an IPv6 and IPv4 address with ports defined in both cases (mandatory).\\n```\\n[1200:0000:AB00:1234:0000:2552:7777:1313]:54321\\n9.255.255.255:54321\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-clustering/README.md', 'section': '## Distribution\\n\\nIt is assumed this module is NOT part of H2O by default. It must be put onto the classpath manually. As it only relies on the contract of\\n`AbstractEmbeddedH2OConfig`, which is considered to be fixed, it is possible to put this module on a classpath\\nof even older H2Os which contain `AbstractEmbeddedH2OConfig`. This enables assisted clustering even to legacy\\nH2O versions.'}\n",
      "{'filename': 'h2o-3-master/h2o-clustering/README.md', 'section': '## Testing\\n\\nThis module, besides its own test suite, is tested in Kubernetes environment inside the [h2o-k8s](../h2o-k8s/tests/clustering/README.md)\\nmodule.'}\n",
      "{'filename': 'h2o-3-master/h2o-clustering/README.md', 'section': '## Cluster status endpoint\\n\\nThere is an endpoint for H2O Cluster status provided by this module, located via `GET /cluster/status`. \\nIf H2O has not clustered yet, the endpoint `HTTP 204` - No content, according to [RFC-2616](https://tools.ietf.org/html/rfc2616#section-10.2.5).\\nOnce H2O cluster is formed, this endpoint returns `HTTP 200` with a JSON containing two arrays - a list of healthy nodes and a list of\\nunhealthy nodes.\\n\\nAny node of the cluster can be queried and each will return the same response.\\n\\n\\nExample `curl --location --request GET \\'localhost:8080/cluster/status\\'`:\\n\\n```json\\n{\\n  \"leader_node\": \"192.168.0.149:54321\",\\n  \"healthy_nodes\": [\"192.168.0.149:54321\"],\\n  \"unhealthy_nodes\": []\\n}\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-core/src/main/resources/docs/pieces/columnSummary.md', 'section': '## parameters ##\\n\\nname | type | description\\n----- | ----- | -----\\nkey | [Frame](Docs.json/schemas/FrameSchema) | Frame of interest\\ncolumn | string | Name of column of interest'}\n",
      "{'filename': 'h2o-3-master/h2o-core/src/main/resources/docs/pieces/columnSummary.md', 'section': '## Sample URI ##\\n\\nhttp://127.0.0.1:54321/3/Frames.json/allyears2k_headers.hex/columns/Year/summary'}\n",
      "{'filename': 'h2o-3-master/h2o-core/src/main/resources/docs/pieces/columnSummary.md', 'section': '## Sample output ##\\n\\n```json\\n{\\n    \"frames\": [\\n        {\\n            \"key\": null,\\n            \"off\": 0,\\n            \"len\": 100,\\n            \"checksum\": 722728,\\n            \"rows\": 43978,\\n            \"byteSize\": 44058,\\n            \"isText\": false,\\n            \"default_pctiles\": [\\n                0.01,\\n                0.1,\\n                0.25,\\n                0.3333333333333333,\\n                0.5,\\n                0.6666666666666666,\\n                0.75,\\n                0.9,\\n                0.99\\n            ],\\n            \"columns\": [\\n                {\\n                    \"label\": \"Year\",\\n                    \"missing\": 0,\\n                    \"zeros\": 0,\\n                    \"pinfs\": 0,\\n                    \"ninfs\": 0,\\n                    \"mins\": [\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987\\n                    ],\\n                    \"maxs\": [\\n                        2008,\\n                        2008,\\n                        2008,\\n                        2008,\\n                        2008\\n                    ],\\n                    \"mean\": 1997.5,\\n                    \"sigma\": 6.34436090171059,\\n                    \"type\": \"int\",\\n                    \"domain\": null,\\n                    \"data\": [\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987,\\n                        1987\\n                    ],\\n                    \"str_data\": null,\\n                    \"precision\": 0,\\n                    \"bins\": [\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999,\\n                        1999\\n                    ],\\n                    \"base\": 1987,\\n                    \"stride\": 1,\\n                    \"pctiles\": [\\n                        1987.2196098049023,\\n                        1989.1995997999,\\n                        1992.4997498749374,\\n                        1994.3331665832916,\\n                        1998,\\n                        2001.6663331665834,\\n                        2003.4997498749374,\\n                        2006.799899949975,\\n                        2008.7798899449724\\n                    ]\\n                }\\n            ],\\n            \"compatible_models\": null\\n        }\\n    ],\\n    \"compatible_models\": null\\n}\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-dist/README.md', 'section': '## Files\\n\\n* `index.html`: The main template file.\\n* `buildinfo.json`: Metadata describing the build (version, branch name, ..). This is also a template (see strings prefixed with \"SUBST_\").'}\n",
      "{'filename': 'h2o-3-master/h2o-dist/README.md', 'section': '## Local Development\\n\\nIn order to make changes and develop the download page locally, you need to have a valid `buildinfo.json` file. The repository\\nversion is not a valid JSON file as-is - the placeholders need to be substituted for valid values. One way to get a valid\\n`buildinfo.json` file is to download the latest from the actual website, make the substitutions manually or use provided\\nconvenience script (this is the easiest way).\\n\\nScript `local_dev.py` will start a local webserver on port 8080 and make necessary substitutions in `buildinfo.json`\\ntemplate dynamically.\\n\\nRun it using Python 3.x:\\n\\n```\\npython3 local_dev.py\\n```\\n\\nHit `Ctrl-C` to terminate the script.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/README.md', 'section': '## src/product\\n\\nThis folder includes the main product documentation. The documentation is built using [Sphinx](http://www.sphinx-doc.org/) with the [Read The Docs theme](https://sphinx-rtd-theme.readthedocs.io/en/stable/).\\n\\nNote that there are also some directories under the **/product** folder from the original h2o-3 pre-sphinx docs build system.\\n\\n### Requirements\\n\\n- Python 3.6+\\n- [Sphinx](http://www.sphinx-doc.org/) \\n- [Read The Docs theme](https://sphinx-rtd-theme.readthedocs.io/en/stable/)\\n- Additional extensions\\n  - [recommonmark](https://recommonmark.readthedocs.io/en/latest/)\\n  - [sphinx-prompt](https://pypi.org/project/sphinx-prompt/) version 1.1.0\\n  - [sphinx-tabs](https://pypi.org/project/sphinx-tabs/1.1.12/) version 1.1.12\\n  - [sphinx-substitutions-extension](https://pypi.org/project/Sphinx-Substitution-Extensions/2019.6.15.0/) version 2019.6.15.0\\n\\nRun the following to install Sphinx and the RTD theme. \\n\\n```\\npython3 -m pip install sphinx==2.1.1\\npython3 -m pip install sphinx_rtd_theme==0.2.4\\n```\\n\\nRun the following to install the additional required extensions:\\n\\n```\\npython3 -m pip install recommonmark\\npython3 -m pip install sphinx_prompt==1.1.0\\npython3 -m pip install sphinx-tabs==1.1.12\\npython3 -m pip install sphinx_substitution_extensions==2019.6.15.0\\npython3 -m pip install docutils==0.16\\npython3 -m pip install future\\npython3 -m pip install \"jinja2<3.1\"\\n```\\n\\nThe makefile for building the docs is in the **/src/product** folder. Run the following to build the H2O-3 User Guide.\\n\\n```\\ncd src/product\\nmake html\\n```\\n\\nThe output will be available in:\\n\\n> src/product/_build/html/index.html'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/README.md', 'section': '## src/booklets/v2_2015\\n\\nThis folder contains latex source code for H2O-3 booklets. The booklets can be built from the **/h2o-3** folder.\\n\\n```\\ncd ..\\n./gradlew booklets\\n```\\n\\nThe output PDFs are available in:\\n\\n> src/booklets/v2_2015/source'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/README.md', 'section': '## src/api\\n\\nThis folder provides an overview of the H2O-3 REST API and includes an example. The REST API docs are available on the [docs site](https://docs.h2o.ai).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/README.md', 'section': '## src/cheatsheets\\n\\nThis folder includes parity information between R and Python functions and a conversion table for H2ODataFrame to Pandas DataFrame.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/README.md', 'section': '## src/dev\\n\\nThis folder contains information about the H2OApp and about custom functions.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/README.md', 'section': '## src/front\\n\\nThis folder is deprecated and is no longer maintained. It will be removed at a later date.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/StyleGuide.md', 'section': '## Capitalization\\n\\n### Mixed\\n\\n(always capitalize as below)\\n\\n- CoffeeScript\\n- GitHub\\n- H2OFrame\\n- H2Ostream\\n- IDEA IntelliJ (use 1st time, then use either IDEA or IntelliJ after) %Which is more common?\\n- IPython\\n- JUnit\\n- K-fold/N-fold\\n- MapReduce/MapR\\n- MLLib\\n- PyPI\\n- RUnit\\n\\n### Initial Caps\\n\\n(always capitalize as below)\\n\\n- Bayesian\\n- Flow\\n- Gaussian\\n- Gedeon method\\n- Hadoop\\n- Java\\n- Maven\\n- Laplace\\n- Lasso\\n- Nesterov accelerated gradient method\\n- Newton method\\n- Python\\n- Poisson\\n- R\\n- Scala\\n- Spark Shell\\n- Sparkling Water\\n- Terminal\\n- Tweedie\\n\\n\\n### Always All Caps\\n\\nFor any acronym, spell out in 1st use, then provide acronym in parentheses after; for pluralization, add a lowercase \"s\" (POJO -> POJOs, API -> APIs)\\n\\n- ADADELTA \\n- ADMM (alternating direction method of multipliers)\\n- AIC (Akaike information criterion)\\n- AUC (area under curve)\\n- API (application programming interface)\\n- AWS (Amazon Web Services)\\n- CDH (Cloudera Distribution Including Apache Hadoop)\\n- CPU (central processor unit)\\n- CRAN (Comprehensive R Archive Network)\\n- EC2 (Amazon Elastic Compute Cloud)\\n- GBM (Gradient Boosted Models)\\n- GLM (Generalized Linear Modeling)\\n- H2O\\n- HDFS (Hadoop Distributed File System)\\n- ID (identity)\\n- IDE (integrated development environment)\\n- IP (internet protocol)\\n- IRLSM (Iteratively Reweighted Least Squares Method)\\n- JSON (JavaScript Object Notation)\\n- JVM (Java virtual machine)\\n- L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm)\\n- MSE (mean squared error)\\n- NA (not available)\\n- NFS (network file system)\\n- PCA (Principal Components Analysis)\\n- POJO (Plain Old Java Object)\\n- REST (Representational State Transfer)\\n- S3 (Simple Storage Service)\\n- SGD (stochastic gradient descent)\\n- RDD (Resilient Distributed Dataset)\\n- YARN (Yet Another Resource Negotiator) \\n\\n### Always Lowercase\\n\\n(unless starting a sentence or in heading)\\n\\n- alpha\\n- autoencoder\\n- gamma\\n- lambda\\n- rho\\n- tanh'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/StyleGuide.md', 'section': '## Algos\\n\\n- Nave Bayes\\nLATIN SMALL LETTER I WITH DIAERESIS\\nUnicode: U+00EF, UTF-8: C3 AF\\n- Distributed Random Forest (DRF); never just RF\\n- Deep Learning (for all cases, not just our version)\\n- K-means\\n\\n\\n# One word (not two)\\n\\n- Dataset'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/README.md', 'section': '## Examples\\n\\nThe examples below provide a Rosetta Stone-like translation of common use cases across many language environments.\\n\\nGoals include:\\n\\n* Providing at least one environment that a person is familiar with to allow that person to explore a consistent workflow across the other, possibly unfamiliar, environments.\\n\\n* To allow all the workflows to be stared at side-by-side to look for consistencies and inconsistencies during API design.\\n\\n\\n### Data science workflows\\n\\n* [Data science example 1](data-science-example-1/README.md)\\n\\n\\n### Resources\\n\\n* [Numpy for R users](http://mathesaurus.sourceforge.net/r-numpy.html)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': \"## Why?\\n\\nWhen we look to using H2O in a production environment there are two major aspects to consider: model training, and model deployment (aka predictions).\\n\\n### Model Training\\nModel training is the set of steps you use to load your training and optional validation data into H2O, prepare it, and train one or more models.  You will generally do this in Flow, Python, R, Scala or Java.\\n\\nIf you've done this work in Flow, *or* if you want to drive the workflow from another application (e.g., a data collection pipeline), you can use the REST API to automate these steps.\\n\\n### Predictions\\nThere are two ways to use the models you've trained in H2O to generate new predictions:\\n\\n - by exporting your model as a POJO (Plain Old Java object) and incorporating it into a JVM-based application, or\\n - by using the REST API to call a model which is loaded into a running H2O instance.\\n \\nIn general, if you are able to use a POJO it is the better choice because it doesn't depend on running H2O at prediction time.  However if you don't have a JVM-based application to integrate with, or if you want to generate predictions during the model training workflow in order to evaluate your models, you may wish to use the REST API for predictions.\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## What?\\n\\nThe H2O REST API allows you to access all the capabilities of H2O from an external program or script, via JSON over HTTP.  \\n\\nThe REST API is used by the Flow UI, as well as both the R and Python bindings: everything that you can do with those clients can be done by using the REST API, including data import, model building and generating predictions.\\n\\nYou can call the REST API:\\n\\n - from your browser \\n - using browser tools such as *Postman* in Chrome\\n - using *curl*\\n - using the language of your choice\\n\\nGenerated payload POJOs for Java are available as part of the release in a separate bindings Jar file and are simple to generate for other languages if desired.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## Reference Documentation\\n\\nReference documentation for the REST API is available in the Help sidebar in Flow, as well as on the H2O.ai website, [http://docs.h2o.ai/](http://docs.h2o.ai/).  The reference documentation is all generated from the H2O server via the Metadata facilities described below so  it is always up to date.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## Versioning and Stability\\n\\nBoth the endpoints and the payloads for the REST API are versioned for stability; the current stable version for both is 3.  Versions will be supported for some time after a new major version is released to provide time to upgrade your clients.\\n\\nIn general you will want to write to a specific version, such as 3, and upgrade shortly after a new major version is released.  Once we release a new major version of the REST API most new features will be added only to the new version.\\n\\n### Non-breaking changes\\n\\nWe continue to add features to the APIs, but we only allow *non-breaking changes* in a published API such as version 3.  Breaking changes force a new major version number.\\n\\nA non-breaking change will not change the behavior of a well-written client.  Examples of non-breaking changes are:\\n\\n* adding additional output fields to a response  \\n* adding a parameter with a default value that maintains the old behavior if the parameter is omitted\\n\\nWe test backward compatibility by running a full set of tests against each new release (including nightlies) using old releases of the Flow, R, and Python clients.\\n\\n### The EXPERIMENTAL version\\n\\nFeatures that are under development and are not yet stable use version 99, which indicates that they may change between releases.  Once those features become stable, we change the version from 99 to the current stable version.\\n\\nFor request URLs, you may use EXPERIMENTAL as the version number to make it clear in your client code that you are making requests to an experimental endpoint:\\n\\n`GET http://127.0.0.1:54321/EXPERIMENTAL/Sample`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## URLs\\n\\nYour H2O cluster is typically referenced by the host name and HTTP port of the first server in the cluster.  By default this is *http://localhost:54321* (or *https://localhost:54321*, if you have an enterprise license).  Append the endpoint request URI to this to form your request URL.\\n\\nH2O REST API URIs begin with a version followed by a resource type, such as */3/Frames* or */3/Models* or */3/Cloud*.  Typically a GET to this kind of resource collection URI will return all the instances of the resource type.\\n\\nAll endpoints that deal with a resource type will begin with the same prefix.  As an example, *GET /3/Frames* returns the list of all *Frames*, while *GET /3/Frames/my\\\\_frame* returns the *Frame*  named *my\\\\_frame*.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## HTTP Verbs\\n\\nAs is standard for REST APIs, the HTTP verbs GET, HEAD, POST and DELETE are used to interact with the resources in the server.\\n\\n- **GET** requests fetch data and do not cause side effects.  All parameters for the request are contained within the URL, either within the path (e.g., /3/Frames/*my\\\\_frame\\\\_name*/*a\\\\_column\\\\_name*) or as query parameters (e.g., /3/Frames/my\\\\_frame\\\\_name*?row\\\\_offset=10000&row\\\\_count=1000*)\\n\\n- **HEAD** requests return just the HTTP status for accessing the resource.\\n\\n- **POST** requests create a new object within the H2O cluster.  Examples are importing or parsing a file into a Frame or training a new Model.  Some parameters may be given in the URL, but most are given using a request *schema*.  The fields of the request schema are sent in the POST body using *x-www-form-urlencoded* format, like an HTML form.  More on this below in the **Formats** secion.\\n\\n  A future version of H2O will move to using *application/json*.\\n\\n- **DELETE** requests delete an object, generally from the distributed object store.\\n\\n- **PUT** is used for requests that modify objects; it is not used yet.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## HTTP Status Codes\\n\\nH2O uses standard HTTP status codes for all its responses.  Refer to [Wikipedia](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) for more information on their meanings.\\n\\nThe status codes currently used by H2O are:\\n\\n* **200 OK** (all is well)\\n* **400 Bad Request** (the request URL is bad)\\n* **404 Not Found** (a specified object was not found)\\n* **412 Precondition Failed** (bad parameters or other problem handling the request)\\n* **500 Internal Server Error** (unanticipated failure occurred in the server)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## Formats\\n\\nThe payloads for each endpoint are implemented as versioned *schemas*.  These schemas are self-describing for simplicity and ease of implementation, especially if you persist them for later.\\n\\n### Schemas\\n\\nSchemas specify all the relevant properties of each field of an input or response including name, type, default value, help string, direction (*in, out* or *inout*), whether or not input fields are required and how important they are to specify, allowed values for enumerated fields, and so on.  Schema fields can be simple values or nested schemas, or arrays or dictionaries (maps) of these.\\n\\nThis example shows the *model_id* field returned by a model builder call:\\n\\n            \"parameters\": [\\n                {\\n                    \"__meta\": {\\n                        \"schema_name\": \"ModelParameterSchemaV3\",\\n                        \"schema_type\": \"Iced\",\\n                        \"schema_version\": 3\\n                    },\\n                    \"actual_value\": {\\n                        \"URL\": \"/3/Models/prostate_glm\",\\n                        \"__meta\": {\\n                            \"schema_name\": \"ModelKeyV3\",\\n                            \"schema_type\": \"Key<Model>\",\\n                            \"schema_version\": 3\\n                        },\\n                        \"name\": \"prostate_glm\",\\n                        \"type\": \"Key<Model>\"\\n                    },\\n                    \"default_value\": null,\\n                    \"help\": \"Destination id for this model; auto-generated if not specified\",\\n                    \"level\": \"critical\",\\n                    \"name\": \"model_id\",\\n                    \"required\": false,\\n                    \"type\": \"Key<Model>\",\\n                    \"values\": []\\n                },\\n                ...\\n            ],\\n            ...\\n### POST bodies\\nThe fields of the request schema are sent in the POST body using *x-www-form-urlencoded* format, like an HTML form.  A future version of H2O will move to using *application/json*.  In the meantime, complex fields such as arrays are POSTed in the same format they would be in the JSON.  For example, an array of ints might be posted in a field as [1, 10, 100].  Note the array of strings for the *ignored_columns* parameter in this GLM model builder POST body:\\n\\n    model_id=prostate_glm&training_frame=prostate.hex&nfolds=0&response_column=CAPSULE&ignored_columns=%5B%22%22%5D&ignore_const_cols=true&family=binomial&solver=AUTO&alpha=&lambda=&lambda_search=false&standardize=true&non_negative=false&score_each_iteration=false&max_iterations=-1&link=family_default&intercept=true&objective_epsilon=0.00001&beta_epsilon=0.0001&gradient_epsilon=0.0001&prior=-1&max_active_predictors=-1\\n\\nThe value is [\"ID\"], urlencoded as %5B%22ID%22%5D.\\n\\n### Metadata\\nThe formats of all payloads (*schemas*) are available dynamically from the server using the */Metadata/schemas* endpoints. You can fetch additional metadata for model builder (model algorithm) parameters from the */ModelBuilders* endpoints.  This metadata allows you to write a client that automatically adapts to new fields.  \\n\\nAs an example, Flow has no hardwired knowledge of any of the model algos.  It discovers the list of algos and  their parameter information dynamically.  This means that if you extend H2O with new algorithms or new fields for the built-in algorithms Flow will Just Work (tm).\\n\\nSimilarly, all the endpoints (URL patterns) are described dynamically by the */Metadata/endpoints* endpoints.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## Error Condition Payloads\\n\\nAll errors return one of the non-2xx HTTP status codes mentioned above and return standardized error payloads.  These contain an end-user-directed message, a developer-oriented message, the HTTP status, an optional dictionary of relevant values, and exception information if applicable.\\n\\nHere is the result of requesting a Frame that is not present in the server: \\n\\n`GET http://127.0.0.1:54321/3/Frames/missing_frame`\\n\\n            {\\n                \"__meta\": {\\n                    \"schema_version\": 3,\\n                    \"schema_name\": \"H2OErrorV3\",\\n                    \"schema_type\": \"H2OError\"\\n                },\\n                \"timestamp\": 1438634936808,\\n                \"error_url\": \"/3/Frames/missing_frame\",\\n                \"msg\": \"Object \\'missing_frame\\' not found for argument: key\",\\n                \"dev_msg\": \"Object \\'missing_frame\\' not found for argument: key\",\\n                \"http_status\": 404,\\n                \"values\": {\\n                    \"argument\": \"key\",\\n                    \"name\": \"missing_frame\"\\n                },\\n                \"exception_type\": \"water.exceptions.H2OKeyNotFoundArgumentException\",\\n                \"exception_msg\": \"Object \\'missing_frame\\' not found for argument: key\",\\n                \"stacktrace\": [\\n                    \"water.api.FramesHandler.getFromDKV(FramesHandler.java:154)\",\\n                    \"water.api.FramesHandler.doFetch(FramesHandler.java:239)\",\\n                    \"water.api.FramesHandler.fetch(FramesHandler.java:225)\",\\n                    ...'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## Control query parameters\\n\\nH2O also supports \"meta\" query parameters to control the result payload.  Currently the only one is *exclude_fields*, but more will be supported in subsequent releases.\\n\\n### exclude_fields\\nThe result payload of some calls can get quite large.  For example, a Frame or a Model built with a Frame that has 5,000 categorical columns may have a very large list of *domains*, or categorical levels.  \\n\\nIf you don\\'t require the server to return certain fields you can use the *exclude_fields* query parameter to exclude them.  This reduces the size of the result, sometimes considerably, which speeds up JSON parsing in the client and reduces the chance that limited memory clients such as web browsers will run out of memory while processing the result.\\n\\nThe *exclude_fields* parameter accepts a comma-separated list of field names.  Nested field names are separated by slashes.\\n\\nAs an example, one call of Flow to /Frames/{frame_id} uses this query parameter:\\n\\n    exclude_fields=frames/vec_ids,frames/columns/data,frames/columns/domain,frames/columns/histogram_bins,frames/columns/percentiles'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': \"## Example Endpoints\\n\\nThis section lists a few endpoints to give you an idea of the functions that are available through the REST API.  The reference documentation contains the full list.\\n\\nRemember, Flow and the R and Python bindings access H2O only through the REST API, so if you find functionality in those clients, you'll find it in the REST API as well.  The only caveat is data munging (e.g., slicing, creating new columns, etc); that functionality is available through the /99/Rapids endpoint, which is under rapid change (pun intended).  Contact us if you need to access those functions through the REST API.\\n\\n### Loading and parsing data files\\n    GET /3/ImportFiles\\n    Import raw data files into a single-column H2O Frame.\\n\\n    POST /3/ParseSetup\\n    Guess the parameters for parsing raw byte-oriented data into an H2O Frame.\\n\\n    POST /3/Parse\\n    Parse a raw byte-oriented Frame into a useful columnar data Frame.\\n\\n### Frames\\n\\n    GET /3/Frames\\n    Return all Frames in the H2O distributed K/V store.\\n    \\n    GET /3/Frames/(?.*)\\n    Return the specified Frame.\\n\\n    GET /3/Frames/(?.*)/summary\\n    Return a Frame, including the histograms, after forcing computation of rollups.\\n\\n    GET /3/Frames/(?.*)/columns/(?.*)/summary\\n    Return the summary metrics for a column, e.g. mins, maxes, mean, sigma, percentiles, etc.\\n\\n    DELETE /3/Frames/(?.*)\\n    Delete the specified Frame from the H2O distributed K/V store.\\n\\n    DELETE /3/Frames\\n    Delete all Frames from the H2O distributed K/V store.\\n\\n### Building models\\n\\n    GET /3/ModelBuilders\\n    Return the Model Builder metadata for all available algorithms.\\n\\n    GET /3/ModelBuilders/(?.*)\\n    Return the Model Builder metadata for the specified algorithm.\\n\\n    POST /3/ModelBuilders/deeplearning/parameters\\n    Validate a set of Deep Learning model builder parameters.\\n\\n    POST /3/ModelBuilders/deeplearning\\n    Train a Deep Learning model on the specified Frame.\\n\\n    POST /3/ModelBuilders/glm/parameters\\n    Validate a set of GLM model builder parameters.\\n\\n    POST /3/ModelBuilders/glm\\n    Train a GLM model on the specified Frame.\\n\\n    ...\\n### Accessing and using models\\n\\n    GET /3/Models\\n    Return all Models from the H2O distributed K/V store.\\n\\n    GET /3/Models/(?.*?)(\\\\.java)?\\n    Return the specified Model from the H2O distributed K/V store, optionally with the list of compatible Frames.  Using the .java extension will return the Java POJO.\\n\\n    POST /3/Predictions/models/(?.*)/frames/(?.*)\\n    Score (generate predictions) for the specified Frame with the specified Model. Both the Frame of predictions and the metrics will be returned.\\n\\n    DELETE /3/Models/(?.*)\\n    Delete the specified Model from the H2O distributed K/V store.\\n\\n    DELETE /3/Models\\n    Delete all Models from the H2O distributed K/V store.\\n\\n### Administrative and utility\\n\\n    GET /3/About\\n    Return information about this H2O cluster.\\n\\n    GET /3/Cloud\\n    Determine the status of the nodes in the H2O cloud.\\n\\n    HEAD /3/Cloud\\n    Determine the status of the nodes in the H2O cloud.\\n\\n\\n### Job management and polling\\n\\n    GET /3/Jobs\\n    Get a list of all the H2O Jobs (long-running actions).\\n    \\n    GET /3/Jobs/(?.*)\\n    Get the status of the given H2O Job (long-running action).\\n\\n    POST /3/Jobs/(?.*)/cancel\\n    Cancel a running job.\\n    \\n### Persistence\\n\\n    POST /3/Frames/(?.*)/export\\n    Export a Frame to the given path with optional overwrite.\\n    \\n    POST /99/Models.bin/(?.*)\\n    Import given binary model into H2O.\\n\\n    GET /99/Models.bin/(?.*)\\n    Export given model.\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md', 'section': '## GBM_Example.flow\\n\\nThis section reproduces the Flow example *GBM_Example.flow* using three languages:\\n\\n * *Coffeescript* (Flow\\'s control language), \\n * *curl*, and \\n * *Java* (making REST API calls from an external Java program).\\n\\nThe Java code uses the bindings found in H2O\\'s *h2o-bindings.jar*, which use the *Retrofit* REST API library from Square: [http://square.github.io/retrofit/](http://square.github.io/retrofit/).  \\n\\nIf you use Java / Retrofit you\\'ll get the results back from the server as Java objects.  For brevity these are not shown here.  \\n\\nThe source files for the payload schemas and for the endpoint proxies are documented with help text for the fields and Javadoc for the methods, and the default constructors set each field to its default value.  Each payload schema class includes a *toString()* method to ease debugging.\\n\\nIf you have downloaded H2O as a zip file you\\'ll find the Java Retrofit bindings in *bindings/java/h2o-bindings-{version}.jar* and the sources in *bindings/java/h2o-bindings-sources.jar*.  If you have built H2O from source you\\'ll find them in *h2o-bindings/build/libs/h2o-bindings.jar* and the sources in *h2o-bindings/build/src-gen/main/java/water/bindings/*.\\n\\n### GBM_Example.flow, Step 1: Import\\nIn Flow:\\n\\n```coffeescript\\nimportFiles [\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"]\\n```\\n    \\nIn curl:\\n\\n```bash\\ncurl -X GET http://127.0.0.1:54321/3/ImportFiles?path=http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\\n```\\n\\nIn Java / Retrofit:\\n\\n```java\\nImportFilesV3 importBody = \\nimportService.importFiles(\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\", null).execute().body();\\n```\\n\\nResult JSON:\\n\\n```\\n{\\n    \"__meta\": {\\n        \"schema_name\": \"ImportFilesV3\",\\n        \"schema_type\": \"Iced\",\\n        \"schema_version\": 3\\n    },\\n    \"_exclude_fields\": \"\",\\n    \"dels\": [],\\n    \"destination_frames\": [\\n        \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"\\n    ],\\n    \"fails\": [],\\n    \"files\": [\\n        \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"\\n    ],\\n    \"path\": \"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"\\n}\\n\\n```\\n\\n### GBM_Example.flow, Step 2: ParseSetup\\n\\nIn Flow:\\n\\n```coffeescript\\nsetupParse paths: [\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"]\\n```\\n\\nIn curl:\\n\\n```bash\\ncurl -X POST http://127.0.0.1:54321/3/ParseSetup --data \\'source_frames=[\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"]\\'\\n```\\n\\nIn Java / Retrofit:\\n\\n```java\\nParseSetupV3 parseSetupBody = \\nparseSetupService.guessSetup(importBody.destination_frames, ParserParserType.GUESS, (byte)\\',\\', false, -1, null, null, null, null, 0, 0, 0, null).execute().body();\\n```\\n\\n\\n### GBM_Example.flow, Step 2 Result\\n\\n```javascript\\n{\\n  \"__meta\": {\\n    \"schema_version\": 3,\\n    \"schema_name\": \"ParseSetupV3\",\\n    \"schema_type\": \"ParseSetup\"\\n  },\\n  \"_exclude_fields\": \"\",\\n  \"source_frames\": [\\n    {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"FrameKeyV3\",\\n        \"schema_type\": \"Key<Frame>\"\\n      },\\n      \"name\": \"http:\\\\/\\\\/s3.amazonaws.com\\\\/h2o-public-test-data\\\\/smalldata\\\\/flow_examples\\\\/arrhythmia.csv.gz\",\\n      \"type\": \"Key<Frame>\",\\n      \"URL\": \"\\\\/3\\\\/Frames\\\\/http:\\\\/\\\\/s3.amazonaws.com\\\\/h2o-public-test-data\\\\/smalldata\\\\/flow_examples\\\\/arrhythmia.csv.gz\"\\n    }\\n  ],\\n  \"parse_type\": \"CSV\",\\n  \"separator\": 44,\\n  \"single_quotes\": false,\\n  \"check_header\": -1,\\n  \"column_names\": null,\\n  \"column_types\": [\\n    \"Numeric\",\\n    ...\\n  ],\\n  \"na_strings\": null,\\n  \"column_name_filter\": null,\\n  \"column_offset\": 0,\\n  \"column_count\": 0,\\n  \"destination_frame\": \"arrhythmia.hex\",\\n  \"header_lines\": 0,\\n  \"number_columns\": 280,\\n  \"data\": [\\n    [\\n      \"75\",\\n      \"0\",\\n      \"190\",\\n      ...\\n   ]\\n   ...\\n  ],\\n  \"chunk_size\": 4194304,\\n  \"total_filtered_column_count\": 280\\n}\\n```\\n\\n### GBM_Example.flow, Step 3: Parse\\n\\nIn Flow:\\n\\n```coffeescript\\nparseFiles\\n  paths: [\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"]\\n  destination_frame: \"arrhythmia.hex\"\\n  parse_type: \"CSV\"\\n  separator: 44\\n  number_columns: 280\\n  single_quotes: false\\n  column_names: null\\n  column_types: [\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\"]\\n  delete_on_done: true\\n  check_header: -1\\n  chunk_size: 4194304\\n```\\n\\nIn curl:\\n\\n```bash\\ncurl -X POST http://127.0.0.1:54321/3/Parse --data \\'destination_frame=arrhythmia.hex&source_frames=[\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\"]&parse_type=CSV&separator=44&number_columns=280&single_quotes=false&column_names=&column_types=[\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\"]&check_header=-1&delete_on_done=true&chunk_size=4194304\\'\\n\\n```\\n\\nIn Java / Retrofit:\\n\\n```java\\nList<String> source_frames = new ArrayList<>();\\nfor (FrameKeyV3 frame : parseSetupBody.source_frames)\\n  source_frames.add(frame.name);\\n\\nParseV3 parseBody = parseService.parse(\"arrhythmia.hex\",\\n   source_frames.toArray(new String[0]),\\n   parseSetupBody.parse_type,\\n   parseSetupBody.separator,\\n   parseSetupBody.single_quotes,\\n   parseSetupBody.check_header,\\n   parseSetupBody.number_columns,\\n   parseSetupBody.column_names,\\n   parseSetupBody.column_types,\\n   null, // domains\\n   parseSetupBody.na_strings,\\n   parseSetupBody.chunk_size,\\n   true,\\n   true,\\n   null).execute().body();\\n```\\n\\n\\n\\n### GBM_Example.flow, Step 3 Result\\n\\n```javascript\\n{\\n  \"__meta\": {\\n    \"schema_version\": 3,\\n    \"schema_name\": \"ParseV3\",\\n    \"schema_type\": \"Iced\"\\n  },\\n  \"_exclude_fields\": \"\",\\n  \"destination_frame\": {\\n    \"__meta\": {\\n      \"schema_version\": 3,\\n      \"schema_name\": \"FrameKeyV3\",\\n      \"schema_type\": \"Key<Frame>\"\\n    },\\n    \"name\": \"arrhythmia.hex\",\\n    \"type\": \"Key<Frame>\",\\n    \"URL\": \"\\\\/3\\\\/Frames\\\\/arrhythmia.hex\"\\n  },\\n  \"source_frames\": [\\n    {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"FrameKeyV3\",\\n        \"schema_type\": \"Key<Frame>\"\\n      },\\n      \"name\": \"http:\\\\/\\\\/s3.amazonaws.com\\\\/h2o-public-test-data\\\\/smalldata\\\\/flow_examples\\\\/arrhythmia.csv.gz\",\\n      \"type\": \"Key<Frame>\",\\n      \"URL\": \"\\\\/3\\\\/Frames\\\\/http:\\\\/\\\\/s3.amazonaws.com\\\\/h2o-public-test-data\\\\/smalldata\\\\/flow_examples\\\\/arrhythmia.csv.gz\"\\n    }\\n  ],\\n  \"parse_type\": \"CSV\",\\n  \"separator\": 44,\\n  \"single_quotes\": false,\\n  \"check_header\": -1,\\n  \"number_columns\": 280,\\n  \"column_names\": null,\\n  \"column_types\": [\\n    \"Numeric\",\\n    \"Numeric\",\\n...\\n    \"Numeric\",\\n    \"Numeric\",\\n    \"Numeric\"\\n  ],\\n  \"domains\": null,\\n  \"na_strings\": null,\\n  \"chunk_size\": 4194304,\\n  \"delete_on_done\": true,\\n  \"blocking\": false,\\n  \"remove_frame\": false,\\n  \"job\": {\\n    \"__meta\": {\\n      \"schema_version\": 3,\\n      \"schema_name\": \"JobV3\",\\n      \"schema_type\": \"Job\"\\n    },\\n    \"key\": {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"JobKeyV3\",\\n        \"schema_type\": \"Key<Job>\"\\n      },\\n      \"name\": \"$03010a010a7f32d4ffffffff$_b98fc5bba38d21ea53da2a0834c44f7a\",\\n      \"type\": \"Key<Job>\",\\n      \"URL\": \"\\\\/3\\\\/Jobs\\\\/$03010a010a7f32d4ffffffff$_b98fc5bba38d21ea53da2a0834c44f7a\"\\n    },\\n    \"description\": \"Parse\",\\n    \"status\": \"RUNNING\",\\n    \"progress\": 0,\\n    \"progress_msg\": \"Ingesting files.\",\\n    \"start_time\": 1438888896402,\\n    \"msec\": 65,\\n    \"dest\": {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"FrameKeyV3\",\\n        \"schema_type\": \"Key<Frame>\"\\n      },\\n      \"name\": \"arrhythmia.hex\",\\n      \"type\": \"Key<Frame>\",\\n      \"URL\": \"\\\\/3\\\\/Frames\\\\/arrhythmia.hex\"\\n    },\\n    \"exception\": null,\\n    \"messages\": [\\n      \\n    ],\\n    \"error_count\": 0\\n  },\\n  \"rows\": 0,\\n  \"vec_ids\": null\\n}\\n\\n```\\n\\n### GBM_Example.flow, Step 4: Poll for job completion\\n\\nFlow polls for Job completion automagically:\\n\\n\\n![inline fill](parse_job_polling.png)\\n\\n\\nIn curl:\\n\\n```bash\\ncurl -X GET \\'http://127.0.0.1:54321/3/Jobs/%2403010a010a7f32d4ffffffff%24_b98fc5bba38d21ea53da2a0834c44f7a\\'\\n```\\n\\n### GBM_Example.flow, Step 4: Result\\n\\n```javascript\\n{\\n  \"__meta\": {\\n    \"schema_version\": 3,\\n    \"schema_name\": \"JobsV3\",\\n    \"schema_type\": \"Iced\"\\n  },\\n  \"_exclude_fields\": \"\",\\n  \"job_id\": {\\n    \"URL\": \"\\\\/3\\\\/Jobs\\\\/$03010a010a7f32d4ffffffff$_b98fc5bba38d21ea53da2a0834c44f7a\"\\n  },\\n  \"jobs\": [\\n    {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"JobV3\",\\n        \"schema_type\": \"Job\"\\n      },\\n      \"key\": {\\n        \"__meta\": {\\n          \"schema_version\": 3,\\n          \"schema_name\": \"JobKeyV3\",\\n          \"schema_type\": \"Key<Job>\"\\n        },\\n        \"name\": \"$03010a010a7f32d4ffffffff$_b98fc5bba38d21ea53da2a0834c44f7a\",\\n        \"type\": \"Key<Job>\",\\n        \"URL\": \"\\\\/3\\\\/Jobs\\\\/$03010a010a7f32d4ffffffff$_b98fc5bba38d21ea53da2a0834c44f7a\"\\n      },\\n      \"description\": \"Parse\",\\n      \"status\": \"RUNNING\",\\n      \"progress\": 1,\\n      \"progress_msg\": \"Ingesting files.\",\\n      \"start_time\": 1438888896402,\\n      \"msec\": 267,\\n      \"dest\": {\\n        \"__meta\": {\\n          \"schema_version\": 3,\\n          \"schema_name\": \"FrameKeyV3\",\\n          \"schema_type\": \"Key<Frame>\"\\n        },\\n        \"name\": \"arrhythmia.hex\",\\n        \"type\": \"Key<Frame>\",\\n        \"URL\": \"\\\\/3\\\\/Frames\\\\/arrhythmia.hex\"\\n      },\\n      \"exception\": null,\\n      \"messages\": [\\n        \\n      ],\\n      \"error_count\": 0\\n    }\\n  ]\\n}\\n```\\n\\n### GBM_Example.flow, Step 5: Train the Model\\n\\nIn Flow:\\n\\n```coffeescript\\nbuildModel \\'gbm\\', {\"model_id\":\"gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\",\"training_frame\":\"arrhythmia.hex\",\"score_each_iteration\":false,\"response_column\":\"C1\",\"ntrees\":\"20\",\"max_depth\":5,\"min_rows\":\"25\",\"nbins\":20,\"learn_rate\":\"0.3\",\"distribution\":\"AUTO\",\"balance_classes\":false,\"max_confusion_matrix_size\":20,\"class_sampling_factors\":[],\"max_after_balance_size\":5,\"seed\":0}\\n```\\n\\nIn curl:\\n\\n```bash\\ncurl -X POST http://127.0.0.1:54321/3/ModelBuilders/gbm --data \\'model_id=gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1&training_frame=arrhythmia.hex&score_each_iteration=false&response_column=C1&ntrees=20&max_depth=5&min_rows=25&nbins=20&learn_rate=0.3&distribution=AUTO&balance_classes=false&max_confusion_matrix_size=20&class_sampling_factors=&max_after_balance_size=5&seed=0\\'\\n```\\n\\nIn Java / Retrofit:\\n\\n```java\\nGBMParametersV3 gbm_parms = new GBMParametersV3();\\n\\nFrameKeyV3 training_frame = new FrameKeyV3();\\ntraining_frame.name = \"arrhythmia.hex\";\\n\\ngbm_parms.training_frame = training_frame;\\n\\nColSpecifierV3 response_column = new ColSpecifierV3();\\nresponse_column.column_name = \"C1\";\\ngbm_parms.response_column = response_column;\\n\\nGBMV3 gbmBody = (GBMV3)ModelBuilders.Helper.train_gbm(modelBuildersService, gbm_parms).execute().body();\\n\\n```\\n\\n### GBM_Example.flow, Step 5: Result\\n\\n```javascript\\n{\\n  \"__meta\": {\\n    \"schema_version\": 3,\\n    \"schema_name\": \"GBMV3\",\\n    \"schema_type\": \"GBM\"\\n  },\\n  \"_exclude_fields\": \"\",\\n  \"job\": {\\n    \"__meta\": {\\n      \"schema_version\": 3,\\n      \"schema_name\": \"JobV3\",\\n      \"schema_type\": \"Job\"\\n    },\\n    \"key\": {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"JobKeyV3\",\\n        \"schema_type\": \"Key<Job>\"\\n      },\\n      \"name\": \"$03010a010a7f32d4ffffffff$_881e60f52af792b71d20540604b742dd\",\\n      \"type\": \"Key<Job>\",\\n      \"URL\": \"\\\\/3\\\\/Jobs\\\\/$03010a010a7f32d4ffffffff$_881e60f52af792b71d20540604b742dd\"\\n    },\\n    \"description\": \"GBM\",\\n    \"status\": \"RUNNING\",\\n    \"progress\": 0,\\n    \"progress_msg\": \"Running...\",\\n    \"start_time\": 1438888898858,\\n    \"msec\": 185,\\n    \"dest\": {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"ModelKeyV3\",\\n        \"schema_type\": \"Key<Model>\"\\n      },\\n      \"name\": \"gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\",\\n      \"type\": \"Key<Model>\",\\n      \"URL\": \"\\\\/3\\\\/Models\\\\/gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\"\\n    },\\n    \"exception\": null,\\n    \"messages\": [\\n      \\n    ],\\n    \"error_count\": 0\\n  },\\n  \"algo\": \"gbm\",\\n  \"algo_full_name\": \"Gradient Boosting Machine\",\\n  \"can_build\": [\\n    \"Regression\",\\n    \"Binomial\",\\n    \"Multinomial\"\\n  ],\\n  \"visibility\": \"Stable\",\\n  \"messages\": [\\n    \\n  ],\\n  \"error_count\": 0,\\n  \"parameters\": [\\n    {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"ModelParameterSchemaV3\",\\n        \"schema_type\": \"Iced\"\\n      },\\n      \"name\": \"model_id\",\\n      \"help\": \"Destination id for this model; auto-generated if not specified\",\\n      \"required\": false,\\n      \"type\": \"Key<Model>\",\\n      \"default_value\": null,\\n      \"actual_value\": {\\n        \"__meta\": {\\n          \"schema_version\": 3,\\n          \"schema_name\": \"ModelKeyV3\",\\n          \"schema_type\": \"Key<Model>\"\\n        },\\n        \"name\": \"gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\",\\n        \"type\": \"Key<Model>\",\\n        \"URL\": \"\\\\/3\\\\/Models\\\\/gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\"\\n      },\\n      \"level\": \"critical\",\\n      \"values\": [\\n        \\n      ],\\n      \"is_member_of_frames\": [\\n        \\n      ],\\n      \"is_mutually_exclusive_with\": [\\n        \\n      ]\\n    }, ...\\n  ]\\n}\\n```\\n\\n### GBM_Example.flow, Step 6: Poll for job completion\\n\\nSame as for Parse\\n\\n### GBM_Example.flow, Step 7: View the Model\\n\\nIn Flow:\\n\\n```coffeescript\\ngetModel \"gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\"\\n```\\n\\nIn curl:\\n\\n```bash\\ncurl -X GET \\'http://127.0.0.1:54321/3/Models/gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\\'\\n```\\n\\nIn Java /Retrofit:\\n\\n```java\\nKeyV3 model_key = job.dest;\\nModelsV3 models = modelsService.fetch(model_key.name).execute().body();\\nSystem.out.println(\"new GBM model: \" + models.models[0]);\\n```\\n\\n### GBM_Example.flow, Step 7: Result\\n\\n```javascript\\n{\\n  \"models\": [\\n    {\\n      \"model_id\": {\\n        \"URL\": \"\\\\/3\\\\/Models\\\\/gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\"\\n      },\\n      \"algo\": \"gbm\",\\n      \"algo_full_name\": \"Gradient Boosting Machine\",\\n      \"parameters\": [\\n\\t  ...\\n      ],\\n      \"output\": {\\n        \"__meta\": {\\n          \"schema_name\": \"GBMModelOutputV3\",\\n        },\\n        \"model_category\": \"Regression\",\\n\\t  ...\\n        \"scoring_history\": {\\n\\t    ...\\n        },\\n        \"training_metrics\": {\\n          \"model_category\": \"Regression\",\\n          \"MSE\": 31.32188458883,\\n          \"r2\": 0.88422887487626,\\n          \"mean_residual_deviance\": 31.32188458883\\n        },\\n        \"status\": \"DONE\",\\n        \"run_time\": 3211,\\n     },\\n    }\\n  ],\\n}\\n\\n```\\n\\n### GBM_Example.flow, Step 8: Predictions\\n\\nIn Flow:\\n\\n```coffeescript\\npredict model: \"gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\", frame: \"arrhythmia.hex\", predictions_frame: \"prediction-9d6f23f3-45c2-4e1f-a48e-393b1b7de6db\"\\n```\\n\\nIn curl:\\n\\n```bash\\ncurl -X POST \\'http://127.0.0.1:54321/3/Predictions/models/gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1/frames/arrhythmia.hex\\' --data \\'predictions_frame=prediction-9d6f23f3-45c2-4e1f-a48e-393b1b7de6db\\'\\n```\\n\\nIn Java / Retrofit:\\n\\n```java\\nModelMetricsListSchemaV3 predictions = predictionsService.predict(model_key.name, \\n      training_frame.name, \\n      \"predictions\",\\n      false, false, -1, false, false, false, false, null).execute().body();\\n```\\n\\n### GBM_Example.flow, Step 8: Result\\n\\n```javascript\\n{\\n  \"__meta\": {\\n    \"schema_version\": 3,\\n    \"schema_name\": \"ModelMetricsListSchemaV3\",\\n    \"schema_type\": \"ModelMetricsList\"\\n  },\\n  \"predictions_frame\": {\\n    \"__meta\": {\\n      \"schema_version\": 3,\\n      \"schema_name\": \"FrameKeyV3\",\\n      \"schema_type\": \"Key<Frame>\"\\n    },\\n    \"name\": \"prediction-9d6f23f3-45c2-4e1f-a48e-393b1b7de6db\",\\n    \"type\": \"Key<Frame>\",\\n    \"URL\": \"\\\\/3\\\\/Frames\\\\/prediction-9d6f23f3-45c2-4e1f-a48e-393b1b7de6db\"\\n  },\\n  \"model_metrics\": [\\n    {\\n      \"__meta\": {\\n        \"schema_version\": 3,\\n        \"schema_name\": \"ModelMetricsRegressionV3\",\\n        \"schema_type\": \"ModelMetricsRegression\"\\n      },\\n      \"model\": {\\n        \"__meta\": {\\n          \"schema_version\": 3,\\n          \"schema_name\": \"ModelKeyV3\",\\n          \"schema_type\": \"Key<Model>\"\\n        },\\n        \"name\": \"gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\",\\n        \"type\": \"Key<Model>\",\\n        \"URL\": \"\\\\/3\\\\/Models\\\\/gbm-51b9780b-70d0-40d0-9b5a-c723a3f358c1\"\\n      },\\n      \"model_checksum\": 7.1488755500207e+18,\\n      \"frame\": {\\n        \"__meta\": {\\n          \"schema_version\": 3,\\n          \"schema_name\": \"FrameKeyV3\",\\n          \"schema_type\": \"Key<Frame>\"\\n        },\\n        \"name\": \"arrhythmia.hex\",\\n        \"type\": \"Key<Frame>\",\\n        \"URL\": \"\\\\/3\\\\/Frames\\\\/arrhythmia.hex\"\\n      },\\n      \"frame_checksum\": -1.6112849483913e+17,\\n      \"model_category\": \"Regression\",\\n      \"scoring_time\": 1438888905373,\\n      \"predictions\": {\\n        \"__meta\": {\\n          \"schema_version\": 3,\\n          \"schema_name\": \"FrameV3\",\\n          \"schema_type\": \"Frame\"\\n        },\\n        \"frame_id\": {\\n          \"__meta\": {\\n            \"schema_version\": 3,\\n            \"schema_name\": \"FrameKeyV3\",\\n            \"schema_type\": \"Key<Frame>\"\\n          },\\n          \"name\": \"prediction-9d6f23f3-45c2-4e1f-a48e-393b1b7de6db\",\\n          \"type\": \"Key<Frame>\",\\n          \"URL\": \"\\\\/3\\\\/Frames\\\\/prediction-9d6f23f3-45c2-4e1f-a48e-393b1b7de6db\"\\n        },\\n        \"byte_size\": 3684,\\n        \"is_text\": false,\\n        \"row_offset\": 0,\\n        \"row_count\": 100,\\n        \"column_offset\": 0,\\n        \"column_count\": 1,\\n        \"total_column_count\": 1,\\n        \"checksum\": 3.1483215706755e+18,\\n        \"rows\": 452,\\n        \"columns\": [\\n          {\\n            \"__meta\": {\\n              \"schema_version\": 3,\\n              \"schema_name\": \"ColV3\",\\n              \"schema_type\": \"Vec\"\\n            },\\n            \"label\": \"predict\",\\n            \"missing_count\": 0,\\n            \"zero_count\": 0,\\n            \"positive_infinity_count\": 0,\\n            \"negative_infinity_count\": 0,\\n            \"mins\": null,\\n            \"maxs\": null,\\n            \"mean\": 0,\\n            \"sigma\": 0,\\n            \"type\": \"real\",\\n            \"domain\": null,\\n            \"domain_cardinality\": 0,\\n            \"data\": [\\n              27.761375975688,\\n              55.923557338198,\\n              28.388683621664,\\n              35.275735166748,\\n              53.253980894466,\\n              41.531820529033\\n            ],\\n          }\\n        ],\\n      \"MSE\": 31.321880321916,\\n      \"r2\": 0.88422889064751,\\n      \"mean_residual_deviance\": 31.321880321916\\n    }\\n  ]\\n}\\n\\n```\\n\\n# Documentation\\n - this document:\\n https://github.com/h2oai/h2o-3/blob/master/\\n h2o-docs/src/api/REST/h2o\\\\_3\\\\_rest\\\\_api\\\\_overview.md\\n - reference in the Help sidebar in Flow\\n - reference on the H2O.ai website, [http://docs.h2o.ai/](http://docs.h2o.ai/)\\n - reference doc is generated via the */Metadata* endpoints, so it\\'s always current'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/data-science-example-1/README.md', 'section': '## Use case\\n\\n* Read in data\\n* Handle categorical columns specially, if required (e.g. scikit-learn)\\n* Perform train/test split of data\\n* Build GBM model\\n* Make predictions'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/data-science-example-1/README.md', 'section': '## Dataset\\n\\n* [allyears_tiny.csv](allyears_tiny.csv)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/data-science-example-1/README.md', 'section': '## Scripts and UI\\n\\n### Native\\n\\n* [Native R](example-native.R)\\n* [Native Python/Pandas/Scikit](example-native-pandas-scikit.py)\\n* [Native iPython Notebook Pandas/Scikit](example-native-pandas-scikit.ipynb)\\n\\n### H2O\\n\\n* [H2O REST API](example-h2o-REST.md)\\n* [H2O R](example-h2o.R)\\n* [H2O Python](example-h2o.py)\\n* [Java](example.java)\\n* [Scala](example.scala)\\n* [Flow UI](example-flow.md)\\n* [Flow JSON](example-flow.json)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md', 'section': '## Algo implementation throws exception to REST API handler'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md', 'section': '## REST API handler catches exception\\n\\n### Expected exception\\n\\n### Unexpected exception\\n\\n### Assertion failure'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md', 'section': '## REST API handler throws exception to Nano'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md', 'section': '## Nano catches exception and turns it into a NanoHTTPD.Response\\n\\n### JSON payload'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md', 'section': '## REST API client receives payload'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/api/rest-api-error-handling/README.md', 'section': '## REST API client decodes payload and prints to console'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/booklets/source/DeepWaterBookletErrata.md', 'section': '## Errata Items\\n\\n1.  [issue] [date]'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/custom_functions.md', 'section': '## Motivation\\n\\nProvide a way to represent custom functions to allow user to define their\\nown custom loss functions and evaluation metrics.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/custom_functions.md', 'section': '## Backend\\n\\n### Main idea\\n>**Object stored in K/V can contain code which can be loaded dynamically\\nat runtime**\\n\\nThe H2O K/V store can hold any values including unstructured byte array.\\nThe byte array can represent a jar file which contains any resources.\\nThat means it can include not only class files but also code in languages which\\nsupports JSR 223 (scripting extension of JVM).\\n\\nSuch code can be loaded with help of a dedicated classloader and executed\\nin any H2O context (e.g., in the context of MRTask or Rapids).\\n\\n### Design\\n\\n#### What is function?\\nIn this content, a custom function is a class which implements `water.udf.CFunc` interface.\\nThe interface is used only as a marker, actual functions implements an ancestor of the `CFunc` interface.\\n\\n#### Storage of functions in K/V\\nThe K/V holds a binary content of a jar file which includes definition of\\nfunction(s). The jar file can include multiple functions, it can even\\nmix functions from different languages - e.g., Java and Python.\\n\\n#### Referencing a function stored in K/V\\nThe functions are referenced by combination of `language`, `K/V key` and `function name`:\\n  - The language is used to select function loader which can instantiate a function stored in the jar file.\\n  - The key is used to reference a jar file stored in K/V store\\n  - The function name is used to select a function stored in the jar file.\\n\\n#### Access to functions stored in K/V\\nThe access to jar file stored in K/V is provided via a dedicated classloader implemented\\nin the `water.udf.DkvClassLoader` which inherits from standard `URLClassLoader`.\\nCurrent implementation dumps content of K/V referenced by a given key into a file and\\nuse `URLClassLoader#addURL` method to append another URL to load from.\\n\\n#### Creation of functions\\nGiven the access a jar file stored in K/V, the next step is to load and\\ninstantiate a function. This is driven by `water.udf.CFuncLoaderService` which\\nfinds for given language (e.g., `Java`) instance of `water.udf.CFuncLoader`.\\nThe loader provides an interface to instantiate a given method:\\n\\n```java\\npublic abstract <F> F load(String jfuncName, Class<? extends F> targetKlazz, ClassLoader classLoader);\\n```\\n\\nThe loaders `water.udf.CFuncLoader` are registered via Java Service Provider Interface.\\n\\n### Custom metrics\\nThe custom metrics is a function which implements `water.udf.CMetricFunc` interface.\\nThe interface follows design of `hex.MetricBuilder` and contains three methods to support\\ndistributed invocation:\\n  - `map` : the method which maps a row into array of doubles. The method is designed to be called as\\n  part of `water.MRTask#map` call and it corresponds to  `hex.MetricBuilder#map` call.\\n  - `reduce` : the method combines 2 row results. It is called as part of `water.MRTask#map` and `water.MRTask#reduce` calls.\\n  - `metric` : the method computes the final metric value from given array of doubles. The method\\n  is called in the context of `water.MRtask#postGlobal` and corresponds to `hex.MetricBuilder#postGlobal` call.\\n\\n### Custom distributions\\n\\nThe custom distribution is a function which implements `water.udf.CDistributionFunc` interface.\\nThe interface follows the design of `hex.Distribution` and contains four methods to support\\ndistributed invocation:\\n  - `link` : the method returns type of link function transformation of the probability of response variable to a continuous scale that is unbounded.\\n  The method is designed to be called where `hex.Distribution#link` and `hex.Distribution#linkInv` methods are used.\\n  It can return `identity` by default (Identity Link Function).\\n  - `init` : the method combines weight, actual response and offset to compute numerator and denominator of the initial value.\\n  It can return `[ weight * (response - offset), weight]` by default.\\n  The method is designed to be called where `hex.Distribution#initFNum` and `hex.Distribution#initFDenom` methods are used.\\n  - `gamma` : the method combines weight, actual response, residual and predicted response to compute numerator \\n  and denominator of size of step in terminal node estimate - gamma (The Elements of Statistical Learning II:, page 387.  Step 2b iii.).\\n  The method is designed to be called where `hex.Distribution#gammaNum` and `hex.Distribution#gammaDenom` methods are used.\\n  - `gradient` : the method computes (Negative half) Gradient of deviance function at predicted value for actual response\\n   in one GBM learning step (The Elements of Statistical Learning II, page 387, Steps 2a, 2b). \\n   The method is designed to be called where `hex.Distribution#negHalfGradient` method is used.\\n  \\n### Other design alternatives\\n  - Translation into Rapids\\n    - Advantages:\\n      - Rapids is a common backend representation for client code\\n    - Problems:\\n      - limited expressiveness of Rapids (cannot specify custom row/reduce transformations easily)\\n      - missing transpiler from Python into Rapids (we have limited transpiler for Lambdas)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/custom_functions.md', 'section': '## Python Client\\n\\nThe idea is to pass definition of custom metric or custom distribution directly from Python in the form of Python code.\\nThat needs:\\n  - an API which represents a custom metric function at caller side\\n  - backend support to interpret Python code\\n\\n### Public API\\n\\n#### Custom Metric Function\\nThe custom metric function is defined in Python as a class which provides\\n3 methods following the semantics of Java API above:\\n  - `map`\\n  - `reduce`\\n  - `metric`\\n\\nFor example, custom RMSE model metric:\\n\\n```python\\nclass CustomRmseFunc:\\n    def map(self, pred, act, weight, offset, model):\\n        idx = int(act[0])\\n        err = 1 - pred[idx + 1] if idx + 1 < len(pred) else 1\\n        return [err * err, 1]\\n\\n    def reduce(self, l, r):\\n        return [l[0] + r[0], l[1] + r[1]]\\n\\n    def metric(self, l):\\n        # Use Java API directly\\n        import java.lang.Math as math\\n        return math.sqrt(l[0] / l[1])\\n```\\n\\n> Note: please mention, that code above is also referencing a java class `java.lang.Math`\\n\\n##### Publishing custom metric function in cluster\\nThe client local custom function represented as a class can be uploaded into running\\nH2O cluster by calling method `h2o.upload_custom_metric(klazz, func_name, func_file)`:\\n  - `klazz` represent custom function as described above\\n  - `func_name` assigns a name with uploaded custom functions, the name corresponds to name of key in K/V\\n  - `func_file` name of file to store function in uploaded jar. The source code of given class is saved into a file,\\n  the file is zipped, and uploaded as zip-archive, and saved into K/V store.\\n\\nThe call returns a reference to uploaded custom metric function. The internal form of reference\\nis constructed based on passed parameters and follows the following structure: `<language>:<func_name>:<func_file>.<klazz-name>Wrapper`.\\n\\n> Note: The parameters `func_name` and `func_file` need to be unique for each uploaded custom metric!\\n\\nFor example:\\n```python\\ncustom_mm_func = h2o.upload_custom_metric(CustomRmseFunc, func_name=\"rmse\", func_file=\"mm_rmse.py\")\\n```\\n\\nreturns a function reference which has the following value:\\n\\n```\\n> print(custom_mm_func)\\npython:rmse=mm_rmse.CustomRmseFuncWrapper\\n```\\n\\n##### Using custom model metric functions\\nAn algorithm model builder interface can expose parameter `custom_metric_func`\\nwhich accepts a reference to uploaded custom metric function:\\n\\n```python\\nmodel = H2OGradientBoostingEstimator(ntrees=3, max_depth=5,\\n                  score_each_iteration=True,\\n                  custom_metric_func=custom_mm_func)\\nmodel.train(y=\"AGE\", x=ftrain.names, training_frame=ftrain, validation_frame=fvalid)\\n```\\n\\n> Note: Currently, only GBM and RandomForest expose the parameter.\\n\\nThe computed custom model metric is part of model metric object and available\\nvia methods `custom_metric_name()` and `custom_metric_value()`.\\n\\n#### Custom Distribution Function\\n\\nThe custom distribution function is defined in Python as a class which provides\\nfour methods following the semantics of Java API above:\\n  - `link`\\n  - `init`\\n  - `gamma`\\n  - `gradient`\\n  \\nFor example, custom Gaussian distribution:\\n\\n```python\\nclass CustomDistributionGaussian:\\n\\n    def link(self):\\n        return \"identity\"\\n\\n    def init(self, w, o, y):\\n        return [w * (y - o), w]\\n        \\n    def gamma(self, w, y, z, f):\\n        return [w * z, w]\\n    \\n    def gradient(self, y, f):\\n        return y - f\\n```\\n\\nOr custom Multinomial distribution:\\n   \\n   ```python\\nclass CustomDistributionMultinomial:\\n   \\n    def link(self):\\n        return \"log\"\\n\\n    def init(self, w, o, y):\\n        return [w * (y - o), w]\\n        \\n    def gradient(self, y, f, l):\\n        return 1 - f if y == l else 0 - f\\n\\n    def gamma(self, w, y, z, f):\\n        import java.lang.Math as math\\n        absz = math.abs(z)\\n        return [w * z, w * (absz * (1 - absz))]\\n   ```\\n\\n##### Publishing custom distribution function in cluster\\nThe client local custom function represented as a class can be uploaded into running\\nH2O cluster by calling method `h2o.upload_custom_distribution(klazz, func_name, func_file)`:\\n  - `klazz` represent custom function as described above\\n  - `func_name` assigns a name with uploaded custom functions, the name corresponds to name of key in K/V\\n  - `func_file` name of file to store function in uploaded jar. The source code of given class is saved into a file,\\n  the file is zipped, and uploaded as zip-archive, and saved into K/V store.\\n\\nThe call returns a reference to uploaded custom distribution function. The internal form of reference\\nis constructed based on passed parameters and follows the following structure: `<language>:<func_name>:<func_file>.<klazz-name>Wrapper`.\\n\\n> Note: The parameters `func_name` and `func_file` need to be unique for each uploaded custom distribution!\\n\\nFor example:\\n```python\\nfrom h2o.utils.distributions import CustomDistributionGaussian\\ncustom_dist_func = h2o.upload_custom_distribution(CustomDistributionGaussian, func_name=\"gaussian\", func_file=\"dist_gaussian.py\")\\n```\\n\\nreturns a function reference which has the following value:\\n\\n```\\n> print(custom_dist_func)\\npython:gaussian=dist_gaussian.CustomDistributionGaussianWrapper\\n```\\n\\n##### Using custom distribution functions\\nAn algorithm model builder interface can expose parameter `custom_distribution_func`\\nwhich accepts a reference to uploaded custom distribution function:\\n\\n```python\\nmodel = H2OGradientBoostingEstimator(ntrees=3, max_depth=5,\\n                  score_each_iteration=True,\\n                  custom_distribution_func=custom_dist_func)\\nmodel.train(y=\"AGE\", x=ftrain.names, training_frame=ftrain, validation_frame=fvalid)\\n```\\n\\n> Note: Currently, only GBM expose the parameter.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/lifecycle.md', 'section': \"## H2OApp vs. H2OClientApp\\n\\nThere are two main classes to start an H2O.\\n\\nH2OApp starts a worker H2O node that participates as a full DKV member and can execute work.  This is the normal way to start H2O.  This is how standalone H2O and all the nodes in H2O on Hadoop work.\\n\\nH2OClientApp starts an observer H2O client node that does not execute work.  This is used by the one driver node in Sparkling Water (the remaining worker executor nodes are all regular H2OApp nodes).\\n\\n#### CAUTION\\n\\nIf you just call H2O.main() directly by itself, the REST API won't get registered and REST API requests will never be served.  You probably meant to call H2OApp.main() instead!  This is a common mistake to make.\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/lifecycle.md', 'section': '## Embedding H2O\\n\\nOne way to Embed H2O is to use water.init.AbstractEmbeddedH2OConfig.  H2O on Hadoop uses this.   The EmbeddedH2OConfig registers a callback to assist with gathering the IP address and Port of individual H2O nodes and distributing them.\\n\\nSee <https://github.com/h2oai/h2o-3/blob/master/h2o-hadoop/h2o-mapreduce-generic/src/main/java/water/hadoop/h2omapper.java>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/lifecycle.md', 'section': '## Startup\\n\\nThe main class for Standalone H2O is H2OApp.\\n\\nSee <https://github.com/h2oai/h2o-3/blob/master/h2o-app/src/main/java/water/H2OApp.java>\\n\\nH2OApp uses a helper class called H2OStarter.\\n\\nSee <https://github.com/h2oai/h2o-3/blob/master/h2o-core/src/main/java/water/H2OStarter.java>\\n\\n\\nThe overall flow is shown below:\\n\\n```\\n    H2O.configureLogging();\\n    H2O.registerExtensions();\\n\\n    // Fire up the H2O Cluster\\n    H2O.main(args);\\n\\n    H2O.registerRestApis(relativeResourcePath);\\n    H2O.finalizeRegistration();\\n```\\nThe call to registerExtensions hooks in any H2O extensions found on the classpath.  It uses reflection to find all classes that inherit from water.AbstractH2OExtension\\n\\n\\nThe call to H2O.main() allocates ports, prepares the web server, and does all kinds of other startup work.\\n\\nThe call to registerRestApis() adds REST API routes for their respective subsystems.  (water is from h2o-core and hex is from h2o-algos.)  It uses reflection to find all classes that inherit from water.api.AbstractRegister.\\n\\nThe call to H2O.finalizeRegistration() signals that all routes have been added and tells the in-H2O web server to start accepting REST API requests.\\n\\nH2O cloud formation can occur even after H2O.finalizeRegistration.  New H2O nodes are allowed to join until the cloud receives a piece of work to do.  Usually this means until the cloud receives a REST API request or writes to the DKV.\\n\\nSee <https://github.com/h2oai/h2o-3/blob/master/h2o-core/src/main/java/water/H2O.java>\\n\\n> Use case:  Adding a new algorithm with a REST API endpoint requires the following parts:\\n>\\n> 2.  A MyAlgoSchema class that extends water.api.Schema and defines the inputs and outputs of the endpoint.\\n> 3.  A MyAlgoHandler class that extends water.api.Handler and implements the endpoint.\\n> 1.  A Register class that extends water.api.AbstractRegister.  This calls a registration method of H2O (water.H2O.registerGET() or water.H2O.registerPOST()) to tell the embedded Jetty about the new endpoint.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/dev/lifecycle.md', 'section': '## Shutdown\\n\\nThere are various ways to shut H2O down, as shown below.  H2O does not support graceful in-process.  You need to exit the process.\\n\\n### Java / Scala\\n\\nH2O.exit(0)\\n\\n### REST API\\n\\n/3/Shutdown\\n\\n### R\\n\\nh2o.shutdown()'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Introduction\\n\\nThis guide will walk you through how to use H2O\\'s web UI, H2O Flow. To view a demo video of H2O Flow, click <a href=\"https://www.youtube.com/watch?feature=player_embedded&v=wzeuFfbW7WE\" target=\"_blank\">here</a>. \\n\\n\\n---\\n\\n<a name=\"GetHelp\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Getting Help\\n\\n---\\n\\nFirst, let\\'s go over the basics. Type `h` to view a list of helpful shortcuts. \\n\\nThe following help window displays: \\n\\n![help menu](https://raw.githubusercontent.com/h2oai/h2o/master/docs/Flow-images/Shortcuts.png)\\n\\nTo close this window, click the **X** in the upper-right corner, or click the **Close** button in the lower-right corner. You can also click behind the window to close it. You can also access this list of shortcuts by clicking the **Help** menu and selecting **Keyboard Shortcuts**. \\n\\nFor additional help, click **Help** > **Assist Me** or click the **Assist Me!** button in the row of buttons below the menus. \\n\\n![Assist Me](images/Flow_AssistMeButton.png) \\n\\nYou can also type `assist` in a blank cell and press **Ctrl+Enter**. A list of common tasks displays to help you find the correct command. \\n\\n ![Assist Me links](images/Flow_assist.png)\\n \\nThere are multiple resources to help you get started with Flow in the **Help** sidebar. \\n\\n>**Note**: To hide the sidebar, click the **>>** button above it. ![Flow - Hide Sidebar](images/Flow_SidebarHide.png)\\n>\\n>To display the sidebar if it is hidden, click the **<<** button. ![Flow - Hide Sidebar](images/Flow_SidebarDisplay.png)\\n\\nTo access this documentation, select the **Flow Web UI...** link below the **General** heading in the Help sidebar. \\n\\nYou can also explore the pre-configured flows available in H2O Flow for a demonstration of how to create a flow. To view the example flows:\\n\\n- Click the **view example Flows** link below the **Quickstart Videos** button in the **Help** sidebar \\n  ![Flow - View Example Flows link](images/Flow_ViewExampleFlows.png)\\n \\n   or \\n\\n- Click the **Browse installed packs...** link in the **Packs** subsection of the **Help** sidebar. Click the **examples** folder and select the example flow from the list. \\n\\n  ![Flow Packs](images/Flow_ExampleFlows.png)\\n\\nIf you have a flow currently open, a confirmation window appears asking if the current notebook should be replaced. To load the example flow, click the **Load Notebook** button. \\n\\nTo view the REST API documentation, click the **Help** tab in the sidebar and then select the type of REST API documentation (**Routes** or **Schemas**). \\n\\n ![REST API documentation](images/Flow_REST_docs.png)\\n\\nBefore getting started with H2O Flow, make sure you understand the different cell modes. Certain actions can only be performed when the cell is in a specific mode. \\n\\n---\\n\\n<a name=\"Cell\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Understanding Cell Modes\\n\\nThere are two modes for cells: edit and command. \\n\\n<a name=\"EditMode\"></a>\\n### Using Edit Mode\\nIn edit mode, the cell is yellow with a blinking bar to indicate where text can be entered and there is an orange flag to the left of the cell.\\n\\n\\n![Edit Mode](images/Flow_EditMode.png)\\n \\n<a name=\"CmdMode\"></a>\\n### Using Command Mode\\n In command mode, the flag is yellow. The flag also indicates the cell\\'s format: \\n\\n- **MD**: Markdown \\n   \\n   >**Note**: Markdown formatting is not applied until you run the cell by:\\n   > \\n   >- clicking the **Run** button ![Flow - Run Button](images/Flow_RunButton.png)\\n        > or\\n      \\n   >- pressing **Ctrl+Enter**\\n\\n ![Flow - Markdown](images/Flow_markdown.png)\\n\\n- **CS**: Code (default)\\n\\n ![Flow - Code](images/Flow_parse_code_ex.png)\\n\\n- **RAW**: Raw format (for code comments) \\n\\n ![Flow - Raw](images/Flow_raw.png)\\n\\n- **H[1-6]**: Heading level (where 1 is a first-level heading) \\n\\n ![Flow - Heading Levels](images/Flow_headinglevels.png)\\n\\n>**NOTE**: If there is an error in the cell, the flag is red. \\n\\n ![Cell error](images/Flow_redflag.png)\\n \\n If the cell is executing commands, the flag is teal. The flag returns to yellow when the task is complete. \\n \\n ![Cell executing](images/Flow_cellmode_runningflag.png)\\n\\n### Changing Cell Formats\\n\\nTo change the cell\\'s format (for example, from code to Markdown), make sure you are in command (not edit) mode and that the cell you want to change is selected. The easiest way to do this is to click on the flag to the left of the cell. Enter the keyboard shortcut for the format you want to use. The flag\\'s text changes to display the current format. \\n\\nCell Mode     | Keyboard Shortcut\\n------------- | -----------------\\nCode          | `y`\\nMarkdown      | `m`\\nRaw text      | `r`\\nHeading 1     | `1`\\nHeading 2     | `2`\\nHeading 3     | `3` \\nHeading 4     | `4` \\nHeading 5     | `5`\\nHeading 6     | `6` \\n\\n\\n### Running Cells\\n\\n\\nThe series of buttons at the top of the page below the menus run cells in a flow. \\n \\n ![Flow - Run Buttons](images/Flow_RunButtons.png)\\n\\n- To run all cells in the flow, click the **Flow** menu, then click **Run All Cells**. \\n- To run the current cell and all subsequent cells, click the **Flow** menu, then click **Run All Cells Below**. \\n- To run an individual cell in a flow, confirm the cell is in [Edit Mode](#EditMode), then: \\n\\n - press **Ctrl+Enter**\\n\\n     or\\n  \\n - click the **Run** button ![Flow - Run Button](images/Flow_RunButton.png)\\n\\n\\n\\n### Running Flows\\nWhen you run the flow, a progress bar indicates the current status of the flow. You can cancel the currently running flow by clicking the **Stop** button in the progress bar. \\n\\n  ![Flow Progress Bar](images/Flow_progressbar.png)\\n\\nWhen the flow is complete, a message displays in the upper right.\\n\\n  ![Flow - Completed Successfully](images/Flow_run_pass.png)\\n  ![Flow - Did Not Complete](images/Flow_run_fail.png) \\n \\n>**Note**: If there is an error in the flow, H2O Flow stops at the cell that contains the error. \\n\\n\\n### Using Keyboard Shortcuts\\n\\nHere are some important keyboard shortcuts to remember: \\n\\n- Click a cell and press **Enter** to enter edit mode, which allows you to change the contents of a cell. \\n- To exit edit mode, press **Esc**. \\n- To execute the contents of a cell, press the **Ctrl** and **Enter** buttons at the same time.\\n\\nThe following commands must be entered in [command mode](#CmdMode).  \\n\\n- To add a new cell *above* the current cell, press **a**. \\n- To add a new cell *below* the current cell, press **b**. \\n- To delete the current cell, press the **d** key *twice*. (**dd**). \\n\\nYou can view these shortcuts by clicking **Help** > **Keyboard Shortcuts** or by clicking the **Help** tab in the sidebar. \\n\\n### Using Variables in Cells\\n\\nVariables can be used to store information such as download locations. To use a variable in Flow: \\n\\n1. Define the variable in a code cell (for example, `locA = \"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/kdd2009/small-churn/kdd_train.csv\"`). \\n  ![Flow variable definition](images/Flow_VariableDefinition.png)\\n2. Run the cell. H2O validates the variable. \\n  ![Flow variable validation](images/Flow_VariableValidation.png)\\n3. Use the variable in another code cell (for example, `importFiles [locA]`). \\n  ![Flow variable example](images/Flow_VariableExample.png)\\nTo further simplify your workflow, you can save the cells containing the variables and definitions as [clips](#Clips). \\n\\n\\n### Using Flow Buttons\\nThere are also a series of buttons at the top of the page below the flow name that allow you to save the current flow, add a new cell, move cells up or down, run the current cell, and cut, copy, or paste the current cell. If you hover over the button, a description of the button\\'s function displays. \\n\\n ![Flow buttons](images/Flow_buttons.png)\\n \\nYou can also use the menus at the top of the screen to edit the order of the cells, toggle specific format types (such as input or output), create models, or score models. You can also access troubleshooting information or obtain help with Flow.  \\n ![Flow menus](images/Flow_menus.png)\\n\\n>**Note**: To disable the code input and use H2O Flow strictly as a GUI, click the **Cell** menu, then **Toggle Cell Input**. \\n\\nNow that you are familiar with the cell modes, let\\'s import some data. \\n\\n---\\n\\n<a name=\"ImportData\"></a>\\n# ... Importing Data\\n\\nIf you don\\'t have any data of your own to work with, you can find some example datasets at <a href=\"http://data.h2o.ai\" target=\"_blank\">http://data.h2o.ai</a>.\\n\\n\\nThere are multiple ways to import data in H2O flow:\\n\\n- Click the **Assist Me!** button in the row of buttons below the menus, then click the **importFiles** link. Enter the file path in the auto-completing **Search** entry field and press **Enter**. Select the file from the search results and confirm it by clicking the **Add All** link.\\n ![Flow - Import Files Auto-Suggest](images/Flow_Import_AutoSuggest.png)\\n \\n- In a blank cell, select the CS format, then enter `importFiles [\"path/filename.format\"]` (where `path/filename.format` represents the complete file path to the file, including the full file name. The file path can be a local file path or a website address. \\n\\n>**Note**: For S3 file locations, use the format `importFiles [ \"s3n:/path/to/bucket/file/file.tab.gz\" ]`\\n  \\n- For an example of how to import a single file or a directory in R, refer to the following [example](https://github.com/h2oai/h2o-2/blob/master/R/tests/testdir_hdfs/runit_s3n_basic.R).  \\n\\nAfter selecting the file to import, the file path displays in the \"Search Results\" section. To import a single file, click the plus sign next to the file. To import all files in the search results, click the **Add all** link. The files selected for import display in the \"Selected Files\" section. \\n![Import Files](images/Flow_import.png)\\n\\n>**Note**: If the file is compressed, it will only be read using a single thread. For best performance, we recommend uncompressing the file before importing, as this will allow use of the faster multithreaded distributed parallel reader during import. Please note that .zip files containing multiple files are not currently supported. \\n\\n\\n\\n- To import the selected file(s), click the **Import** button. \\n\\n- To remove all files from the \"Selected Files\" list, click the **Clear All** link. \\n\\n- To remove a specific file, click the **X** next to the file path. \\n\\nAfter you click the **Import** button, the raw code for the current job displays. A summary displays the results of the file import, including the number of imported files and their Network File System (nfs) locations. \\n\\n ![Import Files - Results](images/Flow_import_results.png)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Uploading Data\\n\\nTo upload a local file, click the **Data** menu and select **Upload File...**. Click the **Choose File** button, select the file, click the **Choose** button, then click the **Upload** button. \\n  \\n  ![File Upload Pop-Up](images/Flow_UploadDataset.png)\\n  \\n  When the file has uploaded successfully, a message displays in the upper right and the **Setup Parse** cell displays. \\n\\n  \\n  ![File Upload Successful](images/Flow_FileUploadPass.png)\\n\\nOk, now that your data is available in H2O Flow, let\\'s move on to the next step: parsing. Click the **Parse these files** button to continue. \\n\\n---\\n\\n<a name=\"ParseData\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Parsing Data\\n\\nAfter you have imported your data, parse the data.\\n\\n ![Flow - Parse options](images/Flow_parse_setup.png)\\n\\nThe read-only **Sources** field displays the file path for the imported data selected for parsing. \\n\\nThe **ID** contains the auto-generated name for the parsed data (by default, the file name of the imported file with `.hex` as the file extension). Use the default name or enter a custom name in this field. \\n\\nSelect the parser type (if necessary) from the drop-down **Parser** list. For most data parsing, H2O automatically recognizes the data type, so the default settings typically do not need to be changed. The following options are available: \\n\\n- Auto\\n- ARFF\\n- XLS\\n- XLSX\\n- CSV\\n- SVMLight\\n\\n   >**Note**: For SVMLight data, the column indices must be >= 1 and the columns must be in ascending order. \\n\\nIf a separator or delimiter is used, select it from the **Separator** list. \\n\\nSelect a column header option, if applicable: \\n\\n- **Auto**: Automatically detect header types.\\n- **First row contains column names**: Specify heading as column names.\\n- **First row contains data**: Specify heading as data. This option is selected by default.\\n\\nSelect any necessary additional options: \\n\\n- **Enable single quotes as a field quotation character**: Treat single quote marks (also known as apostrophes) in the data as a character, rather than an enum. This option is not selected by default. \\n- **Delete on done**: Check this checkbox to delete the imported data after parsing. This option is selected by default. \\n\\nA preview of the data displays in the \"Edit Column Names and Types\" section. \\n\\n\\nTo change or add a column name, edit or enter the text in the column\\'s entry field. In the screenshot below, the entry field for column 16 is highlighted in red.  \\n  \\n ![Flow - Column Name Entry Field](images/Flow_ColNameEntry.png)\\n\\nTo change the column type, select the drop-down list to the right of the column name entry field and select the data type. The options are: \\n  \\n  - Unknown\\n  - Numeric\\n  - Enum\\n  - Time\\n  - UUID\\n  - String\\n  - Invalid\\n\\nYou can search for a column by entering it in the *Search by column name...* entry field above the first column name entry field. As you type, H2O displays the columns that match the specified search terms.\\n\\n**Note**: Only custom column names are searchable. Default column names cannot be searched. \\n\\nTo navigate the data preview, click the **<- Previous page** or **-> Next page** buttons.  \\n\\n  ![Flow - Pagination buttons](images/Flow_PageButtons.png)\\n\\nAfter making your selections, click the **Parse** button. \\n\\nAfter you click the **Parse** button, the code for the current job displays. \\n\\n ![Flow - Parse code](images/Flow_parse_code_ex.png)\\n \\nSince we\\'ve submitted a couple of jobs (data import & parse) to H2O now, let\\'s take a moment to learn more about jobs in H2O.  \\n \\n--- \\n \\n<a name=\"ViewJobs\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Jobs\\n\\nAny command (such as `importFiles`) you enter in H2O is submitted as a job, which is associated with a key. The key identifies the job within H2O and is used as a reference.\\n\\n### Viewing All Jobs\\n\\nTo view all jobs, click the **Admin** menu, then click **Jobs**, or enter `getJobs` in a cell in CS mode. \\n\\n ![View Jobs](images/Flow_getJobs.png)\\n\\nThe following information displays: \\n\\n- Type (for example, `Frame` or `Model`)\\n- Link to the object \\n- Description of the job type (for example, `Parse` or `GBM`)\\n- Start time\\n- End time\\n- Run time\\n\\nTo refresh this information, click the **Refresh** button. To view the details of the job, click the **View** button. \\n\\n### Viewing Specific Jobs\\n\\nTo view a specific job, click the link in the \"Destination\" column. \\n\\n![View Job - Model](images/Flow_ViewJob_Model.png)\\n\\nThe following information displays: \\n\\n- Type (for example, `Frame`)\\n- Link to object (key)\\n- Description (for example, `Parse`)\\n- Status\\n- Run time\\n- Progress\\n\\n>**NOTE**: For a better understanding of how jobs work, make sure to review the [Viewing Frames](#ViewFrames) section as well. \\n \\nOk, now that you understand how to find jobs in H2O, let\\'s submit a new one by building a model. \\n\\n---\\n\\n<a name=\"BuildModel\"></a>\\n# ... Building Models\\n\\nTo build a model: \\n\\n- Click the **Assist Me!** button in the row of buttons below the menus and select **buildModel**\\n\\n  or \\n\\n- Click the **Assist Me!** button, select **getFrames**, then click the **Build Model...** button below the parsed .hex data set\\n\\n  or \\n\\n- Click the **View** button after parsing data, then click the **Build Model** button\\n\\n  or \\n\\n- Click the drop-down **Model** menu and select the model type from the list\\n\\n\\nThe **Build Model...** button can be accessed from any page containing the .hex key for the parsed data (for example, `getJobs` > `getFrame`). The following image depicts the K-Means model type. Available options vary depending on model type. \\n\\n\\n ![Model Builder](images/Flow_ModelBuilder.png)\\n\\n \\nIn the **Build a Model** cell, select an algorithm from the drop-down menu: \\n\\n<a name=\"Kmeans\"></a>\\n- **K-means**: Create a K-Means model.\\n\\n<a name=\"GLM\"></a>\\n- **Generalized Linear Model**: Create a Generalized Linear model.\\n\\n<a name=\"DRF\"></a>\\n- **Distributed RF**: Create a distributed Random Forest model.  \\n\\n<a name=\"NB\"></a>\\n- **Nave Bayes**: Create a Nave Bayes model. \\n\\n<a name=\"PCA\"></a> \\n- **Principal Component Analysis**: Create a Principal Components Analysis model for modeling without regularization or performing dimensionality reduction. \\n\\n<a name=\"GBM\"></a>\\n- **Gradient Boosting Machine**: Create a Gradient Boosted model\\n\\n<a name=\"DL\"></a>\\n- **Deep Learning**: Create a Deep Learning model.\\n\\n\\nThe available options vary depending on the selected model. If an option is only available for a specific model type, the model type is listed. If no model type is specified, the option is applicable to all model types. \\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates an ID containing the model type (for example, `gbm-6f6bdc8b-ccbc-474a-b590-4579eea44596`). \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **nfolds**: ([GLM](#GLM), [GBM](#GBM), [DL](#DL), [DRF](#DRF)) Specify the number of folds for cross-validation. \\n\\n- **response_column**: (Required for [GLM](#GLM), [GBM](#GBM), [DL](#DL), [DRF](#DRF), [Nave Bayes](#NB)) Select the column to use as the independent variable.\\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: (Optional) Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **transform**: ([PCA](#PCA)) Select the transformation method for the training data: None, Standardize, Normalize, Demean, or Descale.  \\n\\n- **pca_method**: ([PCA](#PCA)) Select the algorithm to use for computing the principal components: \\n\\t- *GramSVD*: Uses a distributed computation of the Gram matrix, followed by a local SVD using the JAMA package\\n\\t- *Power*: Computes the SVD using the power iteration method\\n\\t- *Randomized*: Uses randomized subspace iteration method \\n\\t- *GLRM*: Fits a generalized low-rank model with L2 loss function and no regularization and solves for the SVD using local matrix algebra\\n\\n- **family**: ([GLM](#GLM)) Select the model type (Gaussian, Binomial, Multinomial, Poisson, Gamma, or Tweedie).\\n\\n- **solver**: ([GLM](#GLM)) Select the solver to use (AUTO, IRLSM, L\\\\_BFGS, COORDINATE\\\\_DESCENT\\\\_NAIVE, or COORDINATE\\\\_DESCENT). IRLSM is fast on on problems with a small number of predictors and for lambda-search with L1 penalty, while [L_BFGS](http://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf) scales better for datasets with many columns. COORDINATE\\\\_DESCENT is IRLSM with the covariance updates version of cyclical coordinate descent in the innermost loop. COORDINATE\\\\_DESCENT\\\\_NAIVE is IRLSM with the naive updates version of cyclical coordinate descent in the innermost loop. COORDINATE\\\\_DESCENT\\\\_NAIVE and COORDINATE\\\\_DESCENT are currently experimental. \\n\\n- **link**: ([GLM](#GLM)) Select a link function (Identity, Family_Default, Logit, Log, Inverse, or Tweedie).\\n\\n- **alpha**: ([GLM](#GLM)) Specify the regularization distribution between L2 and L2.  \\n\\n- **lambda**: ([GLM](#GLM)) Specify the regularization strength.  \\n\\n- **lambda_search**: ([GLM](#GLM)) Check this checkbox to enable lambda search, starting with lambda max. The given lambda is then interpreted as lambda min. \\n\\n- **non-negative**: ([GLM](#GLM)) To force coefficients to be non-negative, check this checkbox. \\n\\n- **standardize**: ([K-Means](#Kmeans), [GLM](#GLM)) To standardize the numeric columns to have mean of zero and unit variance, check this checkbox. Standardization is highly recommended; if you do not use standardization, the results can include components that are dominated by variables that appear to have larger variances relative to other attributes as a matter of scale, rather than true contribution. This option is selected by default. \\n\\n- **beta_constraints**: ([GLM](#GLM)) To use beta constraints, select a dataset from the drop-down menu. The selected frame is used to constraint the coefficient vector to provide upper and lower bounds. \\n\\n- **ntrees**: ([GBM](#GBM), [DRF](#DRF)) Specify the number of trees.  \\n\\n- **max\\\\_depth**: ([GBM](#GBM), [DRF](#DRF)) Specify the maximum tree depth.  \\n\\n- **min\\\\_rows**: ([GBM](#GBM), [DRF](#DRF)) Specify the minimum number of observations for a leaf (\"nodesize\" in R). \\n\\n- **nbins**: ([GBM](#GBM), [DRF](#DRF)) (Numerical [real/int] only) Specify the minimum number of bins for the histogram to build, then split at the best point.   \\n\\n- **nbins_cats**: ([GBM](#GBM), [DRF](#DRF)) (Categorical [factors/enums] only) Specify the maximum number of bins for the histogram to build, then split at the best point. Higher values can lead to more overfitting.  The levels are ordered alphabetically; if there are more levels than bins, adjacent levels share bins. This value has a more significant impact on model fitness than **nbins**. Larger values may increase runtime, especially for deep trees and large clusters, so tuning may be required to find the optimal value for your configuration. \\n\\n- **learn_rate**: ([GBM](#GBM)) Specify the learning rate. The range is 0.0 to 1.0. \\n\\n- **distribution**: ([GBM](#GBM), [DL](#DL)) Select the distribution type from the drop-down list. The options are auto, bernoulli, multinomial, gaussian, poisson, gamma, or tweedie.\\n\\n- **sample_rate**: ([GBM](#GBM), [DRF](#DRF)) Specify the row sampling rate (x-axis). The range is 0.0 to 1.0. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled. For details, refer to \"Stochastic Gradient Boosting\" ([Friedman, 1999](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)). \\n\\n- **col\\\\_sample_rate**: ([GBM](#GBM), [DRF](#DRF)) Specify the column sampling rate (y-axis). The range is 0.0 to 1.0. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled. For details, refer to \"Stochastic Gradient Boosting\" ([Friedman, 1999](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)). \\n\\n- **mtries**: ([DRF](#DRF)) Specify the columns to randomly select at each level. If the default value of `-1` is used, the number of variables is the square root of the number of columns for classification and p/3 for regression (where p is the number of predictors).   \\n   \\n- **binomial\\\\_double\\\\_trees**: ([DRF](#DRF)) (Binary classification only) Build twice as many trees (one per class). Enabling this option can lead to higher accuracy, while disabling can result in faster model building. This option is disabled by default. \\n\\n- **score\\\\_each\\\\_iteration**: ([K-Means](#Kmeans), [DRF](#DRF), [Nave Bayes](#NB), [PCA](#PCA), [GBM](#GBM), [GLM](#GLM)) To score during each iteration of the model training, check this checkbox. \\n\\n- **k***: ([K-Means](#Kmeans), [PCA](#PCA)) For K-Means, specify the number of clusters. For PCA, specify the rank of matrix approximation.  \\n\\n- **user_points**: ([K-Means](#Kmeans)) For K-Means, specify the number of initial cluster centers.  \\n\\n- **max_iterations**: ([K-Means](#Kmeans), [PCA](#PCA), [GLM](#GLM)) Specify the number of training iterations. \\n\\n- **init**: ([K-Means](#Kmeans)) Select the initialization mode. The options are Furthest, PlusPlus, Random, or User. \\n\\n  >**Note**: If PlusPlus is selected, the initial Y matrix is chosen by the final cluster centers from the K-Means PlusPlus algorithm. \\n\\n- **tweedie_variance_power**: ([GLM](#GLM)) (Only applicable if *Tweedie* is selected for **Family**) Specify the Tweedie variance power. \\n\\n- **tweedie_link_power**: ([GLM](#GLM)) (Only applicable if *Tweedie* is selected for **Family**) Specify the Tweedie link power. \\n\\n- **activation**: ([DL](#DL)) Select the activation function (Tanh, TanhWithDropout, Rectifier, RectifierWithDropout, Maxout, MaxoutWithDropout). The default option is Rectifier. \\n\\n- **hidden**: ([DL](#DL)) Specify the hidden layer sizes (e.g., 100,100). For Grid Search, use comma-separated values: (10,10),(20,20,20). The default value is [200,200]. The specified value(s) must be positive. \\n\\n- **epochs**: ([DL](#DL)) Specify the number of times to iterate (stream) the dataset. The value can be a fraction.  \\n\\n- **variable_importances**: ([DL](#DL)) Check this checkbox to compute variable importance. This option is not selected by default. \\n\\n- **laplace**: ([Nave Bayes](#NB)) Specify the Laplace smoothing parameter. \\n\\n- **min\\\\_sdev**: ([Nave Bayes](#NB)) Specify the minimum standard deviation to use for observations without enough data.  \\n\\n- **eps\\\\_sdev**: ([Nave Bayes](#NB)) Specify the threshold for standard deviation. If this threshold is not met, the **min\\\\_sdev** value is used.  \\n\\n- **min\\\\_prob**: ([Nave Bayes](#NB)) Specify the minimum probability to use for observations without enough data.  \\n\\n- **eps\\\\_prob**: ([Nave Bayes](#NB)) Specify the threshold for standard deviation. If this threshold is not met, the **min\\\\_sdev** value is used. \\n\\n- **compute_metrics**: ([Nave Bayes](#NB), [PCA](#PCA)) To compute metrics on training data, check this checkbox. The Nave Bayes classifier assumes independence between predictor variables conditional on the response, and a Gaussian distribution of numeric predictors with mean and standard deviation computed from the training dataset. When building a Nave Bayes classifier, every row in the training dataset that contains at least one NA will be skipped completely. If the test dataset has missing values, then those predictors are omitted in the probability calculation during prediction. \\n\\n**Advanced Options**\\n\\n- **fold_assignment**: ([GLM](#GLM), [GBM](#GBM), [DL](#DL), [DRF](#DRF), [K-Means](#Kmeans)) (Applicable only if a value for **nfolds** is specified and **fold_column** is not selected) Select the cross-validation fold assignment scheme. The available options are Random or [Modulo](https://en.wikipedia.org/wiki/Modulo_operation). \\n\\n- **fold_column**: ([GLM](#GLM), [GBM](#GBM), [DL](#DL), [DRF](#DRF), [K-Means](#Kmeans)) Select the column that contains the cross-validation fold index assignment per observation. \\n\\n- **offset_column**: ([GLM](#GLM), [DRF](#DRF), [GBM](#GBM))  Select a column to use as the offset. \\n\\t>*Note*: Offsets are per-row \"bias values\" that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. For more information, refer to the following [link](http://www.idg.pl/mirrors/CRAN/web/packages/gbm/vignettes/gbm.pdf). \\n\\n- **weights_column**: ([GLM](#GLM), [DL](#DL), [DRF](#DRF), [GBM](#GBM)) Select a column to use for the observation weights. The specified `weights_column` must be included in the specified `training_frame`. *Python only*: To use a weights column when passing an H2OFrame to `x` instead of a list of column names, the specified `training_frame` must contain the specified `weights_column`. \\n\\t>*Note*: Weights are per-row observation weights and do not increase the size of the data frame. This is typically the number of times a row is repeated, but non-integer values are supported as well. During training, rows with higher weights matter more, due to the larger loss function pre-factor.  \\n\\n- **loss**: ([DL](#DL)) Select the loss function. For DL, the options are Automatic, Quadratic, CrossEntropy, Huber, or Absolute and the default value is Automatic. Absolute, Quadratic, and Huber are applicable for regression or classification, while CrossEntropy is only applicable for classification. Huber can improve for regression problems with outliers.\\n\\n- **checkpoint**: ([DL](#DL), [DRF](#DRF), [GBM](#GBM)) Enter a model key associated with a previously-trained model. Use this option to build a new model as a continuation of a previously-generated model.\\n\\n- **use\\\\_all\\\\_factor\\\\_levels**: ([DL](#DL), [PCA](#PCA)) Check this checkbox to use all factor levels in the possible set of predictors; if you enable this option, sufficient regularization is required. By default, the first factor level is skipped. For Deep Learning models, this option is useful for determining variable importances and is automatically enabled if the autoencoder is selected. \\n\\n- **train\\\\_samples\\\\_per\\\\_iteration**: ([DL](#DL)) Specify the number of global training samples per MapReduce iteration. To specify one epoch, enter 0. To specify all available data (e.g., replicated training data), enter -1. To use the automatic values, enter -2. \\n\\n- **adaptive_rate**: ([DL](#DL)) Check this checkbox to enable the adaptive learning rate (ADADELTA). This option is selected by default. If this option is enabled, the following parameters are ignored: `rate`, `rate_decay`, `rate_annealing`, `momentum_start`, `momentum_ramp`, `momentum_stable`, and `nesterov_accelerated_gradient`. \\n\\n- **input\\\\_dropout\\\\_ratio**: ([DL](#DL)) Specify the input layer dropout ratio to improve generalization. Suggested values are 0.1 or 0.2. The range is >= 0 to <1. \\n\\n- **l1**: ([DL](#DL)) Specify the L1 regularization to add stability and improve generalization; sets the value of many weights to 0. \\n\\n- **l2**: ([DL](#DL)) Specify the L2 regularization to add stability and improve generalization; sets the value of many weights to smaller values. \\n\\n- **balance_classes**: ([GBM](#GBM), [DL](#DL)) Oversample the minority classes to balance the class distribution. This option is not selected by default and can increase the data frame size. This option is only applicable for classification. Majority classes can be undersampled to satisfy the **Max\\\\_after\\\\_balance\\\\_size** parameter.\\n\\n  >**Note**: `balance_classes` balances over just the target, not over all classes in the training frame. \\n\\n- **max\\\\_confusion\\\\_matrix\\\\_size**: ([DRF](#DRF), [DL](#DL), [Nave Bayes](#NB), [GBM](#GBM), [GLM](#GLM)) Specify the maximum size (in number of classes) for confusion matrices to be printed in the Logs. \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: ([DRF](#DRF), [DL](#DL), [Nave Bayes](#NB), [GBM](#GBM), [GLM](#GLM)) Specify the maximum number (top K) of predictions to use for hit ratio computation. Applicable to multinomial only. To disable, enter 0. \\n\\n- **r2_stopping**: ([GBM](#GBM), [DRF](#DRF)) r2_stopping is no longer supported and will be ignored if set - please use stopping_rounds, stopping_metric and stopping_tolerance instead.\\n\\n- **build\\\\_tree\\\\_one\\\\_node**: ([DRF](#DRF), [GBM](#GBM)) To run on a single node, check this checkbox. This is suitable for small datasets as there is no network overhead but fewer CPUs are used. The default setting is disabled. \\n\\n- **rate**: ([DL](#DL)) Specify the learning rate. Higher rates result in less stable models and lower rates result in slower convergence. Not applicable if **adaptive_rate** is enabled. \\n\\n- **rate_annealing**: ([DL](#DL)) Specify the learning rate annealing. The formula is rate/(1+rate\\\\_annealing value \\\\* samples). Not applicable if **adaptive\\\\_rate** is enabled.\\n\\n- **momentum_start**: ([DL](#DL)) Specify the initial momentum at the beginning of training. A suggested value is 0.5. Not applicable if **adaptive_rate** is enabled.\\n\\n- **momentum_ramp**: ([DL](#DL)) Specify the number of training samples for increasing the momentum. Not applicable if **adaptive_rate** is enabled.\\n\\n- **momentum_stable**: ([DL](#DL)) Specify the final momentum value reached after the **momentum_ramp** training samples. Not applicable if **adaptive_rate** is enabled. \\n\\n- **nesterov\\\\_accelerated\\\\_gradient**: ([DL](#DL)) Check this checkbox to use the Nesterov accelerated gradient. This option is recommended and selected by default. Not applicable is **adaptive_rate** is enabled. \\n\\n- **hidden\\\\_dropout\\\\_ratios**: ([DL](#DL)) Specify the hidden layer dropout ratios to improve generalization. Specify one value per hidden layer, each value between 0 and 1 (exclusive). There is no default value. This option is applicable only if *TanhwithDropout*, *RectifierwithDropout*, or *MaxoutWithDropout* is selected from the **Activation** drop-down list. \\n\\n- **tweedie_power**: ([DL](#DL), [GBM](#GBM)) (Only applicable if *Tweedie* is selected for **Family**) Specify the Tweedie power. The range is from 1 to 2. For a normal distribution, enter `0`. For Poisson distribution, enter `1`. For a gamma distribution, enter `2`. For a compound Poisson-gamma distribution, enter a value greater than 1 but less than 2. For more information, refer to [Tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution). \\n\\n- **score_interval**: ([DL](#DL)) Specify the shortest time interval (in seconds) to wait between model scoring.  \\n\\n- **score\\\\_training\\\\_samples**: ([DL](#DL)) Specify the number of training set samples for scoring. To use all training samples, enter 0.  \\n\\n- **score\\\\_validation\\\\_samples**: ([DL](#DL)) (Requires selection from the **validation_frame** drop-down list) This option is applicable to classification only. Specify the number of validation set samples for scoring. To use all validation set samples, enter 0.  \\n\\n- **score\\\\_duty\\\\_cycle**: ([DL](#DL)) Specify the maximum duty cycle fraction for scoring. A lower value results in more training and a higher value results in more scoring. The value must be greater than 0 and less than 1. \\n\\n- **autoencoder**: ([DL](#DL)) Check this checkbox to enable the Deep Learning autoencoder. This option is not selected by default. \\n   >**Note**: This option requires a loss function other than CrossEntropy. If this option is enabled, **use\\\\_all\\\\_factor\\\\_levels** must be enabled. \\n\\n**Expert Options**\\n\\n- **keep\\\\_cross\\\\_validation\\\\_predictions**: ([GLM](#GLM), [GBM](#GBM), [DL](#DL), [DRF](#DRF), [K-Means](#Kmeans)) To keep the cross-validation predictions, check this checkbox. \\n\\n- **class\\\\_sampling\\\\_factors**: ([DRF](#DRF), [GBM](#GBM), [DL](#DL)) Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance. This option is only applicable for classification problems and when **balance_classes** is enabled. \\n\\n- **overwrite\\\\_with\\\\_best\\\\_model**: ([DL](#DL)) Check this checkbox to overwrite the final model with the best model found during training. This option is selected by default. \\n\\n- **target\\\\_ratio\\\\_comm\\\\_to\\\\_comp**: ([DL](#DL)) Specify the target ratio of communication overhead to computation. This option is only enabled for multi-node operation and if **train\\\\_samples\\\\_per\\\\_iteration** equals -2 (auto-tuning).  \\n\\n- **rho**: ([DL](#DL)) Specify the adaptive learning rate time decay factor. This option is only applicable if **adaptive_rate** is enabled. \\n\\n- **epsilon**: ([DL](#DL)) Specify the adaptive learning rate time smoothing factor to avoid dividing by zero. This option is only applicable if **adaptive_rate** is enabled. \\n\\n- **max_w2**: ([DL](#DL)) Specify the constraint for the squared sum of the incoming weights per unit (e.g., for Rectifier). \\n\\n- **initial\\\\_weight\\\\_distribution**: ([DL](#DL)) Select the initial weight distribution (Uniform Adaptive, Uniform, or Normal). If Uniform Adaptive is used, the **initial\\\\_weight\\\\_scale** parameter is not applicable. \\n \\n- **initial\\\\_weight\\\\_scale**: ([DL](#DL)) Specify the initial weight scale of the distribution function for Uniform or Normal distributions. For Uniform, the values are drawn uniformly from initial weight scale. For Normal, the values are drawn from a Normal distribution with the standard deviation of the initial weight scale. If Uniform Adaptive is selected as the **initial\\\\_weight\\\\_distribution**, the **initial\\\\_weight\\\\_scale** parameter is not applicable.\\n\\n- **classification_stop**: ([DL](#DL)) (Applicable to discrete/categorical datasets only) Specify the stopping criterion for classification error fractions on training data. To disable this option, enter -1.  \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: ([DL](#DL), [GLM](#GLM)) (Classification only) Specify the maximum number (top K) of predictions to use for hit ratio computation (for multinomial only). To disable this option, enter 0.  \\n\\n- **regression_stop**: ([DL](#DL)) (Applicable to real value/continuous datasets only) Specify the stopping criterion for regression error (MSE) on the training data. To disable this option, enter -1.  \\n\\n- **diagnostics**: ([DL](#DL)) Check this checkbox to compute the variable importances for input features (using the Gedeon method). For large networks, selecting this option can reduce speed. This option is selected by default. \\n\\n- **fast_mode**: ([DL](#DL)) Check this checkbox to enable fast mode, a minor approximation in back-propagation. This option is selected by default. \\n\\n- **force\\\\_load\\\\_balance**: ([DL](#DL)) Check this checkbox to force extra load balancing to increase training speed for small datasets and use all cores. This option is selected by default. \\n\\n- **single\\\\_node\\\\_mode**: ([DL](#DL)) Check this checkbox to force H2O to run on a single node for fine-tuning of model parameters. This option is not selected by default. \\n\\n- **replicate\\\\_training\\\\_data**: ([DL](#DL)) Check this checkbox to replicate the entire training dataset on every node for faster training on small datasets. This option is not selected by default. This option is only applicable for clouds with more than one node. \\n\\n- **shuffle\\\\_training\\\\_data**: ([DL](#DL)) Check this checkbox to shuffle the training data. This option is recommended if the training data is replicated and the value of **train\\\\_samples\\\\_per\\\\_iteration** is close to the number of nodes times the number of rows. This option is not selected by default. \\n\\n- **missing\\\\_values\\\\_handling**: ([DL](#DL), [GLM](#GLM) Select how to handle missing values (Skip or MeanImputation). \\n\\n- **quiet_mode**: ([DL](#DL)) Check this checkbox to display less output in the standard output. This option is not selected by default.\\n\\n- **sparse**: ([DL](#DL)) Check this checkbox to enable sparse data handling, which is more efficient for data with many zero values. \\n\\n- **col_major**: ([DL](#DL)) Check this checkbox to use a column major weight matrix for the input layer. This option can speed up forward propagation but may reduce the speed of backpropagation. This option is not selected by default.  \\n  \\n  >**Note**: This parameter has been deprecated. \\n\\n- **average_activation**: ([DL](#DL)) Specify the average activation for the sparse autoencoder. If **Rectifier** is selected as the **Activation** type, this value must be positive. For Tanh, the value must be in (-1,1). \\n\\n- **sparsity_beta**: ([DL](#DL)) Specify the sparsity-based regularization optimization. For more information, refer to the following [link](http://www.mit.edu/~9.520/spring09/Classes/class11_sparsity.pdf).  \\n\\n- **max\\\\_categorical\\\\_features**: ([DL](#DL)) Specify the maximum number of categorical features enforced via hashing. \\n\\n- **reproducible**: ([DL](#DL)) To force reproducibility on small data, check this checkbox. If this option is enabled, the model takes more time to generate, since it uses only one thread. \\n\\n- **export\\\\_weights\\\\_and\\\\_biases**: ([DL](#DL)) To export the neural network weights and biases as H2O frames, check this checkbox. \\n\\n- **max\\\\_after\\\\_balance\\\\_size**: ([DRF](#DRF), [GBM](#GBM), [DL](#DL)) Specify the maximum relative size of the training data after balancing class counts (can be less than 1.0). Requires **balance\\\\_classes**. \\n\\n- **nbins\\\\_top\\\\_level**: ([DRF](#DRF), [GBM](#GBM)) (For numerical [real/int] columns only) Specify the maximum number of bins at the root level to use to build the histogram. This number will then be decreased by a factor of two per level.  \\n\\n- **seed**: ([K-Means](#Kmeans), [GLM](#GLM), [GBM](#GBM), [DL](#DL), [DRF](#DRF)) Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n- **intercept**: ([GLM](#GLM)) To include a constant term in the model, check this checkbox. This option is selected by default. \\n\\n- **objective_epsilon**: ([GLM](#GLM)) Specify a threshold for convergence. If the objective value is less than this threshold, the model is converged. \\n\\n- **beta_epsilon**: ([GLM](#GLM)) Specify the beta epsilon value. If the L1 normalization of the current beta change is below this threshold, consider using convergence. \\n\\n- **gradient_epsilon**: ([GLM](#GLM)) (For L-BFGS only) Specify a threshold for convergence. If the objective value (using the L-infinity norm) is less than this threshold, the model is converged. \\n\\n- **prior**: ([GLM](#GLM)) Specify prior probability for y ==1. Use this parameter for logistic regression if the data has been sampled and the mean of response does not reflect reality.  \\n\\n- **max\\\\_active\\\\_predictors**: ([GLM](#GLM)) Specify the maximum number of active predictors during computation. This value is used as a stopping criterium to prevent expensive model building with many predictors. \\n\\n\\n\\n---\\n\\n<a name=\"ViewModel\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Models\\n\\nClick the **Assist Me!** button, then click the **getModels** link, or enter `getModels` in the cell in CS mode and press **Ctrl+Enter**. A list of available models displays. \\n\\n ![Flow Models](images/Flow_getModels.png)\\n\\nTo view all current models, you can also click the **Model** menu and click **List All Models**. \\n\\nTo inspect a model, check its checkbox then click the **Inspect** button, or click the **Inspect** button to the right of the model name. \\n\\n ![Flow Model](images/Flow_GetModel.png)\\n \\n A summary of the model\\'s parameters displays. To display more details, click the **Show All Parameters** button. \\n \\nTo delete a model, click the **Delete** button. \\n\\nTo generate a Plain Old Java Object (POJO) that can use the model outside of H2O, click the **Download POJO** button. \\n\\n>**Note**: A POJO can be run in standalone mode or it can be integrated into a platform, such as [Hadoop\\'s Storm](https://github.com/h2oai/h2o-training/blob/master/tutorials/streaming/storm/README.md). To make the POJO work in your Java application, you will also need the `h2o-genmodel.jar` file (available in `h2o-3/h2o-genmodel/build/libs/h2o-genmodel.jar`).\\n\\n---\\n\\n### Exporting and Importing Models\\n\\n**To export a built model:**\\n\\n1. Click the **Model** menu at the top of the screen. \\n2. Select *Export Model...*\\n3. In the `exportModel` cell that appears, select the model from the drop-down *Model:* list.\\n4. Enter a location for the exported model in the *Path:* entry field. \\n\\t>**Note**: If you specify a location that doesn\\'t exist, it will be created. For example, if you only enter `test` in the *Path:* entry field, the model will be exported to `h2o-3/test`. \\n5. To overwrite any files with the same name, check the *Overwrite:* checkbox. \\n6. Click the **Export** button. A confirmation message displays when the model has been successfully exported. \\n\\n  ![Export Model](images/ExportModel.png)\\n\\n\\n**To import a built model:** \\n\\n1. Click the **Model** menu at the top of the screen. \\n2. Select *Import Model...*\\n3. Enter the location of the model in the *Path:* entry field. \\n\\t>**Note**: The file path must be complete (e.g., `Users/h2o-user/h2o-3/exported_models`). Do not rename models while importing. \\n4. To overwrite any files with the same name, check the *Overwrite:* checkbox. \\n5. Click the **Import** button. A confirmation message displays when the model has been successfully imported. To view the imported model, click the **View Model** button. \\n\\n  ![Import Model](images/ImportModel.png)\\n\\n---\\n\\n### Using Grid Search \\n\\n\\nTo include a parameter in a grid search in Flow, check the checkbox in the *Grid?* column to the right of the parameter name (highlighted in red in the image below). \\n\\n  ![Grid Search Column](images/Flow_GridSearch.png)\\n\\n\\n- If the parameter selected for grid search is Boolean (T/F or Y/N), both values are included when the *Grid?* checkbox is selected. \\n- If the parameter selected for grid search is a list of values, the values display as checkboxes when the *Grid?* checkbox is selected. More than one option can be selected. \\n- If the parameter selected for grid search is a numerical value, use a semicolon (;) to separate each additional value. \\n- To view a list of all grid searches, select the **Model** menu, then click **List All Grid Search Results**, or click the **Assist Me** button and select **getGrids**. \\n\\n\\n---\\n\\n### Checkpointing Models\\n\\nSome model types, such as DRF, GBM, and Deep Learning, support checkpointing. A checkpoint resumes model training so that you can iterate your model. The dataset must be the same. The following  model parameters must be the same when restarting a model from a checkpoint:\\n\\n\\nMust be the same as in checkpoint model         |            |      | \\n--------------------|------------------|-----------------------|\\n `drop_na20_cols` | `response_column` | `activation` |\\n `use_all_factor_levels` | `adaptive_rate` | `autoencoder` |\\n`rho` | `epsilon` | `sparse` |\\n`sparsity_beta` | `col_major` | `rate` |\\n`rate_annealing` | `rate_decay` | `momentum_start` |\\n`momentum_ramp` | `momentum_stable` | `nesterov_accelerated_gradient`|\\n`ignore_const_cols`| `max_categorical_features` |`nfolds`|\\n`distribution` | `tweedie_power` | |\\n\\n\\nThe following parameters can be modified when restarting a model from a checkpoint: \\n\\nCan be modified | | | \\n----------------|-|-|\\n`seed` | `checkpoint`| `epochs` | \\n`score_interval`| `train_samples_per_iteration`| `target_ratio_comm_to_comp`\\n`score_duty_cycle`| `score_training_samples`| `score_validation_samples`\\n`score_validation_sampling`| `classification_stop`| `regression_stop`\\n`quiet_mode` | `max_confusion_matrix_size`| `mini_batch_size`\\n`diagnostics` | `variable_importances`| `initial_weight_distribution`\\n`initial_weight_scale` | `force_load_balance` | `replicate_training_data`\\n`shuffle_training_data`| `single_node_mode` | `fast_mode`\\n`l1`|`l2`| `max_w2`\\n`input_dropout_ratio`| `hidden_dropout_ratios` | `loss`\\n`overwrite_with_best_model`| `missing_values_handling` | `average_activation`\\n`reproducible` | `export_weights_and_biases`| `elastic_averaging`\\n`elastic_averaging_moving_rate`| `elastic_averaging_regularization`| \\n\\n\\n1. After building your model, copy the `model_id`. To view the `model_id`, click the **Model** menu then click **List All Models**. \\n2. Select the model type from the drop-down **Model** menu. \\n\\t>**Note**: The model type must be the same as the checkpointed model. \\n3. Paste the copied `model_id` in the *checkpoint* entry field. \\n4. Click the **Build Model** button. The model will resume training. \\n\\n\\n---\\n\\n### Interpreting Model Results\\n\\n**Scoring history**: [GBM](#GBM), [DL](#DL) Represents the error rate of the model as it is built. Typically, the error rate will be higher at the beginning (the left side of the graph) then decrease as the model building completes and accuracy improves. Can include mean squared error (MSE) and deviance. \\n\\n  ![Scoring History example](images/Flow_ScoringHistory.png)\\n\\n**Variable importances**: [GBM](#GBM), [DL](#DL) Represents the statistical significance of each variable in the data in terms of its affect on the model. Variables are listed in order of most to least importance. The percentage values represent the percentage of importance across all variables, scaled to 100%. The method of computing each variable\\'s importance depends on the algorithm.  To view the scaled importance value of a variable, use your mouse to hover over the bar representing the variable. \\n\\n  ![Variable Importances example](images/Flow_VariableImportances.png)\\n\\n**Confusion Matrix**: [DL](#DL) Table depicting performance of algorithm in terms of false positives, false negatives, true positives, and true negatives. The actual results display in the columns and the predictions display in the rows; correct predictions are highlighted in yellow. In the example below, `0` was predicted correctly 902 times, while `8` was predicted correctly 822 times and `0` was predicted as `4` once.\\n\\n  ![Confusion Matrix example](images/Flow_ConfusionMatrix.png)\\n\\n**ROC Curve**: [DL](#DL), [GLM](#GLM), [DRF](#DRF) Graph representing the ratio of true positives to false positives. To view a specific threshold, select a value from the drop-down **Threshold** list. To view any of the following details, select it from the drop-down **Criterion** list: \\n\\n- Max f1\\n- Max f2\\n- Max f0point5\\n- Max accuracy\\n- Max precision\\n- Max absolute MCC (the threshold that maximizes the absolute Matthew\\'s Correlation Coefficient)\\n- Max min per class accuracy\\n\\nThe lower-left side of the graph represents less tolerance for false positives while the upper-right represents more tolerance for false positives. Ideally, a highly accurate ROC resembles the following example. \\n\\n ![ROC Curve example](images/Flow_ROC.png)\\n\\n**Hit Ratio**: [GBM](#GBM), [DRF](#DRF), [NaiveBayes](#NB), [DL](#DL), [GLM](#GLM) (Multinomial Classification only) Table representing the number of times that the prediction was correct out of the total number of predictions. \\n\\n ![Hit Ratio Table](images/HitRatioTable.png)\\n\\n\\n**Standardized Coefficient Magnitudes** [GLM](#GLM)  Bar chart representing the relationship of a specific feature to the response variable. Coefficients can be positive (orange) or negative (blue). A positive coefficient indicates a positive relationship  between the feature and the response, where an increase in the feature corresponds with an increase in the response, while a negative coefficient represents a negative relationship between the feature and the response where an increase in the feature corresponds with a decrease in the response (or vice versa). \\n\\n  ![Standardized Coefficient Magnitudes](images/SCM.png)\\n\\n\\nTo learn how to make predictions, continue to the next section. \\n\\n---\\n\\n\\n<a name=\"Predict\"></a>\\n# ... Making Predictions\\n\\nAfter creating your model, click the key link for the model, then click the **Predict** button. \\nSelect the model to use in the prediction from the drop-down **Model:** menu and the data frame to use in the prediction from the drop-down **Frame:** menu, then click the **Predict** button. \\n\\n ![Making Predictions](images/Flow_makePredict.png)\\n\\n\\n---\\n \\n<a name=\"ViewPredict\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Predictions\\n\\nClick the **Assist Me!** button, then click the **getPredictions** link, or enter `getPredictions` in the cell in CS mode and press **Ctrl+Enter**. A list of the stored predictions displays. \\nTo view a prediction, click the **View** button to the right of the model name. \\n\\n ![Viewing Predictions](images/Flow_getPredict.png)\\n\\nYou can also view predictions by clicking the drop-down **Score** menu and selecting **List All Predictions**.\\n\\n\\n \\n\\n---\\n\\n<a name=\"ViewFrame\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Frames\\n\\nTo view a specific frame, click the \"Key\" link for the specified frame, or enter `getFrameSummary \"FrameName\"` in a cell in CS mode (where `FrameName` is the name of a frame, such as `allyears2k.hex`).\\n\\n ![Viewing specified frame](images/Flow_getFrame.png) \\n\\n\\nFrom the `getFrameSummary` cell, you can: \\n\\n- view a truncated list of the rows in the data frame by clicking the **View Data** button\\n- split the dataset by clicking the **Split...** button\\n- view the columns, data, and factors in more detail or plot a graph by clicking the **Inspect** button\\n- create a model by clicking the **Build Model** button\\n- make a prediction based on the data by clicking the **Predict** button\\n- download the data as a .csv file by clicking the **Download** button\\n- view the characteristics or domain of a specific column by clicking the **Summary** link\\n\\nWhen you view a frame, you can \"drill-down\" to the necessary level of detail (such as a specific column or row) using the **Inspect** button or by clicking the links. The following screenshot displays the results of clicking the **Inspect** button for a frame.\\n\\n![Inspecting Frames](images/Flow_inspectFrame.png)\\n\\nThis screenshot displays the results of clicking the **columns** link. \\n\\n![Inspecting Columns](images/Flow_inspectCol.png)\\n\\n\\nTo view all frames, click the **Assist Me!** button, then click the **getFrames** link, or enter `getFrames` in the cell in CS mode and press **Ctrl+Enter**. You can also view all current frames by clicking the drop-down **Data** menu and selecting **List All Frames**. \\n\\nA list of the current frames in H2O displays that includes the following information for each frame: \\n\\n\\n- Link to the frame (the \"key\")\\n- Number of rows and columns\\n- Size \\n\\n\\nFor parsed data, the following information displays: \\n\\n- Link to the .hex file\\n- The **Build Model**, **Predict**, and **Inspect** buttons\\n\\n ![Parsed Frames](images/Flow_getFrames.png)\\n\\nTo make a prediction, check the checkboxes for the frames you want to use to make the prediction, then click the **Predict on Selected Frames** button. \\n\\n---\\n\\n### Splitting Frames\\n\\nDatasets can be split within Flow for use in model training and testing. \\n\\n ![splitFrame cell](images/Flow_splitFrame.png)\\n\\n1. To split a frame, click the **Assist Me** button, then click **splitFrame**.\\n  \\n  >**Note**: You can also click the drop-down **Data** menu and select **Split Frame...**.\\n2. From the drop-down **Frame:** list, select the frame to split. \\n3. In the second **Ratio** entry field, specify the fractional value to determine the split. The first **Ratio** field is automatically calculated based on the values entered in the second **Ratio** field. \\n   \\n  >**Note**: Only fractional values between 0 and 1 are supported (for example, enter `.5` to split the frame in half). The total sum of the ratio values must equal one. H2O automatically adjusts the ratio values to equal one; if unsupported values are entered, an error displays.  \\n4. In the **Key** entry field, specify a name for the new frame. \\n5. (Optional) To add another split, click the **Add a split** link. To remove a split, click the `X` to the right of the **Key** entry field. \\n6. Click the **Create** button.  \\n\\n---\\n### Creating Frames\\n\\nTo create a frame with a large amount of random data (for example, to use for testing), click the drop-down **Admin** menu, then select **Create Synthetic Frame**. Customize the frame as needed, then click the **Create** button to create the frame. \\n\\n  ![Flow - Creating Frames](images/Flow_CreateFrame.png)\\n\\n---\\n\\n### Plotting Frames\\n\\nTo create a plot from a frame, click the **Inspect** button, then click the **Plot** button. \\n\\nSelect the type of plot (point, path, or rect) from the drop-down **Type** menu, then select the x-axis and y-axis from the following options: \\n\\n- label \\n- type\\n- missing \\n- zeros\\n- +Inf\\n- -Inf\\n- min\\n- max\\n- mean\\n- sigma\\n- cardinality\\n\\nSelect one of the above options from the drop-down **Color** menu to display the specified data in color, then click the **Plot** button to plot the data. \\n\\n ![Flow - Plotting Frames](images/Flow_plot.png)\\n\\n>**Note**: Because H2O stores enums internally as numeric then maps the integers to an array of strings, any `min`, `max`, or `mean` values for categorical columns are not meaningful and should be ignored. Displays for categorical data will be modified in a future version of H2O. \\n\\n---\\n\\n<a name=\"Flows\"></a>\\n\\n# ... Using Flows\\n\\nYou can use and modify flows in a variety of ways:\\n\\n- Clips allow you to save single cells \\n- Outlines display summaries of your workflow\\n- Flows can be saved, duplicated, loaded, or downloaded\\n\\n---\\n\\n\\n<a name=\"Clips\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Using Clips\\n\\nClips enable you to save cells containing your workflow for later reuse. To save a cell as a clip, click the paperclip icon to the right of the cell (highlighted in the red box in the following screenshot). \\n ![Paperclip icon](images/Flow_clips_paperclip.png)\\n\\nTo use a clip in a workflow, click the \"Clips\" tab in the sidebar on the right. \\n\\n ![Clips tab](images/Flow_clips.png)\\n\\nAll saved clips, including the default system clips (such as `assist`, `importFiles`, and `predict`), are listed. Clips you have created are listed under the \"My Clips\" heading. To select a clip to insert, click the circular button to the left of the clip name. To delete a clip, click the trashcan icon to right of the clip name. \\n\\n>**NOTE**: The default clips listed under \"System\" cannot be deleted. \\n\\nDeleted clips are stored in the trash. To permanently delete all clips in the trash, click the **Empty Trash** button. \\n\\n>**NOTE**: Saved data, including flows and clips, are persistent as long as the same IP address is used for the cluster. If a new IP is used, previously saved flows and clips are not available. \\n\\n---\\n\\n<a name=\"Outline\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Outlines\\n\\nThe **Outline** tab in the sidebar displays a brief summary of the cells currently used in your flow; essentially, a command history. \\n\\n- To jump to a specific cell, click the cell description. \\n- To delete a cell, select it and press the X key on your keyboard. \\n\\n ![View Outline](images/Flow_outline.png)\\n\\n---\\n\\n<a name=\"SaveFlow\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Saving Flows\\n\\nYou can save your flow for later reuse. To save your flow as a notebook, click the \"Save\" button (the first button in the row of buttons below the flow name), or click the drop-down \"Flow\" menu and select \"Save Flow.\" \\nTo enter a custom name for the flow, click the default flow name (\"Untitled Flow\") and type the desired flow name. A pencil icon indicates where to enter the desired name. \\n\\n ![Renaming Flows](images/Flow_rename.png)\\n\\nTo confirm the name, click the checkmark to the right of the name field. \\n \\n ![Confirm Name](images/Flow_rename2.png)\\n\\nTo reuse a saved flow, click the \"Flows\" tab in the sidebar, then click the flow name. To delete a saved flow, click the trashcan icon to the right of the flow name. \\n\\n ![Flows](images/Flow_flows.png)\\n\\n### Finding Saved Flows on your Disk\\n \\nBy default, flows are saved to the `h2oflows` directory underneath your home directory.  The directory where flows are saved is printed to stdout:\\n \\n```\\n03-20 14:54:20.945 172.16.2.39:54323     95667  main      INFO: Flow dir: \\'/Users/<UserName>/h2oflows\\'\\n```\\n\\nTo back up saved flows, copy this directory to your preferred backup location.  \\n\\nTo specify a different location for saved flows, use the command-line argument `-flow_dir` when launching H2O:\\n\\n`java -jar h2o.jar -flow_dir /<New>/<Location>/<For>/<Saved>/<Flows>`  \\n\\nwhere `/<New>/<Location>/<For>/<Saved>/<Flows>` represents the specified location.  If the directory does not exist, it will be created the first time you save a flow.\\n\\n### Saving Flows on a Hadoop cluster\\n\\nIf you are running H2O Flow on a Hadoop cluster, H2O will try to find the HDFS home directory to use as the default directory for flows. If the HDFS home directory is not found, flows cannot be saved unless a directory is specified while launching using `-flow_dir`:\\n\\n`hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output hdfsOutputDirName -flow_dir hdfs:///<Saved>/<Flows>/<Location>`  \\n\\nThe location specified in `flow_dir` may be either an hdfs or regular filesystem directory.  If the directory does not exist, it will be created the first time you save a flow.\\n\\n### Copying Flows\\n\\nTo create a copy of the current flow, select the **Flow** menu, then click **Make a Copy**. The name of the current flow changes to `Copy of <FlowName>` (where `<FlowName>` is the name of the flow). You can save the duplicated flow using this name by clicking **Flow** > **Save Flow**, or [rename it](#SaveFlow) before saving. \\n\\n\\n### Downloading Flows\\n\\nAfter saving a flow as a notebook, click the **Flow** menu, then select **Download this Flow**. A new window opens and the saved flow is downloaded to the default downloads folder on your computer. The file is exported as `<filename>.flow`, where `<filename>` is the name specified when the flow was saved. \\n\\n**Caution**: You must have an active internet connection to download flows. \\n\\n### Loading Flows\\n\\nTo load a saved flow, click the **Flows** tab in the sidebar at the right. In the pop-up confirmation window that appears, select **Load Notebook**, or click **Cancel** to return to the current flow. \\n\\n ![Confirm Replace Flow](images/Flow_confirmreplace.png)\\n\\nAfter clicking **Load Notebook**, the saved flow is loaded. \\n\\nTo load an exported flow, click the **Flow** menu and select **Open Flow...**. In the pop-up window that appears, click the **Choose File** button and select the exported flow, then click the **Open** button. \\n\\n ![Open Flow](images/Flow_Open.png)\\n\\n>**Notes**: \\n\\n>- Only exported flows using the default .flow filetype are supported. Other filetypes will not open. \\n- If the current notebook has the same name as the selected file, a pop-up confirmation appears to confirm that the current notebook should be overwritten. \\n\\n---\\n\\n<a name=\"Troubleshooting\"></a>\\n# ...Troubleshooting Flow\\n\\nTo troubleshoot issues in Flow, use the **Admin** menu. The **Admin** menu allows you to check the status of the cluster, view a timeline of events, and view or download logs for issue analysis. \\n\\n>**NOTE**: To view the current H2O Flow version, click the **Help** menu, then click **About**.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Cluster Status\\n\\nClick the **Admin** menu, then select **Cluster Status**. A summary of the status of the cluster (also known as a cloud) displays, which includes the same information: \\n\\n- Cluster health\\n- Whether all nodes can communicate (consensus)\\n- Whether new nodes can join (locked/unlocked)\\n  \\n  >**Note**: After you submit a job to H2O, the cluster does not accept new nodes. \\n- H2O version\\n- Number of used and available nodes\\n- When the cluster was created\\n\\n ![Cluster Status](images/Flow_CloudStatus.png)\\n\\n\\nThe following information displays for each node:   \\n\\n- IP address (name)\\n- Time of last ping\\n- Number of cores\\n- Load\\n- Amount of data (used/total)\\n- Percentage of cached data\\n- GC (free/total/max)\\n- Amount of disk space in GB (free/max)\\n- Percentage of free disk space \\n\\nTo view more information, click the **Show Advanced** button. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing CPU Status (Water Meter)\\n\\nTo view the current CPU usage, click the **Admin** menu, then click **Water Meter (CPU Meter)**. A new window opens, displaying the current CPU use statistics. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Logs\\n\\nTo view the logs for troubleshooting, click the **Admin** menu, then click **Inspect Log**. \\n\\n ![Inspect Log](images/Flow_viewLog.png)\\n\\nTo view the logs for a specific node, select it from the drop-down **Select Node** menu. \\n\\n---\\n\\n<a name=\"DL_Logs\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Downloading Logs\\n\\nTo download the logs for further analysis, click the **Admin** menu, then click **Download Log**. A new window opens and the logs download to your default download folder. You can close the new window after downloading the logs. Send the logs to [h2ostream](mailto:h2ostream@googlegroups.com) or [file a JIRA ticket](#ReportIssue) for issue resolution. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Stack Trace Information\\n\\nTo view the stack trace information, click the **Admin** menu, then click **Stack Trace**. \\n\\n ![Stack Trace](images/Flow_stacktrace.png)\\n\\nTo view the stack trace information for a specific node, select it from the drop-down **Select Node** menu. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing Network Test Results\\n\\nTo view network test results, click the **Admin** menu, then click **Network Test**. \\n\\n  ![Network Test Results](images/Flow_NetworkTest.png)\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Accessing the Profiler\\n\\nThe Profiler looks across the cluster to see where the same stack trace occurs, and can be helpful for identifying activity on the current CPU. \\nTo view the profiler, click the **Admin** menu, then click **Profiler**. \\n\\n ![Profiler](images/Flow_profiler.png)\\n\\nTo view the profiler information for a specific node, select it from the drop-down **Select Node** menu. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Viewing the Timeline\\n\\nTo view a timeline of events in Flow, click the **Admin** menu, then click **Timeline**. The following information displays for each event: \\n\\n- Time of occurrence (HH:MM:SS:MS)\\n- Number of nanoseconds for duration\\n- Originator of event (\"who\")\\n- I/O type\\n- Event type\\n- Number of bytes sent & received\\n\\n ![Timeline](images/Flow_timeline.png)\\n\\nTo obtain the most recent information, click the **Refresh** button.  \\n\\n---\\n\\n<a name=\"ReportIssue\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Reporting Issues\\n\\nIf you experience an error with Flow, you can submit a GitHub issue to notify our team.\\n\\n1. First, click the **Admin** menu, then click **Download Logs**. This will download a file contains information that will help our developers identify the cause of the issue.\\n2. Click the **Help** menu, then click **Report an issue**. This will open the GitHub issue page where you can file your ticket.\\n3. Click the **New issue** button at the top of the page.\\n4. On the next page click **Get started** button for the **Bug Report** item.\\n5. Attach the log file from the first step, write a description of the error you experienced, then click the **Submit new issue** button at the bottom of the page.\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Requesting Help\\n\\nIf you have a Google account, you can submit a request for assistance with H2O on our Google Groups page, [H2Ostream](https://groups.google.com/forum/#!forum/h2ostream). \\n\\nTo access H2Ostream from Flow:\\n\\n0. Click the **Help** menu.\\n0. Click **Forum/Ask a question**. \\n0. Click the red **New topic** button.\\n0. Enter your question and click the red **Post** button. If you are requesting assistance for an error you experienced, be sure to include your [logs](#DL_Logs). \\n\\n\\nYou can also email your question to [h2ostream@googlegroups.com](mailto:h2ostream@googlegroups.com). \\n\\nOr, you can post your question on [Stack Overflow](https://stackoverflow.com/questions/tagged/h2o) using the \"h2o\" tag.\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/README.md', 'section': '## Shutting Down H2O\\n\\nTo shut down H2O, click the **Admin** menu, then click **Shut Down**. A *Shut down complete* message displays in the upper right when the cluster has been shut down. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## New Users\\n\\nIf you\\'re just getting started with H2O, here are some links to help you learn more: \\n\\n- <a href=\"http://h2o.ai/product/recommended-systems-for-h2o/\" target=\"_blank\">Recommended Systems</a>: This one-page PDF provides a basic overview of the operating systems, languages and APIs, Hadoop resource manager versions, cloud computing environments, browsers, and other resources recommended to run H2O. \\n  At a minimum, we recommend the following for compatibility with H2O: \\n  \\n  - **Operating Systems**: Windows 7 or later; OS X 10.9 or later, Ubuntu 12.04, or RHEL/CentOS 6 or later\\n  - **Languages**: Java 8 or later; R v.3 or later; Python 3.6.x or 3.8.x (R and Python are not required to use H2O unless you want to use H2O in those environments, but Java is always required)\\n  - **Browsers**: Latest version of Chrome, Firefox, Safari, or Internet Explorer (An internet browser is required to use H2O\\'s web UI, Flow)\\n  - **Hadoop**: Cloudera CDH 5.2 or later (5.3 is recommended); MapR v.3.1.1 or later; Hortonworks HDP 2.1 or later (Hadoop is not required to run H2O unless you want to deploy H2O on a Hadoop cluster)\\n  - **Spark**: v 1.3 or later (Spark is only required if you want to run [Sparkling Water](https://github.com/h2oai/sparkling-water))\\n\\n- <a href=\"http://h2o.ai/download/\" target=\"_blank\">Downloads page</a>: First things first - download a copy of H2O <a href=\"http://h2o.ai/download/\" target=\"_blank\">here</a> by selecting a build under \"Download H2O\" (the \"Bleeding Edge\" build contains the latest changes, while the latest alpha release is a more stable build), then use the installation instruction tabs to install H2O on your client of choice ([standalone](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html), [R](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#R), [Python](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Python), [Hadoop](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Hadoop), or [Maven](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Maven)) . \\n\\n\\tFor first-time users, we recommend downloading the latest alpha release and the default standalone option (the first tab) as the installation method. Make sure to install Java if it is not already installed.\\n\\t\\n- **Tutorials**: To see a step-by-step example of our algorithms in action, select a model type from the following list:\\n\\n\\t- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/dl/dl.md\" target=\"_blank\">Deep Learning</a>\\n\\t- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/gbm/gbm.md\" target=\"_blank\">Gradient Boosting Machine (GBM)</a>\\n\\t- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/glm/glm.md\" target=\"_blank\">Generalized Linear Model (GLM)</a> \\n\\t- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/kmeans/kmeans.md\" target=\"_blank\">K-means</a>\\n\\t- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/rf/rf.md\" target=\"_blank\">Distributed Random Forest (DRF)</a> \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/flow/README.md\" target=\"_blank\">Getting Started with Flow</a>: This document describes our new intuitive web interface, Flow. This interface is similar to IPython notebooks, and allows you to create a visual workflow to share with others. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/H2O-DevCmdLine.md\" target=\"_blank\">Launch from the command line</a>: This document describes some of the additional options that you can configure when launching H2O (for example, to specify a different directory for saved Flow data, allocate more memory, or use a flatfile for quick configuration of a cluster).\\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md\" target=\"_blank\">Algorithms</a>: This document describes the science behind our algorithms and provides a detailed, per-algo view of each model type. \\n\\n---\\n\\n<a name=\"Exp\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## Experienced Users\\n\\nIf you\\'ve used previous versions of H2O, the following links will help guide you through the process of upgrading to H2O 3.0. \\n\\n- <a href=\"http://h2o.ai/product/recommended-systems-for-h2o/\" target=\"_blank\">Recommended Systems</a>: This one-page PDF provides a basic overview of the operating systems, languages and APIs, Hadoop resource manager versions, cloud computing environments, browsers, and other resources recommended to run H2O. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/upgrade/Migration.md\" target=\"_blank\">Migration Guide</a>: This document provides a comprehensive guide to assist users in upgrading to H2O 3.0. It gives an overview of the changes to the algorithms and the web UI introduced in this version and describes the benefits of upgrading for users of R, APIs, and Java. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md\" target=\"_blank\">Porting R Scripts</a>: This document is designed to assist users who have created R scripts using previous versions of H2O. Due to the many improvements in R, scripts created using previous versions of H2O need some revision to work with H2O 3.0. This document provides a side-by-side comparison of the changes in R for each algorithm, as well as overall structural enhancements R users should be aware of, and provides a link to a tool that assists users in upgrading their scripts. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/flow/RecentChanges.md\" target=\"_blank\">Recent Changes</a>: This document describes the most recent changes in the latest build of H2O. It lists new features, enhancements (including changed parameter default values), and bug fixes for each release, organized by sub-categories such as Python, R, and Web UI. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/jessica-dev-docs/h2o-docs/src/product/upgrade/H2OvsH2O-Dev.md\" target=\"_blank\">H2O Classic vs H2O 3.0</a>: This document presents a side-by-side comparison of H2O 3.0 and the previous version of H2O. It compares and contrasts the features, capabilities, and supported algorithms between the versions. If you\\'d like to learn more about the benefits of upgrading, this is a great source of information. \\n\\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/flow/images/H2O-Algorithms-Road-Map.pdf\" target=\"_blank\">Algorithms Roadmap</a>: This document outlines our currently implemented features and describes which features are planned for future software versions. If you\\'d like to know what\\'s up next for H2O, this is the place to go. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/CONTRIBUTING.md\" target=\"_blank\">Contributing code</a>: If you\\'re interested in contributing code to H2O, we appreciate your assistance! This document describes how to access our list of Jiras that are suggested tasks for contributors and how to contact us. \\n\\n---\\n\\n<a name=\"OS\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## Enterprise Users\\n\\nIf you\\'re considering using H2O in an enterprise environment, you\\'ll be happy to know that the H2O platform is supported on all major Hadoop distributions including Cloudera Enterprise, Hortonworks Data Platform and the MapR Apache Hadoop Distribution. \\n\\nH2O can be deployed in-memory directly on top of existing Hadoop clusters without the need for data transfers, allowing for unmatched speed and ease of use. To ensure the integrity of data stored in Hadoop clusters, the H2O platform supports native integration of the Kerberos protocol. \\n\\nFor additional sales or marketing assistance, please email [sales@h2o.ai](mailto:sales@h2o.ai). \\n\\n- <a href=\"http://h2o.ai/product/recommended-systems-for-h2o/\" target=\"_blank\">Recommended Systems</a>: This one-page PDF provides a basic overview of the operating systems, languages and APIs, Hadoop resource manager versions, cloud computing environments, browsers, and other resources recommended to run H2O. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/security/Security.md\" target=\"_blank\">Security</a>: This document describes how to use the security features (available only to H2O Enterprise support customers). \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/H2O-DevS3Creds.md\" target=\"_blank\">How to Pass S3 Credentials to H2O</a>: This document describes the necessary step of passing your S3 credentials to H2O so that H2O can be used with AWS, as well as how to run H2O on an EC2 cluster.  \\n\\n\\n- Click [here](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Hadoop) to view instructions on how to set up H2O using Hadoop. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/H2O-DevHadoop.md\" target=\"_blank\">Running H2O on Hadoop</a>: This document describes how to run H2O on Hadoop. \\n\\n\\n\\n\\n---\\n\\n<a name=\"SW\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## Sparkling Water Users\\n\\nSparkling Water is a gradle project with the following submodules: \\n\\n- Core: Implementation of H2OContext, H2ORDD, and all technical integration code\\n- Examples: Application, demos, examples\\n- ML: Implementation of MLLib pipelines for H2O algorithms\\n- Assembly: Creates \"fatJar\" composed of all other modules\\n- py: Implementation of (h2o) Python binding to Sparkling Water\\n\\nThe best way to get started is to modify the core module or create a new module, which extends a project. \\n\\nUsers of our Spark-compatible solution, Sparkling Water, should be aware that Sparkling Water is only supported with the latest version of H2O. For more information about Sparkling Water, refer to the following links. \\n\\nSparkling Water is versioned according to the Spark versioning, so make sure to use the Sparkling Water version that corresponds to the installed version of Spark: \\n\\n- Use [Sparkling Water 1.2](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.2/6/index.html) for Spark 1.2\\n- Use [Sparkling Water 1.3](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.3/7/index.html) for Spark 1.3+\\n- Use [Sparkling Water 1.4](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.4/3/index.html) for Spark 1.4\\n- Use [Sparkling Water 1.5](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.5/3/index.html) for Spark 1.5\\n\\n\\n### Getting Started with Sparkling Water\\n\\n- <a href=\"http://h2o.ai/download/\" target=\"_blank\">Download Sparkling Water</a>: Go here to download Sparkling Water. \\n\\n- <a href=\"https://github.com/h2oai/sparkling-water/blob/master/DEVEL.md\" target=\"_blank\">Sparkling Water Development Documentation</a>: Read this document first to get started with Sparkling Water.  \\n\\n- <a href=\"https://github.com/h2oai/sparkling-water/tree/master/examples#sparkling-water-on-hadoop\" target=\"_blank\">Launch on Hadoop and Import from HDFS</a>: Go here to learn how to start Sparkling Water on Hadoop. \\n\\n- <a href=\"https://github.com/h2oai/sparkling-water/tree/master/examples\" target=\"_blank\">Sparkling Water Tutorials</a>: Go here for demos and examples. \\n\\n\\t- <a href=\"https://github.com/h2oai/sparkling-water/blob/master/examples/src/main/scala/org/apache/spark/examples/h2o/ProstateDemo.scala\" target=\"_blank\">Sparkling Water K-means Tutorial</a>: Go here to view a demo that uses Scala to create a K-means model. \\n\\t\\n\\t- <a href=\"https://github.com/h2oai/sparkling-water/blob/master/examples/src/main/scala/org/apache/spark/examples/h2o/CitiBikeSharingDemo.scala\" target=\"_blank\">Sparkling Water GBM Tutorial</a>: Go here to view a demo that uses Scala to create a GBM model. \\n\\n- <a href=\"http://h2o.ai/blog/2014/11-sparkling-water-on-yarn-example/\" target=\"_blank\">Sparkling Water on YARN</a>: Follow these instructions to run Sparkling Water on a YARN cluster. \\n\\n- <a href=\"http://learn.h2o.ai/content/hackers_station/start_with_sparkling_water.html\" target=\"_blank\">Building Applications on top of H2O</a>: This short tutorial describes project building and demonstrates the capabilities of Sparkling Water using Spark Shell to build a Deep Learning model. \\n\\n- <a href=\"http://h2o.ai/product/faq/#SparklingH2O\" target=\"_blank\">Sparkling Water FAQ</a>: This FAQ provides answers to many common questions about Sparkling Water. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/Connecting_RStudio_to_Sparkling_Water.md\" target=\"_blank\">Connecting RStudio to Sparkling Water</a>: This illustrated tutorial describes how to use RStudio to connect to Sparkling Water. \\n\\n### Sparkling Water Blog Posts\\n\\n- <a href=\"http://h2o.ai/blog/2014/09/how-sparkling-water-brings-h2o-to-spark\" target=\"_blank\">How Sparkling Water Brings H2O to Spark</a>\\n\\n- <a href=\"http://h2o.ai/blog/2014/06/h2o-killer-application-spark\" target=\"_blank\">H2O - The Killer App on Spark</a>\\n\\n- <a href=\"http://h2o.ai/blog/2014/03/spark-h2o/\" target=\"_blank\">In-memory Big Data: Spark + H2O</a>\\n\\n### Sparkling Water Meetup Slide Decks\\n\\n- <a href=\"https://github.com/h2oai/sparkling-water/tree/master/examples/scripts\" target=\"_blank\">Sparkling Water Meetup 02/03/2015\\n\\n- <a href=\"http://www.slideshare.net/0xdata/spa-43755759\" target=\"_blank\">Sparkling Water Meetup</a>\\n\\n- <a href=\"http://www.slideshare.net/0xdata/2014-12-17meetup\" target=\"_blank\">Interactive Session on Sparkling Water</a>\\n\\n- <a href=\"http://www.slideshare.net/0xdata/2014-09-30sparklingwaterhandson\" target=\"_blank\">Sparkling Water Hands-On</a>\\n\\n\\n### PySparkling\\n\\n  >*Note*: PySparkling requires [Sparkling Water 1.5](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.5/3/index.html) or later. \\n\\nH2O\\'s PySparkling package is not available through `pip` (there is [another](https://pypi.python.org/pypi/pysparkling/) similarly-named package). H2O\\'s PySparkling package requires [EasyInstall](http://peak.telecommunity.com/DevCenter/EasyInstall). \\n\\nTo install H2O\\'s PySparkling package, use the egg file included in the distribution. \\n\\n0. Download [Spark 1.5.1](https://spark.apache.org/downloads.html).\\n0. Set the `SPARK_HOME` and `MASTER` variables as described on the [Downloads page](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.5/6/index.html). \\n0. Download [Sparkling Water 1.5](http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.5/6/index.html)\\n0. In the unpacked Sparkling Water directory, run the following command: `easy_install --upgrade sparkling-water-1.5.6/py/dist/pySparkling-1.5.6-py2.7.egg`\\n\\n---\\n\\n<a name=\"Py\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## Python Users\\n\\nPythonistas will be glad to know that H2O now provides support for this popular programming language. Python users can also use H2O with IPython notebooks. For more information, refer to the following links. \\n\\n- Click [here](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Python) to view instructions on how to use H2O with Python. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-py/README.rst\" target=\"_blank\">Python readme</a>: This document describes how to setup and install the prerequisites for using Python with H2O. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-py/docs/index.html\" target=\"_blank\">Python docs</a>: This document represents the definitive guide to using Python with H2O. \\n\\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/upgrade/PythonParity.md\" target=\"_blank\">Python Parity</a>: This document is is a list of Python capabilities that were previously available only through the H2O R interface but are now available in H2O using the Python interface. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_eeg_eyestate.ipynb\" target=\"_blank\"> Grid Search in Python</a>: This notebook demonstrates the use of grid search in Python. \\n\\n\\n\\n---\\n\\n<a name=\"R\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## R Users\\n\\nDon\\'t worry, R users - we still provide R support in the latest version of H2O, just as before. The R components of H2O have been cleaned up, simplified, and standardized, so the command format is easier and more intuitive. Due to these improvements, be aware that any scripts created with previous versions of H2O will need some revision to be compatible with the latest version. \\n\\nWe have provided the following helpful resources to assist R users in upgrading to the latest version, including a document that outlines the differences between versions and a tool that reviews scripts for deprecated or renamed parameters. \\n\\nCurrently, the only version of R that is known to be incompatible with H2O is R version 3.1.0 (codename \"Spring Dance\"). If you are using that version, we recommend upgrading the R version before using H2O. \\n\\nTo check which version of H2O is installed in R, use `versions::installed.versions(\"h2o\")`. \\n\\n- Click [here](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#R) to view instructions for using H2O with R. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-r/h2o_package.pdf\" target=\"_blank\">R User Documentation</a>: This document contains all commands in the H2O package for R, including examples and arguments. It represents the definitive guide to using H2O in R. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md\" target=\"_blank\">Porting R Scripts</a>: This document is designed to assist users who have created R scripts using previous versions of H2O. Due to the many improvements in R, scripts created using previous versions of H2O will not work. This document provides a side-by-side comparison of the changes in R for each algorithm, as well as overall structural enhancements R users should be aware of, and provides a link to a tool that assists users in upgrading their scripts. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/Connecting_RStudio_to_Sparkling_Water.md\" target=\"_blank\">Connecting RStudio to Sparkling Water</a>: This illustrated tutorial describes how to use RStudio to connect to Sparkling Water. \\n\\n\\n### Ensembles\\n\\nEnsemble machine learning methods use multiple learning algorithms to obtain better predictive performance. \\n\\n- <a href=\"https://github.com/h2oai/h2o-2/tree/master/R/ensemble\" target=\"_blank\">H2O Ensemble GitHub repository</a>: Location for the H2O Ensemble R package.\\n\\n- <a href=\"http://learn.h2o.ai/content/tutorials/ensembles-stacking/index.html\" target=\"_blank\">Ensemble Documentation</a>: This documentation provides more details on the concepts behind ensembles and how to use them. \\n\\n---\\n\\n<a name=\"API\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## API Users\\n\\nAPI users will be happy to know that the APIs have been more thoroughly documented in the latest release of H2O and additional capabilities (such as exporting weights and biases for Deep Learning models) have been added. \\n\\nREST APIs are generated immediately out of the code, allowing users to implement machine learning in many ways. For example, REST APIs could be used to call a model created by sensor data and to set up auto-alerts if the sensor data falls below a specified threshold. \\n\\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md\" target=\"_blank\">H2O 3 REST API Overview</a>: This document describes how the REST API commands are used in H2O, versioning, experimental APIs, verbs, status codes, formats, schemas, payloads, metadata, and examples. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-docs/index.html#route-reference\" target=\"_blank\">REST API Reference</a>: This document represents the definitive guide to the H2O REST API. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-docs/index.html#schema-reference\" target=\"_blank\">REST API Schema Reference</a>: This document represents the definitive guide to the H2O REST API schemas. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/api/REST/h2o_3_rest_api_overview.md\" target=\"_blank\">H2O 3 REST API Overview</a>: This document provides an overview of how APIs are used in H2O, including versioning, URLs, HTTP verbs, status codes, formats, schemas, and examples. \\n\\n---\\n\\n<a name=\"Java\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## Java Users\\n\\nFor Java developers, the following resources will help you create your own custom app that uses H2O. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-core/javadoc/index.html\" target=\"_blank\">H2O Core Java Developer Documentation</a>: The definitive Java API guide for the core components of H2O. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-algos/javadoc/index.html\" target=\"_blank\">H2O Algos Java Developer Documentation</a>: The definitive Java API guide for the algorithms used by H2O. \\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-genmodel/javadoc/index.html\" target=\"_blank\">h2o-genmodel (POJO) Javadoc</a>: Provides a step-by-step guide to creating and implementing POJOs in a Java application. \\n\\n### SDK Information\\n\\nThe Java API is generated and accessible from the [download page](http://h2o.ai/download). \\n\\n- [Central repository](http://search.maven.org/#search%7Cga%7C1%7Cai.h2o)\\n- [View code on Github](https://github.com/h2oai/h2o-3/tree/{{last_commit_hash}})\\n- [Apache License](https://github.com/h2oai/h2o-3/blob/master/LICENSE)\\n\\n---\\n\\n<a name=\"Dev\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/flow/SiteIntro.md', 'section': '## Developers\\n\\nIf you\\'re looking to use H2O to help you develop your own apps, the following links will provide helpful references. \\n\\nFor the latest version of IDEA IntelliJ, run `./gradlew idea`, then click **File > Open** within IDEA. Select the `.ipr` file in the repository and click the **Choose** button.  \\n\\nFor older versions of IDEA IntelliJ, run `./gradlew idea`, then **Import Project** within IDEA and point it to the <a href=\"https://github.com/h2oai/h2o-3.git\" target=\"_blank\">h2o-3 directory</a>. \\n >**Note**: This process will take longer, so we recommend using the first method if possible. \\n\\nFor JUnit tests to pass, you may need multiple H2O nodes. Create a \"Run/Debug\" configuration with the following parameters: \\n\\n```\\nType: Application\\nMain class: H2OApp\\nUse class path of module: h2o-app\\n```\\n\\nAfter starting multiple \"worker\" node processes in addition to the JUnit test process, they will cloud up and run the multi-node JUnit tests. \\n\\n- <a href=\"http://h2o.ai/product/recommended-systems-for-h2o/\" target=\"_blank\">Recommended Systems</a>: This one-page PDF provides a basic overview of the operating systems, languages and APIs, Hadoop resource manager versions, cloud computing environments, browsers, and other resources recommended to run H2O. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3#41-building-from-the-command-line-quick-start\" target=\"_blank\">Developer Documentation</a>: Detailed instructions on how to build and launch H2O, including how to clone the repository, how to pull from the repository, and how to install required dependencies. \\n\\n- Click [here](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Maven) to view instructions on how to use H2O with Maven. \\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/build.gradle\" target=\"_blank\">Maven install</a>: This page provides information on how to build a version of H2O that generates the correct IDE files.\\n \\n- <a href=\"http://apps.h2o.ai\" target=\"_blank\">apps.h2o.ai</a>: Apps.h2o.ai is designed to support application developers via events, networking opportunities, and a new, dedicated website comprising developer kits and technical specs, news, and product spotlights. \\n\\n- <a href=\"https://github.com/h2oai/h2o-droplets\" target=\"_blank\">H2O Project Templates</a>: This page provides template info for projects created in Java, Scala, or Sparkling Water. \\n\\n- <a href=\"http://h2o.ai/blog/2014/16/Hacking/Algos/\" target=\"_blank\">Hacking Algos</a>: This blog post by Cliff walks you through building a new algorithm, using K-Means, Quantiles, and Grep as examples. \\n\\n- <a href=\"http://0xdata.com/blog/2014/05/kv-store-memory-analytics-part-2-2/\" target=\"_blank\">KV Store Guide</a>: Learn more about performance characteristics when implementing new algorithms. \\n\\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/CONTRIBUTING.md\" target=\"_blank\">Contributing code</a>: If you\\'re interested in contributing code to H2O, we appreciate your assistance! This document describes how to access our list of Jiras that contributors can work on and how to contact us.\\n\\n---\\n# Downloading H2O\\n\\n* [Download page for this build](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html)\\n* [h2o.ai main download page](http://www.h2o.ai/download)\\n\\nTo download H2O, go to our [downloads page](http://www.h2o.ai/download). Select a build type (bleeding edge or latest alpha), then select an installation method ([standalone](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html), [R](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#R), [Python](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Python), [Hadoop](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Hadoop), or [Maven](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Maven)) by clicking the tabs at the top of the page. Follow the instructions in the tab to install H2O. \\n\\n\\n# Starting H2O ...\\n\\nThere are a variety of ways to start H2O, depending on which client you would like to use. \\n\\n# ... From R\\n\\nTo use H2O in R, follow the instructions on the <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#R\" target=\"_blank\">download page</a>. \\n\\n# ... From Python\\n\\nTo use H2O in Python, follow the instructions on the <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html#Python\" target=\"_blank\">download page</a>.\\n\\n# ... On Spark\\n\\nTo use H2O on Spark, follow the instructions on the Sparkling Water [download page](http://h2o-release.s3.amazonaws.com/sparkling-water/master/latest.html).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/DemosAndTests.md', 'section': '## Demos\\n\\n### R\\n\\n- [Kaggle](https://github.com/h2oai/h2o-3/tree/master/h2o-r/demos/kaggle): Contains Kaggle demos, including \"Beating the Benchmark\" and \"Will It Rain?\"\\n\\n- [Supervised Demo](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/large/supervised.R): Runs four algorithms on categorical or continuous response datasets and reports performance. \\n\\n### Python\\n\\n- [Python Demos](https://github.com/h2oai/h2o-3/tree/master/h2o-py/demos): Contains a library of Python demos and instructions on how to run the demos. \\n\\n### Flow\\n\\n- [Flow Demos](https://github.com/h2oai/h2o-3/tree/master/h2o-docs/src/product/flow/packs/examples): Contains a library of demos that can be run in H2O\\'s web UI, Flow. These demos can also be accessed within Flow by clicking the \"Help\" sidebar, then clicking \"Browse installed packs...\", then clicking the \"Examples\" folder and selecting the demo flow. \\n\\n### Scala\\n\\n- [Scala Demos](https://github.com/h2oai/sparkling-water/tree/master/examples/scripts): Contains Scala demos used at meetups to demonstrate Sparkling Water. \\n\\n\\n### Java\\n\\n>Need location'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/DemosAndTests.md', 'section': '## Tests\\n\\n### R\\n\\n- [Instructions](https://github.com/h2oai/h2o-3/tree/master/h2o-r): Instructions on running R tests. \\n\\n- [Deep Learning](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/deeplearning): Library of Deep Learning R tests. \\n\\n- [GBM](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/gbm): Library of GBM R tests. \\n\\n- [GLM](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/glm): Library of GLM R tests. \\n\\n- [K-means](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/kmeans): Library of K-means R tests. \\n\\n- [Nave Bayes](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/naivebayes): Library of Nave Bayes R tests. \\n\\n- [DRF](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/randomforest): Library of DRF R tests. \\n\\n- [Demos](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_demos): Library of R tests for our demos, including Citibike, Chicago Crime, and Airlines. \\n\\n- [Documentation](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_docexamples): Library of R tests for the examples in the R documentation. \\n\\n- [Golden tests](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_golden): Library of \"golden\" R tests that test each algorithm extensively. \\n\\n- [HDFS](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_hdfs): Library of R tests that pull data from HDFS instead of locally. \\n\\n- [Java Predict](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_javapredict): Library of R tests that demonstrate how to use POJO output.\\n\\n- [Misc](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_misc) and [Munging](https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_munging): These libraries contain R tests for data munging functions, including splicing, filtering, histograms, and column transformation. \\n\\n\\n\\n### Python\\n\\n- [Instructions](https://github.com/h2oai/h2o-3/tree/master/h2o-py): Instructions for running Python tests. \\n\\n- [Deep Learning](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/deeplearning): Library of Deep Learning Python tests. \\n\\n- [GBM](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/gbm): Library of GBM Python tests. \\n\\n- [GLM](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/glm): Library of GLM Python tests. \\n\\n- [K-means](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/kmeans): Library of K-means Python tests. \\n\\n- [DRF](https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/rf): Library of DRF Python tests. \\n\\n### Java\\n\\n- [Instructions](https://github.com/h2oai/h2o-3/blob/master/h2o-core/testMultiNode.sh): Instructions on running Java tests. \\n\\n- [Deep Learning](https://github.com/h2oai/h2o-3/tree/master/h2o-algos/src/test/java/hex/deeplearning): Java Library containing multiple Deep Learning tests. \\n\\n- [GLM](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/test/java/hex/glm/GLMBasicTest.java): Runs GLM on Prostate dataset and scores results. \\n\\n- [K-means](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/test/java/hex/kmeans/KMeansTest.java): Runs K-means on Iris dataset with a seed, checks all clusters are non-zero, and scores results. \\n\\n- [Nave Bayes](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/test/java/hex/naivebayes/NaiveBayesTest.java): Runs Nave Bayes on Iris dataset, Prostate dataset, and Covtype dataset and scores results. \\n\\n- [Split frame](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/test/java/hex/splitframe/ShuffleSplitFrameTest.java): Tests shuffle split frame, splits the frame in half and compares the values. \\n\\n- [DRF](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/test/java/hex/tree/drf/DRFTest.java): Runs DRF on the Iris dataset, builds a POJO, and validates the results. \\n\\n- [GBM](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/test/java/hex/tree/gbm/GBMTest.java): Builds a GBM model using the Airlines dataset using Bernoulli classification.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## General Troubleshooting Tips\\n\\n- Confirm your internet connection is active.\\n\\n- Test connectivity using curl: First, log in to the first node and enter `curl http://<Node2IP>:54321` (where `<Node2IP>` is the IP address of the second node. Then, log in to the second node and enter `curl http://<Node1IP>:54321` (where `<Node1IP>` is the IP address of the first node). Look for output from H2O.\\n\\n- Try allocating more memory to H2O by modifying the `-Xmx` value when launching H2O from the command line (for example, `java -Xmx10g -jar h2o.jar` allocates 10g of memory for H2O). If you create a cluster with four 20g nodes (by specifying `-Xmx20g` four times), H2O will have a total of 80 gigs of memory available. For best performance, we recommend sizing your cluster to be about four times the size of your data. To avoid swapping, the `-Xmx` allocation must not exceed the physical memory on any node. Allocating the same amount of memory for all nodes is strongly recommended, as H2O works best with symmetric nodes.\\n\\n- Confirm that no other sessions of H2O are running. To stop all running H2O sessions, enter `ps -efww | grep h2o` in your shell (OSX or Linux).\\n- Confirm ports 54321 and 54322 are available for TCP. Launch Telnet (for Windows users) or Terminal (for OS X users), then type `telnet localhost 54321`, `telnet localhost 54322`\\n- Confirm your firewall is not preventing the nodes from locating each other. If you can\\'t launch H2O, we recommend temporarily disabling any firewalls until you can confirm they are not preventing H2O from launching.\\n- Confirm the nodes are not using different versions of H2O. If the H2O initialization is not successful, look at the output in the shell - if you see `Attempting to join /localhost:54321 with an H2O version mismatch (md5 differs)`, update H2O on all the nodes to the same version.\\n- Confirm that there is space in the `/tmp` directory.\\n\\t- Windows: In Command Prompt, enter `TEMP` and `%TEMP%` and delete files as needed, or use Disk Cleanup.\\n\\t- OS X: In Terminal, enter `open $TMPDIR` and delete the folder with your username.\\n- Confirm that the username is the same on all nodes; if not, define the cloud in the terminal when launching using `-name`:`java -jar h2o.jar -name myCloud`.\\n- Confirm that there are no spaces in the file path name used to launch H2O.\\n- Confirm that the nodes are not on different networks by confirming that the IP addresses of the nodes are the same in the output:\\n ```\\n INFO: Listening for HTTP and REST traffic on  IP_Address/\\n06-18 10:54:21.586 192.168.1.70:54323    25638  main\\nINFO: H2O cloud name: \\'H2O_User\\' on IP_Address, discovery address /Discovery_Address\\nINFO: Cloud of size 1 formed [IP_Address]\\n```\\n- Check if the nodes have different interfaces; if so, use the -network option to define the network (for example, `-network 127.0.0.1`). To use a network range, use a comma to separate the IP addresses (for example, `-network 123.45.67.0/22,123.45.68.0/24`).\\n- Force the bind address using `-ip`:`java -jar h2o.jar -ip <IP_Address> -port <PortNumber>`.\\n- (Hadoop only) Try launching H2O with a longer timeout: `hadoop jar h2odriver.jar -timeout 1800`\\n- (Hadoop only) Try to launch H2O using more memory: `hadoop jar h2odriver.jar -mapperXmx 10g`. The clusters memory capacity is the sum of all H2O nodes in the cluster.\\n- (Linux only) Check if you have SELINUX or IPTABLES enabled; if so, disable them.\\n- (EC2 only) Check the configuration for the EC2 security group.\\n\\n---\\n\\n**The following error message displayed when I tried to launch H2O - what should I do?**\\n\\n```\\nException in thread \"main\" java.lang.UnsupportedClassVersionError: water/H2OApp\\n: Unsupported major.minor version 51.0\\n        at java.lang.ClassLoader.defineClass1(Native Method)\\n        at java.lang.ClassLoader.defineClassCond(Unknown Source)\\n        at java.lang.ClassLoader.defineClass(Unknown Source)\\n        at java.security.SecureClassLoader.defineClass(Unknown Source)\\n        at java.net.URLClassLoader.defineClass(Unknown Source)\\n        at java.net.URLClassLoader.access$000(Unknown Source)\\n        at java.net.URLClassLoader$1.run(Unknown Source)\\n        at java.security.AccessController.doPrivileged(Native Method)\\n        at java.net.URLClassLoader.findClass(Unknown Source)\\n        at java.lang.ClassLoader.loadClass(Unknown Source)\\n        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\\n        at java.lang.ClassLoader.loadClass(Unknown Source)\\nCould not find the main class: water.H2OApp. Program will exit.\\n```\\nThis error output indicates that your Java version is not supported. Upgrade to [Java 7 (JVM)](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html) or [later](http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html) and H2O should launch successfully.\\n\\n---\\n\\n**I am not launching on Hadoop. How can I increase the amount of time that H2O allows for expected nodes to connect?**\\n\\n For cluster startup, if you are not launching on Hadoop, then you will not need to specify a timeout. You can add additional nodes to the cloud as long as you haven\\'t submitted any jobs to the cluster. When you do submit a job to the cluster, the cluster will lock and will print a message similar to `\"Locking cloud to new members, because <reason>...\"`.\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Algorithms\\n\\n**What does it mean if the r2 value in my model is negative?**\\n\\nThe coefficient of determination (also known as r^2) can be negative if:\\n\\n- linear regression is used without an intercept (constant)\\n- non-linear functions are fitted to the data\\n- predictions compared to the corresponding outcomes are not based on the model-fitting procedure using those data\\n- it is early in the build process (may self-correct as more trees are added)\\n\\nIf your r2 value is negative after your model is complete, your model is likely incorrect. Make sure your data is suitable for the type of model, then try adding an intercept.\\n\\n---\\n\\n**What\\'s the process for implementing new algorithms in H2O?**\\n\\nThis [blog post](http://h2o.ai/blog/2014/16/Hacking/Algos/) by Cliff  walks you through building a new algorithm, using K-Means, Quantiles, and Grep as examples.\\n\\nTo learn more about performance characteristics when implementing new algorithms, refer to Cliff\\'s [KV Store Guide](http://0xdata.com/blog/2014/05/kv-store-memory-analytics-part-2-2/).\\n\\n---\\n\\n**How do I find the standard errors of the parameter estimates (p-values)?**\\n\\nP-values are currently supported for non-regularized GLM. The following requirements must be met:\\n\\n- The family cannot be multinomial\\n- The lambda value must be equal to zero\\n- The IRLSM solver must be used\\n- Lambda search cannot be used\\n\\nTo generate p-values, do one of the following:\\n\\n- check the *compute_p_values* checkbox in the GLM model builder in Flow\\n- use `compute_p_values=TRUE` in R or Python while creating the model\\n\\nThe p-values are listed in the coefficients table (as shown in the following example screenshot):\\n\\n  ![Coefficients Table with P-values](images/Flow_Pvalues.png)\\n\\n\\n---\\n\\n**How do I specify regression or classification for Distributed Random Forest in the web UI?**\\n\\n\\nIf the response column is numeric, H2O generates a regression model. If the response column is enum, the model uses classification. To specify the column type, select it from the drop-down column name list in the **Edit Column Names and Types** section during parsing.\\n\\n---\\n\\n**What\\'s the largest number of classes that H2O supports for multinomial prediction?**\\n\\nFor tree-based algorithms, the maximum number of classes (or levels) for a response column is 1000.\\n\\n---\\n\\n**How do I obtain a tree diagram of my DRF model?**\\n\\nOutput the SVG code for the edges and nodes. A simple tree visitor is available [here](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/main/java/hex/tree/TreeVisitor.java) and the Java code generator is available [here](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/main/java/hex/tree/TreeJCodeGen.java).\\n\\n---\\n\\n**Is Word2Vec available? I can see the Java and R sources, but calling the API generates an error.**\\n\\nWord2Vec, along with other natural language processing (NLP) algos, are currently in development in the current version of H2O.\\n\\n---\\n\\n**What are the \"best practices\" for preparing data for a K-Means model?**\\n\\nThere aren\\'t specific \"best practices,\" as it depends on your data and the column types. However, removing outliers and transforming any categorical columns to have the same weight as the numeric columns will help, especially if you\\'re standardizing your data.\\n\\n\\n---\\n\\n**What is your implementation of Deep Learning based on?**\\n\\n Our Deep Learning algorithm is based on the feedforward neural net. For more information, refer to our Data Science documentation or [Wikipedia](https://en.wikipedia.org/wiki/Feedforward_neural_network).\\n\\n---\\n\\n**How is deviance computed for a Deep Learning regression model?**\\n\\nFor a Deep Learning regression model, deviance is computed as follows:\\n\\nLoss = MeanSquare -> MSE==Deviance\\nFor Absolute/Laplace or Huber -> MSE != Deviance.\\n\\n---\\n\\n**For my 0-tree GBM multinomial model, I got a different score depending on whether or not validation was enabled, even though my dataset was the same - why is that?**\\n\\nDifferent results may be generated because of the way H2O computes the initial MSE.\\n\\n\\n---\\n\\n**How does your Deep Learning Autoencoder work? Is it deep or shallow?**\\n\\nH2Os DL autoencoder is based on the standard deep (multi-layer) neural net architecture, where the entire network is learned together, instead of being stacked layer-by-layer.  The only difference is that no response is required in the input and that the output layer has as many neurons as the input layer. If you dont achieve convergence, then try using the *Tanh* activation and fewer layers.  We have some example test scripts [here](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/), and even some that show [how stacked auto-encoders can be implemented in R](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_stacked_autoencoder_large.R).\\n\\n\\n\\n\\n---\\n\\n**Are there any H2O examples using text for classification?**\\n\\nCurrently, the following examples are available for Sparkling Water:\\n\\na) Use TF-IDF weighting scheme for classifying text messages\\nhttps://github.com/h2oai/sparkling-water/blob/master/examples/scripts/mlconf_2015_hamSpam.script.scala\\n\\nb) Use Word2Vec Skip-gram model + GBM for classifying job titles\\nhttps://github.com/h2oai/sparkling-water/blob/master/examples/scripts/craigslistJobTitles.scala\\n\\n---\\n\\n**Most machine learning tools cannot predict with a new categorical level that was not included in the training set. How does H2O make predictions in this scenario?**\\n\\nHere is an example of how the prediction process works in H2O:\\n\\n1. Train a model using data that has a categorical predictor column with levels B,C, and D (no other levels); this level will be the \"training set domain\": {B,C,D}\\n2. During scoring, the test set has only rows with levels A,C, and E for that column; this is the \"test set domain\": {A,C,E}\\n3. For scoring, a combined \"scoring domain\" is created, which is the training domain appended with the extra test set domain entries: {B,C,D,A,E}\\n4. Each model can handle these extra levels {A,E} separately during scoring.\\n\\nThe behavior for unseen categorical levels depends on the algorithm and how it handles missing levels (NA values):\\n\\n- For DRF and GBM, missing values are interpreted as containing information (i.e., missing for a reason), rather than missing at random. During tree building, split decisions for every node are found by minimizing the loss function and treating missing values as a separate category that can go either left or right.\\n- Deep Learning creates an extra input neuron for missing and unseen categorical levels, which can remain untrained if there were no missing or unseen categorical levels in the training data, resulting in a random contribution to the next layer during testing.\\n- GLM skips unseen levels in the beta*x dot product.\\n\\n---\\n\\n**How are quantiles computed?**\\n\\nThe quantile results in Flow are computed lazily on-demand and cached. It is a fast approximation (max - min / 1024) that is very accurate for most use cases.\\nIf the distribution is skewed, the quantile results may not be as accurate as the results obtained using `h2o.quantile` in R or `H2OFrame.quantile` in Python.\\n\\n\\n---\\n\\n**How do I create a classification model? The model always defaults to regression.**\\n\\nTo create a classification model, the response column type must be `enum` - if the response is `numeric`, a regression model is created.\\n\\nTo convert the response column:\\n\\n- Before parsing, click the drop-down menu to the right of the column name or number and select `Enum`\\n\\n  ![Parsing - Convert to Enum](images/Flow_Parse_ConvertEnum.png)\\n\\n  or\\n\\n- Click on the .hex link for the data frame (or use the `getFrameSummary \"<frame_name>.hex\"` command, where `<frame_name>` is the name of the frame), then click the **Convert to enum** link to the right of the column name or number\\n\\n  ![Summary - Convert to Enum](images/Flow_Summary_ConvertToEnum.png)\\n\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Building H2O\\n\\n**During the build process, the following error message displays. What do I need to do to resolve it?**\\n\\n```\\nError: Missing name at classes.R:19\\nIn addition: Warning messages:\\n1: @S3method is deprecated. Please use @export instead\\n2: @S3method is deprecated. Please use @export instead\\nExecution halted\\n```\\n\\nTo build H2O, [Roxygen2](https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html) version 4.1.1 is required.\\n\\nTo update your Roxygen2 version, install the `versions` package in R, then use `install.versions(\"roxygen2\", \"4.1.1\")`.\\n\\n\\n\\n---\\n\\n**Using `./gradlew build` doesn\\'t generate a build successfully - is there anything I can do to troubleshoot?**\\n\\nUse `./gradlew clean` before running `./gradlew build`.\\n\\n---\\n\\n**I tried using `./gradlew build` after using `git pull` to update my local H2O repo, but now I can\\'t get H2O to build successfully - what should I do?**\\n\\nTry using `./gradlew build -x test` - the build may be failing tests if data is not synced correctly.\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': \"## Clusters\\n\\n**When trying to launch H2O, I received the following error message: `ERROR: Too many retries starting cloud.` What should I do?**\\n\\nIf you are trying to start a multi-node cluster where the nodes use multiple network interfaces, by default H2O will resort to using the default host (127.0.0.1).\\n\\nTo specify an IP address, launch H2O using the following command:\\n\\n`java -jar h2o.jar -ip <IP_Address> -port <PortNumber>`\\n\\nIf this does not resolve the issue, try the following additional troubleshooting tips:\\n\\n- Confirm your internet connection is active.\\n\\n- Test connectivity using curl: First, log in to the first node and enter curl http://<Node2IP>:54321 (where <Node2IP> is the IP address of the second node. Then, log in to the second node and enter curl http://<Node1IP>:54321 (where <Node1IP> is the IP address of the first node). Look for output from H2O.\\n\\n- Confirm ports 54321 and 54322 are available for TCP.\\n- Confirm your firewall is not preventing the nodes from locating each other.\\n- Confirm the nodes are not using different versions of H2O.\\n- Confirm that the username is the same on all nodes; if not, define the cloud in the terminal when launching using `-name`:`java -jar h2o.jar -name myCloud`.\\n- Confirm that the nodes are not on different networks.\\n- Check if the nodes have different interfaces; if so, use the -network option to define the network (for example, `-network 127.0.0.1`).\\n- Force the bind address using `-ip`:`java -jar h2o.jar -ip <IP_Address> -port <PortNumber>`.\\n- (Linux only) Check if you have SELINUX or IPTABLES enabled; if so, disable them.\\n- (EC2 only) Check the configuration for the EC2 security group.\\n\\n---\\n\\n**What should I do if I tried to start a cluster but the nodes started independent clouds that are not connected?**\\n\\nBecause the default cloud name is the user name of the node, if the nodes are on different operating systems (for example, one node is using Windows and the other uses OS X), the different user names on each machine will prevent the nodes from recognizing that they belong to the same cloud. To resolve this issue, use `-name` to configure the same name for all nodes.\\n\\n---\\n\\n**One of the nodes in my cluster is unavailable  what do I do?**\\n\\nH2O does not support high availability (HA). If a node in the cluster is unavailable, bring the cluster down and create a new healthy cluster.\\n\\n---\\n\\n**How do I add new nodes to an existing cluster?**\\n\\nNew nodes can only be added if H2O has not started any jobs. Once H2O starts a task, it locks the cluster to prevent new nodes from joining. If H2O has started a job, you must create a new cluster to include additional nodes.\\n\\n---\\n\\n**How do I check if all the nodes in the cluster are healthy and communicating?**\\n\\nIn the Flow web UI, click the **Admin** menu and select **Cluster Status**.\\n\\n---\\n\\n**How do I create a cluster behind a firewall?**\\n\\nH2O uses two ports:\\n\\n- The `REST_API` port (54321): Specify when launching H2O using `-port`; uses TCP only.\\n- The `INTERNAL_COMMUNICATION` port (54322): Implied based on the port specified as the `REST_API` port, +1; requires TCP.\\n\\nYou can start the cluster behind the firewall, but to reach it, you must make a tunnel to reach the `REST_API` port. To use the cluster, the `REST_API` port of at least one node must be reachable.\\n\\n---\\n\\n\\n**I launched H2O instances on my nodes - why won't they form a cloud?**\\n\\nIf you launch without specifying the IP address by adding argument -ip:\\n\\n`$ java -Xmx20g -jar h2o.jar -flatfile flatfile.txt -port 54321`\\n\\nand multiple local IP addresses are detected, H2O uses the default localhost (127.0.0.1) as shown below:\\n\\n  ```\\n  10:26:32.266 main      WARN WATER: Multiple local IPs detected:\\n  +                                    /198.168.1.161  /198.168.58.102\\n  +                                  Attempting to determine correct address...\\n  10:26:32.284 main      WARN WATER: Failed to determine IP, falling back to localhost.\\n  10:26:32.325 main      INFO WATER: Internal communication uses port: 54322\\n  +                                  Listening for HTTP and REST traffic\\n  +                                  on http://127.0.0.1:54321/\\n  10:26:32.378 main      WARN WATER: Flatfile configuration does not include self:\\n  /127.0.0.1:54321 but contains [/192.168.1.161:54321, /192.168.1.162:54321]\\n  ```\\n\\nTo avoid using 127.0.0.1 on servers with multiple local IP addresses, run the command with the -ip argument to force H2O to launch at the specified IP:\\n\\n`$ java -Xmx20g -jar h2o.jar -flatfile flatfile.txt -ip 192.168.1.161 -port 54321`\\n\\n---\\n\\n**How does the timeline tool work?**\\n\\nThe timeline is a debugging tool that provides information on the current communication between H2O nodes. It shows a snapshot of the most recent messages passed between the nodes. Each node retains its own history of messages sent to or received from other nodes.\\n\\nH2O collects these messages from all the nodes and orders them by whether they were sent or received. Each node has an implicit internal order where sent messages must precede received messages on the other node.\\n\\nThe following information displays for each message:\\n\\n- `HH:MM:SS:MS` and `nanosec`: The local time of the event\\n- `Who`: The endpoint of the message; can be either a source/receiver node or source node and multicast for broadcasted messages\\n- `I/O Type`: The type of communication (TCP)\\n- `Event`: The type of H2O message. The most common type is a distributed task, which displays as `exec` (the requested task) -> `ack` (results of the processed task) -> `ackck` (sender acknowledges receiving the response, task is completed and removed)\\n- `rebooted`: Sent during node startup\\n- `heartbeat`: Provides small message tracking information about node health, exchanged periodically between nodes\\n- `fetchack`: Aknowledgement of the `Fetch` type task, which retrieves the ID of a previously unseen type\\n- `bytes`: Information extracted from the message, including the type of the task and the unique task number\\n\\n\\n\\n---\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Data\\n\\n**How should I format my SVMLight data before importing?**\\n\\nThe data must be formatted as a sorted list of unique integers, the column indices must be >= 1, and the columns must be in ascending order.\\n\\n---\\n\\n\\n**What date and time formats does H2O support?**\\n\\nH2O is set to auto-detect two major date/time formats. Because many date time formats are ambiguous (e.g. 01/02/03), general date time detection is not used.\\n\\nThe first format is for dates formatted as yyyy-MM-dd. Year is a four-digit number, the month is a two-digit number ranging from 1 to 12, and the day is a two-digit value ranging from 1 to 31. This format can also be followed by a space and then a time (specified below).\\n\\nThe second date format is for dates formatted as dd-MMM-yy. Here the day must be one or two digits with a value ranging from 1 to 31. The month must be either a three-letter abbreviation or the full month name but is not case sensitive. The year must be either two or four digits. In agreement with [POSIX](https://en.wikipedia.org/wiki/POSIX) standards, two-digit dates >= 69 are assumed to be in the 20th century (e.g. 1969) and the rest are part of the 21st century. This date format can be followed by either a space or colon character and then a time. The \\'-\\' between the values is optional.\\n\\nTimes are specified as HH:mm:ss. HH is a two-digit hour and must be a value between 0-23 (for 24-hour time) or 1-12 (for a twelve-hour clock). mm is a two-digit minute value and must be a value between 0-59. ss is a two-digit second value and must be a value between 0-59. This format can be followed with either milliseconds, nanoseconds, and/or the cycle (i.e. AM/PM). If milliseconds are included, the format is HH:mm:ss:SSS. If nanoseconds are included, the format is HH:mm:ss:SSSnnnnnn. H2O only stores fractions of a second up to the millisecond, so accuracy may be lost. Nanosecond parsing is only included for convenience. Finally, a valid time can end with a space character and then either \"AM\" or \"PM\". For this format, the hours must range from 1 to 12. Within the time, the \\':\\' character can be replaced with a \\'.\\' character.\\n\\n---\\n\\n**How does H2O handle name collisions/conflicts in the dataset?**\\n\\nIf there is a name conflict (for example, column 48 isn\\'t named, but C48 already exists), then the column name in concatenated to itself until a unique name is created. So for the previously cited example, H2O will try renaming the column to C48C48, then C48C48C48, and so on until an unused name is generated.\\n\\n---\\n\\n**What types of data columns does H2O support?**\\n\\nCurrently, H2O supports:\\n\\n- float (any IEEE double)\\n- integer (up to 64bit, but compressed according to actual range)\\n- factor (same as integer, but with a String mapping, often handled differently in the algorithms)\\n- time (same as 64bit integer, but with a time-since-Unix-epoch interpretation)\\n- UUID (128bit integer, no math allowed)\\n- String\\n\\n---\\n\\n**I am trying to parse a Gzip data file containing multiple files, but it does not parse as quickly as the uncompressed files. Why is this?**\\n\\nParsing Gzip files is not done in parallel, so it is sequential and uses only one core. Other parallel parse compression schemes are on the roadmap.\\n\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## General\\n\\n**How do I score using an exported JSON model?**\\n\\nSince JSON is just a representation format, it cannot be directly executed, so a JSON export can\\'t be used for scoring. However, you can score by:\\n\\n- including the POJO in your execution stream and handing it observations one at a time\\n\\n  or\\n\\n- handing your data in bulk to an H2O cluster, which will score using high throughput parallel and distributed bulk scoring.\\n\\n\\n---\\n\\n**How do I score using an exported POJO?**\\n\\nThe generated POJO can be used indepedently of a H2O cluster. First use `curl` to send the h2o-genmodel.jar file and the java code for model to the server. The following is an example; the ip address and model names will need to be changed.\\n\\n```\\nmkdir tmpdir\\ncd tmpdir\\ncurl http://127.0.0.1:54321/3/h2o-genmodel.jar > h2o-genmodel.jar\\ncurl http://127.0.0.1:54321/3/Models.java/gbm_model > gbm_model.java\\n```\\n\\nTo score a simple .CSV file, download the [PredictCSV.java](https://raw.githubusercontent.com/h2oai/h2o-3/master/h2o-r/tests/testdir_javapredict/PredictCSV.java) file and compile it with the POJO. Make a subdirectory for the compilation (this is useful if you have multiple models to score on).\\n\\n```\\nwget https://raw.githubusercontent.com/h2oai/h2o-3/master/h2o-r/tests/testdir_javapredict/PredictCSV.java\\nmkdir gbm_model_dir\\njavac -cp h2o-genmodel.jar -J-Xmx2g -J-XX:MaxPermSize=128m PredictCSV.java gbm_model.java -d gbm_model_dir\\n```\\n\\nSpecify the following:\\n- the classpath using `-cp`\\n- the model name (or class) using `--model`\\n- the csv file you want to score using `--input`\\n- the location for the predictions using `--output`.\\n\\nYou must match the table column names to the order specified in the POJO. The output file will be in a .hex format, which is a lossless text representation of floating point numbers. Both R and Java will be able to read the hex strings as numerics.\\n\\n```\\njava -ea -cp h2o-genmodel.jar:gbm_model_dir -Xmx4g -XX:MaxPermSize=256m -XX:ReservedCodeCacheSize=256m PredictCSV --header --model gbm_model --input input.csv --output output.csv\\n```\\n\\n---\\n\\n**How do I predict using multiple response variables?**\\n\\nCurrently, H2O does not support multiple response variables. To predict different response variables, build multiple models.\\n\\n---\\n\\n**How do I kill any running instances of H2O?**\\n\\nIn Terminal, enter `ps -efww | grep h2o`, then kill any running PIDs. You can also find the running instance in Terminal and press **Ctrl + C** on your keyboard. To confirm no H2O sessions are still running, go to `http://localhost:54321` and verify that the H2O web UI does not display.\\n\\n---\\n\\n\\n**Why is H2O not launching from the command line?**\\n\\n\\t$ java -jar h2o.jar &\\n\\t% Exception in thread \"main\" java.lang.ExceptionInInitializerError\\n\\tat java.lang.Class.initializeClass(libgcj.so.10)\\n\\tat water.Boot.getMD5(Boot.java:73)\\n\\tat water.Boot.<init>(Boot.java:114)\\n\\tat water.Boot.<clinit>(Boot.java:57)\\n\\tat java.lang.Class.initializeClass(libgcj.so.10)\\n    Caused by: java.lang.IllegalArgumentException\\n    at java.util.regex.Pattern.compile(libgcj.so.10)\\n    at water.util.Utils.<clinit>(Utils.java:1286)\\n    at java.lang.Class.initializeClass(libgcj.so.10)\\n    ...4 more\\n\\nThe only prerequisite for running H2O is a compatible version of Java. We recommend Oracle\\'s [Java 1.7](http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html).\\n\\n\\n---\\n\\n**Why did I receive the following error when I tried to launch H2O?**\\n\\n```\\n[root@sandbox h2o-dev-0.3.0.1188-hdp2.2]hadoop jar h2odriver.jar -nodes 2 -mapperXmx 1g -output hdfsOutputDirName\\nDetermining driver host interface for mapper->driver callback...\\n   [Possible callback IP address: 10.0.2.15]\\n   [Possible callback IP address: 127.0.0.1]\\nUsing mapper->driver callback IP address and port: 10.0.2.15:41188\\n(You can override these with -driverif and -driverport.)\\nMemory Settings:\\n   mapreduce.map.java.opts:     -Xms1g -Xmx1g -Dlog4j.defaultInitOverride=true\\n   Extra memory percent:        10\\n   mapreduce.map.memory.mb:     1126\\n15/05/08 02:33:40 INFO impl.TimelineClientImpl: Timeline service address: http://sandbox.hortonworks.com:8188/ws/v1/timeline/\\n15/05/08 02:33:41 INFO client.RMProxy: Connecting to ResourceManager at sandbox.hortonworks.com/10.0.2.15:8050\\n15/05/08 02:33:47 INFO mapreduce.JobSubmitter: number of splits:2\\n15/05/08 02:33:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1431052132967_0001\\n15/05/08 02:33:51 INFO impl.YarnClientImpl: Submitted application application_1431052132967_0001\\n15/05/08 02:33:51 INFO mapreduce.Job: The url to track the job: http://sandbox.hortonworks.com:8088/proxy/application_1431052132967_0001/\\nJob name \\'H2O_3889\\' submitted\\nJobTracker job ID is \\'job_1431052132967_0001\\'\\nFor YARN users, logs command is \\'yarn logs -applicationId application_1431052132967_0001\\'\\nWaiting for H2O cluster to come up...\\nH2O node 10.0.2.15:54321 requested flatfile\\nERROR: Timed out waiting for H2O cluster to come up (120 seconds)\\nERROR: (Try specifying the -timeout option to increase the waiting time limit)\\n15/05/08 02:35:59 INFO impl.TimelineClientImpl: Timeline service address: http://sandbox.hortonworks.com:8188/ws/v1/timeline/\\n15/05/08 02:35:59 INFO client.RMProxy: Connecting to ResourceManager at sandbox.hortonworks.com/10.0.2.15:8050\\n\\n----- YARN cluster metrics -----\\nNumber of YARN worker nodes: 1\\n\\n----- Nodes -----\\nNode: http://sandbox.hortonworks.com:8042 Rack: /default-rack, RUNNING, 1 containers used, 0.2 / 2.2 GB used, 1 / 8 vcores used\\n\\n----- Queues -----\\nQueue name:            default\\n   Queue state:       RUNNING\\n   Current capacity:  0.11\\n   Capacity:          1.00\\n   Maximum capacity:  1.00\\n   Application count: 1\\n   ----- Applications in this queue -----\\n   Application ID:                  application_1431052132967_0001 (H2O_3889)\\n       Started:                     root (Fri May 08 02:33:50 UTC 2015)\\n       Application state:           FINISHED\\n       Tracking URL:                http://sandbox.hortonworks.com:8088/proxy/application_1431052132967_0001/jobhistory/job/job_1431052132967_0001\\n       Queue name:                  default\\n       Used/Reserved containers:    1 / 0\\n       Needed/Used/Reserved memory: 0.2 GB / 0.2 GB / 0.0 GB\\n       Needed/Used/Reserved vcores: 1 / 1 / 0\\n\\nQueue \\'default\\' approximate utilization: 0.2 / 2.2 GB used, 1 / 8 vcores used\\n\\n----------------------------------------------------------------------\\n\\nERROR:   Job memory request (2.2 GB) exceeds available YARN cluster memory (2.2 GB)\\nWARNING: Job memory request (2.2 GB) exceeds queue available memory capacity (2.0 GB)\\nERROR:   Only 1 out of the requested 2 worker containers were started due to YARN cluster resource limitations\\n\\n----------------------------------------------------------------------\\nAttempting to clean up hadoop job...\\n15/05/08 02:35:59 INFO impl.YarnClientImpl: Killed application application_1431052132967_0001\\nKilled.\\n[root@sandbox h2o-dev-0.3.0.1188-hdp2.2]#\\n```\\n\\nThe H2O launch failed because more memory was requested than was available. Make sure you are not trying to specify more memory in the launch parameters than you have available.\\n\\n---\\n\\n**How does the architecture of H2O work?**\\n\\nThis [PDF](https://github.com/h2oai/h2o-meetups/blob/master/2014_11_18_H2O_in_Big_Data_Environments/H2OinBigDataEnvironments.pdf) includes diagrams and slides depicting how H2O works in big data environments.\\n\\n---\\n\\n**I received the following error message when launching H2O - how do I resolve the error?**\\n\\n```\\nInvalid flow_dir illegal character at index 12...\\n```\\n\\nThis error message means that there is a space (or other unsupported character) in your H2O directory. To resolve this error:\\n\\n- Create a new folder without unsupported characters to use as the H2O directory (for example, `C:\\\\h2o`).\\n\\n  or\\n\\n- Specify a different save directory using the `-flow_dir` parameter when launching H2O: `java -jar h2o.jar -flow_dir test`\\n\\n---\\n\\n**How does `importFiles()` work in H2O?**\\n\\n`importFiles()` gets the basic information for the file and then returns a key representing that file. This key is used during parsing to read in the file and to save space so that the file isn\\'t loaded every time; instead, it is loaded into H2O then referenced using the key. For files hosted online, H2O verifies the destination is valid, creates a vec that loads the file when necessary, and returns a key.\\n\\n\\n---\\n\\n**Does H2O support GPUs?**\\n\\nCurrently, we do not support this capability. If you are interested in contributing your efforts to support this feature to our open-source code database, please contact us at [h2ostream@googlegroups.com](mailto:h2ostream@googlegroups.com).\\n\\n\\n---\\n\\n**How can I continue working on a model in H2O after restarting?**\\n\\nThere are a number of ways you can save your model in H2O:\\n\\n- In the web UI, click the **Flow** menu then click **Save Flow**. Your flow is saved to the *Flows* tab in the **Help** sidebar on the right.\\n- In the web UI, click the **Flow** menu then click **Download this Flow...**. Depending on your browser and configuration, your flow is saved to the \"Downloads\" folder (by default) or to the location you specify in the pop-up **Save As** window if it appears.\\n- (For DRF, GBM, and DL models only): Use model checkpointing to resume training a model. Copy the `model_id` number from a built model and paste it into the *checkpoint* field in the `buildModel` cell.\\n\\n\\n---\\n\\n**How can I find out more about H2O\\'s real-time, nano-fast scoring engine?**\\n\\nH2O\\'s scoring engine uses a Plain Old Java Object (POJO). The POJO code runs quickly but is single-threaded.  It is intended for embedding into lightweight real-time environments.\\n\\nAll the work is done by the call to the appropriate predict method.  There is no involvement from H2O in this case.\\n\\nTo compare multiple models simultaneously, use the POJO to call the models using multiple threads. For more information on using POJOs, refer to the [POJO Quick Start Guide](http://h2o-release.s3.amazonaws.com/h2o/master/3167/docs-website/h2o-docs/index.html#POJO%20Quick%20Start) and [POJO Java Documentation](http://h2o-release.s3.amazonaws.com/h2o/master/3167/docs-website/h2o-genmodel/javadoc/index.html)\\n\\nIn-H2O scoring is triggered on an existing H2O cluster, typically using a REST API call. H2O evaluates the predictions in a parallel and distributed fashion for this case.  The predictions are stored into a new Frame and can be written out using `h2o.exportFile()`, for example.\\n\\n---\\n\\n**I am using an older version of H2O (2.8 or prior) - where can I find documentation for this version?**\\n\\nIf you are using H2O 2.8 or prior, we strongly recommend <a href=\"http://h2o.ai/download/\" target=\"_blank\">upgrading to the latest version of H2O</a> if possible.\\n\\nIf you do not wish to upgrade to the latest version, documentation for H2O Classic is available [here](http://docs.h2o.ai/h2oclassic/index.html).\\n\\n---\\n\\n**I am writing an academic research paper and I would like to cite H2O in my bibliography - how should I do that?**\\n\\nTo cite our software:\\n\\n- The H2O.ai Team. (2015) *h2o: R Interface for H2O*. R package version 3.1.0.99999. http://www.h2o.ai.\\n\\n- The H2O.ai Team. (2015) *h2o: h2o: Python Interface for H2O*. Python package version 3.1.0.99999. http://www.h2o.ai.\\n\\n- - The H2O.ai Team. (2015) *H2O: Scalable Machine Learning*. Version 3.1.0.99999. http://www.h2o.ai.\\n\\n\\nTo cite one of our booklets:\\n\\n-  Nykodym, T., Hussami, N., Kraljevic, T.,Rao, A., and Wang, A. (Sept. 2015). *Generalized Linear Modeling with H2O.* http://h2o.ai/resources.\\n\\n- Candel, A., LeDell, E., Parmar, V., and Arora, A. (Sept. 2015). *Deep Learning with H2O.* http://h2o.ai/resources.\\n\\n- Click, C.,  Malohlava, M., Parmar, V., and Roark, H. (Sept. 2015). *Gradient Boosted Models with H2O.* http://h2o.ai/resources.\\n\\n- Aiello, S., Eckstrand, E., Fu, A., Landry, M., and Aboyoun, P. (Sept. 2015) *Fast Scalable R with H2O.* http://h2o.ai/resources.\\n\\n- Aiello, S., Click, C., Roark, H. and Rehak, L. (Sept. 2015) *Machine Learning with Python and H2O* http://h2o.ai/resources.\\n\\n- Malohlava, M., and Tellez, A. (Sept. 2015) *Machine Learning with Sparkling Water: H2O + Spark*  http://h2o.ai/resources.\\n\\n\\nIf you are using Bibtex:\\n\\n```\\n\\n@Manual{h2o_GLM_booklet,\\n    title = {Generalized Linear Modeling with H2O},\\n    author = {Nykodym, T. and Hussami, N. and Kraljevic, T. and Rao, A. and Wang, A.},\\n    year = {2015},\\n    month = {September},\\n    url = {http://h2o.ai/resources},\\n}\\n\\n@Manual{h2o_DL_booklet,\\n    title = {Deep Learning with H2O},\\n    author = {Candel, A. and LeDell, E. and Arora, A. and Parmar, V.},\\n    year = {2015},\\n    month = {September},\\n    url = {http://h2o.ai/resources},\\n}\\n\\n@Manual{h2o_GBM_booklet,\\n    title = {Gradient Boosted Models},\\n    author = {Click, C. and Lanford, J. and Malohlava, M. and Parmar, V. and Roark, H.},\\n    year = {2015},\\n    month = {September},\\n    url = {http://h2o.ai/resources},\\n}\\n\\n@Manual{h2o_R_booklet,\\n    title = {Fast Scalable R with H2O},\\n    author = {Aiello, S. and Eckstrand, E. and Fu, A. and Landry, M. and Aboyoun, P. },\\n    year = {2015},\\n    month = {September},\\n    url = {http://h2o.ai/resources},\\n}\\n\\n@Manual{h2o_R_package,\\n    title = {h2o: R Interface for H2O},\\n    author = {The H2O.ai team},\\n    year = {2015},\\n    note = {R package version 3.1.0.99999},\\n    url = {http://www.h2o.ai},\\n}\\n\\n\\n@Manual{h2o_Python_module,\\n    title = {h2o: Python Interface for H2O},\\n    author = {The H2O.ai team},\\n    year = {2015},\\n    note = {Python package version 3.1.0.99999},\\n    url = {http://www.h2o.ai},\\n}\\n\\n\\n@Manual{h2o_Java_software,\\n    title = {H2O: Scalable Machine Learning},\\n    author = {The H2O.ai team},\\n    year = {2015},\\n    note = {version 3.1.0.99999},\\n    url = {http://www.h2o.ai},\\n}\\n\\n```\\n\\n---\\n\\n**How can I use Flow to export the prediction results with a dataset?**\\n\\nAfter obtaining your results, click the **Combine predictions with frame** button, then click the **View Frame** button.\\n\\n\\n\\n---\\n\\n\\n**What are these RTMP and py_ temporary Frames? Why are they the same size as my original data?**\\n\\nNo data is copied.\\nH2O does a classic copy-on-write optimization.\\nThat Frame you see - it\\'s nothing more than a thin wrapper over an internal list of columns; the columns are shared to avoid the copying.\\n\\nThe RTMP\\'s now need to be entirely managed by the H2O wrapper - because indeed they are using shared state under the hood.  If you delete one, you probably delete parts of others.  Instead, temp management should be automatic and \"good\" - as in: it\\'s a bug if you need to delete a temp manually, or if passing around Frames, or adding or removing columns turns into large data copies.\\n\\nR\\'s GC is now used to remove unused R temps, and when the last use of a shared column goes away, then the H2O wrapper will tell the H2O cluster to remove that no longer needed column.\\n\\nIn other words:\\nDon\\'t delete RTMPs, they\\'ll disappear at the next R GC.\\nDon\\'t worry about copies (they aren\\'t getting made).\\nDo Nothing and All Is Well.\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Hadoop\\n\\n**Why did I get an error in R when I tried to save my model to my home directory in Hadoop?**\\n\\nTo save the model in HDFS, prepend the save directory with `hdfs://`:\\n\\n```\\n# build model\\nmodel = h2o.glm(model params)\\n\\n# save model\\nhdfs_name_node <- \"mr-0x6\"\\nhdfs_tmp_dir <- \"/tmp/runit\\nmodel_path <- sprintf(\"hdfs://%s%s\", hdfs_name_node, hdfs_tmp_dir)\\nh2o.saveModel(model, dir = model_path, name = mymodel\")\\n```\\n\\n---\\n\\n**How do I specify which nodes should run H2O in a Hadoop cluster?**\\n\\nAfter creating and applying the desired node labels and associating them with specific queues as described in the [Hadoop documentation](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.2.0/YARN_RM_v22/node_labels/index.html#Item1.1), launch H2O using the following command:\\n\\n`hadoop jar h2odriver.jar -Dmapreduce.job.queuename=<my-h2o-queue> -nodes <num-nodes> -mapperXmx 6g -output hdfsOutputDirName`\\n\\n\\n- `-Dmapreduce.job.queuename=<my-h2o-queue>` represents the queue name\\n- `-nodes <num-nodes>` represents the number of nodes\\n- `-mapperXmx 6g` launches H2O with 6g of memory\\n- `-output hdfsOutputDirName` specifies the HDFS output directory as `hdfsOutputDirName`\\n\\n---\\n\\n**How do I import data from HDFS in R and in Flow?**\\n\\nTo import from HDFS in R:\\n\\n```\\nh2o.importFolder(path, pattern = \"\", destination_frame = \"\", parse = TRUE, header = NA, sep = \"\", col.names = NULL, na.strings = NULL)\\n```\\n\\nHere is another example:\\n\\n```\\n# pathToAirlines <- \"hdfs://mr-0xd6.0xdata.loc/datasets/airlines_all.csv\"\\n# airlines.hex <- h2o.importFile(path = pathToAirlines, destination_frame = \"airlines.hex\")\\n```\\n\\n\\nIn Flow, the easiest way is to let the auto-suggestion feature in the *Search:* field complete the path for you. Just start typing the path to the file, starting with the top-level directory, and H2O provides a list of matching files.\\n\\n  ![Flow - Import Auto-Suggest](images/Flow_Import_AutoSuggest.png)\\n\\nClick the file to add it to the *Search:* field.\\n\\n---\\n\\n**Why do I receive the following error when I try to save my notebook in Flow?**\\n\\n```\\nError saving notebook: Error calling POST /3/NodePersistentStorage/notebook/Test%201 with opts\\n```\\n\\nWhen you are running H2O on Hadoop, H2O tries to determine the home HDFS directory so it can use that as the download location. If the default home HDFS directory is not found, manually set the download location from the command line using the `-flow_dir` parameter (for example, `hadoop jar h2odriver.jar <...> -flow_dir hdfs:///user/yourname/yourflowdir`). You can view the default download directory in the logs by clicking **Admin > View logs...** and looking for the line that begins `Flow dir:`.\\n\\n---\\n\\n**How do I access data in HDFS without launching H2O on YARN?**\\n\\nEach h2odriver.jar file is built with a specific Hadoop distribution so in order to have a working HDFS connection download the h2odriver.jar file for your Hadoop distribution.\\n\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-cdh5.4.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-cdh5.5.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-cdh5.6.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-cdh5.7.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-cdh5.8.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-hdp2.2.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-hdp2.3.zip\\n    \\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-hdp2.4.zip\\n    \\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-hdp2.5.zip\\n    \\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-hdp2.6.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-mapr4.0.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-mapr5.0.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-mapr5.1.zip\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/h2o-{{project_version}}-iop4.2.zip\\n\\n\\t**Note**: Enter only one of the above commands.\\n\\n\\nThen run the command to launch the H2O Application in the driver by specifying the classpath:\\n\\n\\t\\tunzip h2o-{{project_version}}-*.zip\\n\\t\\tcd h2o-{{project_version}}-*\\n\\t\\tjava -cp h2odriver.jar water.H2OApp\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Java\\n\\n**How do I use H2O with Java?**\\n\\nThere are two ways to use H2O with Java. The simplest way is to call the REST API from your Java program to a remote cluster and should meet the needs of most users.\\n\\nYou can access the REST API documentation within Flow, or on our [documentation site](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-docs/index.html#route-reference).\\n\\nFlow, Python, and R all rely on the REST API to run H2O. For example, each action in Flow translates into one or more REST API calls. The script fragments in the cells in Flow are essentially the payloads for the REST API calls. Most R and Python API calls translate into a single REST API call.\\n\\nTo see how the REST API is used with H2O:\\n\\n- Using Chrome as your internet browser, open the developer tab while viewing the web UI. As you perform tasks, review the network calls made by Flow.\\n\\n- Write an R program for H2O using the H2O R package that uses `h2o.startLogging()` at the beginning. All REST API calls used are logged.\\n\\nThe second way to use H2O with Java is to embed H2O within your Java application, similar to [Sparkling Water](https://github.com/h2oai/sparkling-water/blob/master/DEVEL.md).\\n\\n---\\n\\n**How do I communicate with a remote cluster using the REST API?**\\n\\nTo create a set of bare POJOs for the REST API payloads that can be used by JVM REST API clients:\\n\\n0. Clone the sources from GitHub.\\n0. Start an H2O instance.\\n0. Enter `% cd py`.\\n0. Enter `% python generate_java_binding.py`.\\n\\nThis script connects to the server, gets all the metadata for the REST API schemas, and writes the Java POJOs to `{sourcehome}/build/bindings/Java`.\\n\\n\\n---\\n\\n**I keep getting a message that I need to install Java. I have the latest version of Java installed, but I am still getting this message. What should I do?**\\n\\nThis error message displays if the `JAVA_HOME` environment variable is not set correctly. The `JAVA_HOME` variable is likely points to Apple Java version 6 instead of Oracle Java version 8.\\n\\nIf you are running OS X 10.7 or earlier, enter the following in Terminal:\\n`export JAVA_HOME=/Library/Internet\\\\ Plug-Ins/JavaAppletPlugin.plugin/Contents/Home`\\n\\nIf you are running OS X 10.8 or later, modify the launchd.plist by entering the following in Terminal:\\n\\n```\\ncat << EOF | sudo tee /Library/LaunchDaemons/setenv.JAVA_HOME.plist\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\\n  <plist version=\"1.0\">\\n  <dict>\\n  <key>Label</key>\\n  <string>setenv.JAVA_HOME</string>\\n  <key>ProgramArguments</key>\\n  <array>\\n    <string>/bin/launchctl</string>\\n    <string>setenv</string>\\n    <string>JAVA_HOME</string>\\n    <string>/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home</string>\\n  </array>\\n  <key>RunAtLoad</key>\\n  <true/>\\n  <key>ServiceIPC</key>\\n  <false/>\\n</dict>\\n</plist>\\nEOF\\n```\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Python\\n\\n**I tried to install H2O in Python but `pip install scikit-learn` failed - what should I do?**\\n\\nUse the following commands (prepending with `sudo` if necessary):\\n\\n```\\neasy_install pip\\npip install numpy\\nbrew install gcc\\npip install scipy\\npip install scikit-learn\\n```\\n\\nIf you are still encountering errors and you are using OSX, the default version of Python may be installed. We recommend installing the Homebrew version of Python instead:\\n\\n```\\nbrew install python\\n```\\n\\nIf you are encountering errors related to missing Python packages when using H2O, refer to the following list for a complete list of all Python packages, including dependencies:\\n\\n<table>\\n    <tr>\\n        <td><code>grip</code></td>\\n        <td><code>tabulate</code></td>\\n        <td><code>wheel</code></td>\\n        <td><code>jsonlite</code></td>\\n        <td><code>ipython</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>numpy</code></td>\\n        <td><code>scipy</code></td>\\n        <td><code>pandas</code></td>\\n        <td><code>-U gensim</code></td>\\n        <td><code>jupyter</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>-U PIL</code></td>\\n        <td><code>nltk</code></td>\\n        <td><code>beautifulsoup4</code></td>\\n        <td><code></code></td>\\n        <td><code></code></td>\\n   </tr>\\n</table>\\n\\n---\\n\\n**How do I specify a value as an enum in Python? Is there a Python equivalent of `as.factor()` in R?**\\n\\nUse `.asfactor()` to specify a value as an enum.\\n\\n---\\n\\n**I received the following error when I tried to install H2O using the Python instructions on the downloads page - what should I do to resolve it?**\\n\\n```\\nDownloading/unpacking http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/12/Python/h2o-3.0.0.12-py2.py3-none-any.whl\\n  Downloading h2o-3.0.0.12-py2.py3-none-any.whl (43.1Mb): 43.1Mb downloaded\\n  Running setup.py egg_info for package from http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/12/Python/h2o-3.0.0.12-py2.py3-none-any.whl\\n    Traceback (most recent call last):\\n      File \"<string>\", line 14, in <module>\\n    IOError: [Errno 2] No such file or directory: \\'/tmp/pip-nTu3HK-build/setup.py\\'\\n    Complete output from command python setup.py egg_info:\\n    Traceback (most recent call last):\\n\\n  File \"<string>\", line 14, in <module>\\n\\nIOError: [Errno 2] No such file or directory: \\'/tmp/pip-nTu3HK-build/setup.py\\'\\n\\n---\\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-nTu3HK-build\\n```\\n\\nWith Python, there is no automatic update of installed packages, so you must upgrade manually. Additionally, the package distribution method recently changed from `distutils` to `wheel`. The following procedure should be tried first if you are having trouble installing the H2O package, particularly if error messages related to `bdist_wheel` or `eggs` display.\\n\\n```\\n# this gets the latest setuptools\\n# see https://pip.pypa.io/en/latest/installing.html\\nwget https://bootstrap.pypa.io/ez_setup.py -O - | sudo python\\n\\n# platform dependent ways of installing pip are at\\n# https://pip.pypa.io/en/latest/installing.html\\n# but the above should work on most linux platforms?\\n\\n# on ubuntu\\n# if you already have some version of pip, you can skip this.\\nsudo apt-get install python-pip\\n\\n# the package manager doesn\\'t install the latest. upgrade to latest\\n# we\\'re not using easy_install any more, so don\\'t care about checking that\\npip install pip --upgrade\\n\\n# I\\'ve seen pip not install to the final version ..i.e. it goes to an almost\\n# final version first, then another upgrade gets it to the final version.\\n# We\\'ll cover that, and also double check the install.\\n\\n# after upgrading pip, the path name may change from /usr/bin to /usr/local/bin\\n# start a new shell, just to make sure you see any path changes\\n\\nbash\\n\\n# Also: I like double checking that the install is bulletproof by reinstalling.\\n# Sometimes it seems like things say they are installed, but have errors during the install. Check for no errors or stack traces.\\n\\npip install pip --upgrade --force-reinstall\\n\\n# distribute should be at the most recent now. Just in case\\n# don\\'t do --force-reinstall here, it causes an issue.\\n\\npip install distribute --upgrade\\n\\n\\n# Now check the versions\\npip list | egrep \\'(distribute|pip|setuptools)\\'\\ndistribute (0.7.3)\\npip (7.0.3)\\nsetuptools (17.0)\\n\\n\\n# Re-install wheel\\npip install wheel --upgrade --force-reinstall\\n\\n```\\n\\nAfter completing this procedure, go to Python and use `h2o.init()` to start H2O in Python.\\n\\n>**Note**:\\n>\\n>If you use gradlew to build the jar yourself, you have to start the jar >yourself before you do `h2o.init()`.\\n>\\n>If you download the jar and the H2O package, `h2o.init()` will work like R >and you don\\'t have to start the jar yourself.\\n\\n---\\n\\n**How should I specify the datatype during import in Python?**\\n\\nRefer to the following example:\\n\\n```\\n#Let\\'s say you want to change the second column \"CAPSULE\" of prostate.csv\\n#to categorical. You have 3 options.\\n\\n#Option 1. Use a dictionary of column names to types.\\nfr = h2o.import_file(\"smalldata/logreg/prostate.csv\", col_types = {\"CAPSULE\":\"Enum\"})\\nfr.describe()\\n\\n#Option 2. Use a list of column types.\\nc_types = [None]*9\\nc_types[1] = \"Enum\"\\nfr = h2o.import_file(\"smalldata/logreg/prostate.csv\", col_types = c_types)\\nfr.describe()\\n\\n#Option 3. Use parse_setup().\\nfraw = h2o.import_file(\"smalldata/logreg/prostate.csv\", parse = False)\\nfsetup = h2o.parse_setup(fraw)\\nfsetup[\"column_types\"][1] = \\'\"Enum\"\\'\\nfr = h2o.parse_raw(fsetup)\\nfr.describe()\\n```\\n\\n---\\n\\n**How do I view a list of variable importances in Python?**\\n\\nUse `model.varimp(return_list=True)` as shown in the following example:\\n\\n```\\nmodel = h2o.gbm(y = \"IsDepDelayed\", x = [\"Month\"], training_frame = df)\\nvi = model.varimp(return_list=True)\\nOut[26]:\\n[(u\\'Month\\', 69.27436828613281, 1.0, 1.0)]\\n```\\n\\n---\\n\\n**What is PySparkling? How can I use it for grid search or early stopping?**\\n\\nPySparkling basically calls H2O Python functions for all operations on H2O data frames. You can perform all H2O Python operations available in H2O Python version 3.6.0.3 or later from PySparkling.\\n\\nFor help on a function within IPython Notebook, run `H2OGridSearch?`\\n\\nHere is an example of grid search in PySparkling:\\n\\n```\\nfrom h2o.grid.grid_search import H2OGridSearch\\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\\n\\niris = h2o.import_file(\"/Users/nidhimehta/h2o-dev/smalldata/iris/iris.csv\")\\n\\nntrees_opt = [5, 10, 15]\\nmax_depth_opt = [2, 3, 4]\\nlearn_rate_opt = [0.1, 0.2]\\nhyper_parameters = {\"ntrees\": ntrees_opt, \"max_depth\":max_depth_opt,\\n          \"learn_rate\":learn_rate_opt}\\n\\ngs = H2OGridSearch(H2OGradientBoostingEstimator(distribution=\\'multinomial\\'), hyper_parameters)\\ngs.train(x=range(0,iris.ncol-1), y=iris.ncol-1, training_frame=iris, nfold=10)\\n\\n#gs.show\\nprint gs.sort_by(\\'logloss\\', increasing=True)\\n```\\n\\nHere is an example of early stopping in PySparkling:\\n\\n```\\nfrom h2o.grid.grid_search import H2OGridSearch\\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator\\n\\nhidden_opt = [[32,32],[32,16,8],[100]]\\nl1_opt = [1e-4,1e-3]\\nhyper_parameters = {\"hidden\":hidden_opt, \"l1\":l1_opt}\\n\\nmodel_grid = H2OGridSearch(H2ODeepLearningEstimator, hyper_params=hyper_parameters)\\nmodel_grid.train(x=x, y=y, distribution=\"multinomial\", epochs=1000, training_frame=train,\\n   validation_frame=test, score_interval=2, stopping_rounds=3, stopping_tolerance=0.05, stopping_metric=\"misclassification\")\\n```\\n\\n---\\n\\n**Do you have a tutorial for grid search in Python?**\\n\\nYes, a notebook is available [here](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_eeg_eyestate.ipynb) that demonstrates the use of grid search in Python.\\n\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## R\\n\\n**Which versions of R are compatible with H2O?**\\n\\nCurrently, the only version of R that is known to not work well with H2O is R version 3.1.0 (codename \"Spring Dance\"). If you are using this version, we recommend upgrading the R version before using H2O.\\n\\n\\n\\n---\\n\\n**What R packages are required to use H2O?**\\n\\nThe following packages are required:\\n\\n- `methods`\\n- `statmod`\\n- `stats`\\n- `graphics`\\n- `RCurl`\\n- `jsonlite`\\n- `tools`\\n- `utils`\\n\\nSome of these packages have dependencies; for example, `bitops` is required, but it is a dependency of the `RCurl` package, so `bitops` is automatically included when `RCurl` is installed.\\n\\nIf you are encountering errors related to missing R packages when using H2O, refer to the following list for a complete list of all R packages, including dependencies:\\n\\n\\n\\n<table>\\n    <tr>\\n        <td><code>statmod</code></td>\\n        <td><code>bitops</code></td>\\n        <td><code>RCurl</code></td>\\n        <td><code>jsonlite</code></td>\\n        <td><code>methods</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>stats</code></td>\\n        <td><code>graphics</code></td>\\n        <td><code>tools</code></td>\\n        <td><code>utils</code></td>\\n        <td><code>stringi</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>magrittr</code></td>\\n        <td><code>colorspace</code></td>\\n        <td><code>stringr</code></td>\\n        <td><code>RColorBrewer</code></td>\\n        <td><code>dichromat</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>munsell</code></td>\\n        <td><code>labeling</code></td>\\n        <td><code>plyr</code></td>\\n        <td><code>digest</code></td>\\n        <td><code>gtable</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>reshape2</code></td>\\n        <td><code>scales</code></td>\\n        <td><code>proto</code></td>\\n        <td><code>ggplot2</code></td>\\n        <td><code>h2oEnsemble</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>gtools</code></td>\\n        <td><code>gdata</code></td>\\n        <td><code>caTools</code></td>\\n        <td><code>gplots</code></td>\\n        <td><code>chron</code></td>\\n    </tr>\\n    <tr>\\n        <td><code>ROCR</code></td>\\n        <td><code>data.table</code></td>\\n        <td><code>cvAUC</code></td>\\n        <td></td>\\n        <td></td>\\n   </tr>\\n</table>\\n\\n---\\n\\n**How can I install the H2O R package if I am having permissions problems?**\\n\\nThis issue typically occurs for Linux users when the R software was installed by a root user. For more information, refer to the following [link](https://stat.ethz.ch/R-manual/R-devel/library/base/html/libPaths.html).\\n\\nTo specify the installation location for the R packages, create a file that contains the `R_LIBS_USER` environment variable:\\n\\n`echo R_LIBS_USER=\\\\\"~/.Rlibrary\\\\\" > ~/.Renviron`\\n\\nConfirm the file was created successfully using `cat`:\\n\\n`$ cat ~/.Renviron`\\n\\nYou should see the following output:\\n\\n`R_LIBS_USER=\"~/.Rlibrary\"`\\n\\nCreate a new directory for the environment variable:\\n\\n`$ mkdir ~/.Rlibrary`\\n\\nStart R and enter the following:\\n\\n`.libPaths()`\\n\\nLook for the following output to confirm the changes:\\n\\n```\\n[1] \"<Your home directory>/.Rlibrary\"\\n[2] \"/Library/Frameworks/R.framework/Versions/3.1/Resources/library\"\\n```\\n\\n---\\n\\n**I received the following error message after launching H2O in RStudio and using `h2o.init` - what should I do to resolve this error?**\\n\\n```\\nError in h2o.init() :\\nVersion mismatch! H2O is running version 3.2.0.9 but R package is version 3.2.0.3\\n```\\n\\nThis error is due to a version mismatch between the H2O R package and the running H2O instance. Make sure you are using the latest version of both files by downloading H2O from the [downloads page](http://h2o.ai/download/) and installing the latest version and that you have removed any previous H2O R package versions by running:\\n\\n```\\nif (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload=TRUE) }\\nif (\"h2o\" %in% rownames(installed.packages())) { remove.packages(\"h2o\") }\\n```\\n\\nMake sure to install the dependencies for the H2O R package as well:\\n\\n```\\nif (! (\"methods\" %in% rownames(installed.packages()))) { install.packages(\"methods\") }\\nif (! (\"statmod\" %in% rownames(installed.packages()))) { install.packages(\"statmod\") }\\nif (! (\"stats\" %in% rownames(installed.packages()))) { install.packages(\"stats\") }\\nif (! (\"graphics\" %in% rownames(installed.packages()))) { install.packages(\"graphics\") }\\nif (! (\"RCurl\" %in% rownames(installed.packages()))) { install.packages(\"RCurl\") }\\nif (! (\"jsonlite\" %in% rownames(installed.packages()))) { install.packages(\"jsonlite\") }\\nif (! (\"tools\" %in% rownames(installed.packages()))) { install.packages(\"tools\") }\\nif (! (\"utils\" %in% rownames(installed.packages()))) { install.packages(\"utils\") }\\n```\\n\\n\\nFinally, install the latest version of the H2O package for R:\\n\\n```\\ninstall.packages(\"h2o\", type=\"source\", repos=(c(\"http://h2o-release.s3.amazonaws.com/h2o/master/{{build_number}}/R\")))\\nlibrary(h2o)\\nlocalH2O = h2o.init(nthreads=-1)\\n```\\n\\nIf your R version is older than the H2O R package, upgrade your R version using `update.packages(checkBuilt=TRUE, ask=FALSE)`.\\n\\n---\\n\\n**I received the following error message after trying to run some code - what should I do?**\\n\\n```\\n> fit <- h2o.deeplearning(x=2:4, y=1, training_frame=train_hex)\\n  |=========================================================================================================| 100%\\nError in model$training_metrics$MSE :\\n  $ operator not defined for this S4 class\\nIn addition: Warning message:\\nNot all shim outputs are fully supported, please see ?h2o.shim for more information\\n```\\n\\nRemove the `h2o.shim(enable=TRUE)` line and try running the code again. Note that the `h2o.shim` is only a way to notify users of previous versions of H2O about changes to the H2O R package - it will not revise your code, but provides suggested replacements for deprecated commands and parameters.\\n\\n---\\n\\n**How do I extract the model weights from a model I\\'ve creating using H2O in R? I\\'ve enabled `extract_model_weights_and_biases`, but the output refers to a file I can\\'t open in R.**\\n\\nFor an example of how to extract weights and biases from a model, refer to the following repo location on [GitHub](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_weights_and_biases.R).\\n\\n---\\n\\n**How do I extract the run time of my model as output?**\\n\\n\\nFor the following example:\\n\\n```\\nout.h2o.rf = h2o.randomForest( x=c(\"x1\", \"x2\", \"x3\", \"w\"), y=\"y\", training_frame=h2o.df.train, seed=555, model_id= \"my.model.1st.try.out.h2o.rf\" )\\n```\\n\\nUse `out.h2o.rf@model$run_time` to determine the value of the `run_time` variable.\\n\\n\\n---\\n\\n**What is the best way to do group summarizations? For example, getting sums of specific columns grouped by a categorical column.**\\n\\nWe strongly recommend using `h2o.group_by` for this function instead of `h2o.ddply`, as shown in the following example:\\n\\n```\\nnewframe <- h2o.group_by(h2oframe, by=\"footwear_category\", nrow(\"email_event_click_ct\"), sum(\"email_event_click_ct\"), mean(\"email_event_click_ct\"), sd(\"email_event_click_ct\"), gb.control = list( col.names=c(\"count\", \"total_email_event_click_ct\", \"avg_email_event_click_ct\", \"std_email_event_click_ct\") ) )\\n```\\n\\nUsing `gb.control` is optional; here it is included so the column names are user-configurable.\\n\\nThe `by` option can take a list of columns if you want to group by more than one column to compute the summary as shown in the following example:\\n\\n```\\nnewframe <- h2o.group_by(h2oframe, by=c(\"footwear_category\",\"age_group\"), nrow(\"email_event_click_ct\"), sum(\"email_event_click_ct\"), mean(\"email_event_click_ct\"), sd(\"email_event_click_ct\"), gb.control = list( col.names=c(\"count\", \"total_email_event_click_ct\", \"avg_email_event_click_ct\", \"std_email_event_click_ct\") ) )\\n```\\n\\n\\n\\n---\\n\\n**I\\'m using CentOS and I want to run H2O in R - are there any dependencies I need to install?**\\n\\nYes, make sure to install `libcurl`, which allows H2O to communicate with R. We also recommend disabling SElinux and any firewalls, at least initially until you have confirmed H2O can initialize.\\n\\n---\\n\\n**How do I change variable/header names on an H2O frame in R?**\\n\\nThere are two ways to change header names. To specify the headers during parsing, import the headers in R and then specify the header as the column name when the actual data frame is imported:\\n\\n```\\nheader <- h2o.importFile(path = pathToHeader)\\ndata   <- h2o.importFile(path = pathToData, col.names = header)\\ndata\\n```\\n\\nYou can also use the `names()` function:\\n```\\nheader <- c(\"user\", \"specified\", \"column\", \"names\")\\ndata   <- h2o.importFile(path = pathToData)\\nnames(data) <- header\\n```\\n\\nTo replace specific column names, you can also use a `sub/gsub` in R:\\n\\n```\\nheader <- c(\"user\", \"specified\", \"column\", \"names\")\\n```\\n\\nTo replace \"user\" column with \"computer\"\\n\\n```\\ndata   <- h2o.importFile(path = pathToData)\\nnames(data) <- sub(pattern = \"user\", replacement = \"computer\", x = names(header))\\n```\\n---\\n\\n**My R terminal crashed - how can I re-access my H2O frame?**\\n\\nLaunch H2O and use your web browser to access the web UI, Flow, at `localhost:54321`. Click the **Data** menu, then click **List All Frames**. Copy the frame ID, then run `h2o.ls()` in R to list all the frames, or use the frame ID in the following code (replacing `YOUR_FRAME_ID` with the frame ID):\\n\\n```\\nlibrary(h2o)\\nlocalH2O = h2o.init(ip=\"sri.h2o.ai\", port=54321, startH2O = F, strict_version_check=T)\\ndata_frame <- h2o.getFrame(frame_id = \"YOUR_FRAME_ID\")\\n```\\n---\\n\\n**How do I remove rows containing NAs in an H2OFrame?**\\n\\nTo remove NAs from rows:\\n\\n```\\n  a   b    c    d    e\\n1 0   NA   NA   NA   NA\\n2 0   2    2    2    2\\n3 0   NA   NA   NA   NA\\n4 0   NA   NA   1    2\\n5 0   NA   NA   NA   NA\\n6 0   1    2    3    2\\n```\\n\\nRemoving rows 1, 3, 4, 5 to get:\\n\\n```\\n  a   b    c    d    e\\n2 0   2    2    2    2\\n6 0   1    2    3    2\\n```\\n\\nUse `na.omit(myFrame)`, where `myFrame` represents the name of the frame you are editing.\\n\\n---\\n\\n**I installed H2O in R using OS X and updated all the dependencies, but the following error message displayed: `Error in .h2o.doSafeREST(h2oRestApiVersion = h2oRestApiVersion, Unexpected CURL error: Empty reply from server` - what should I do?**\\n\\n\\nThis error message displays if the `JAVA_HOME` environment variable is not set correctly. The `JAVA_HOME` variable is likely points to Apple Java version 6 instead of Oracle Java version 8.\\n\\nIf you are running OS X 10.7 or earlier, enter the following in Terminal:\\n`export JAVA_HOME=/Library/Internet\\\\ Plug-Ins/JavaAppletPlugin.plugin/Contents/Home`\\n\\nIf you are running OS X 10.8 or later, modify the launchd.plist by entering the following in Terminal:\\n\\n```\\ncat << EOF | sudo tee /Library/LaunchDaemons/setenv.JAVA_HOME.plist\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\\n  <plist version=\"1.0\">\\n  <dict>\\n  <key>Label</key>\\n  <string>setenv.JAVA_HOME</string>\\n  <key>ProgramArguments</key>\\n  <array>\\n    <string>/bin/launchctl</string>\\n    <string>setenv</string>\\n    <string>JAVA_HOME</string>\\n    <string>/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home</string>\\n  </array>\\n  <key>RunAtLoad</key>\\n  <true/>\\n  <key>ServiceIPC</key>\\n  <false/>\\n</dict>\\n</plist>\\nEOF\\n```\\n\\n---\\n\\n<!---\\n\\nin progress - commenting out until complete\\n\\n**How do I extract the variable importance from the output in R?**\\n\\nLaunch R, then enter the following:\\n\\n```\\nlibrary(h2o)\\nh <- h2o.init()\\nas.h2o(iris)\\nas.h2o(testing)\\nm <- h2o.gbm(x=1:4, y=5, data=hex, importance=T)\\n\\nm@model$varimp\\n             Relative importance Scaled.Values Percent.Influence\\nPetal.Width          7.216290000  1.0000000000       51.22833426\\nPetal.Length         6.851120500  0.9493965043       48.63600147\\nSepal.Length         0.013625654  0.0018881799        0.09672831\\nSepal.Width          0.005484723  0.0007600474        0.03893596\\n```\\n\\nThe variable importances are returned as an R data frame and you can extract the names and values of the data frame as follows:\\n\\n```\\nis.data.frame(m@model$varimp)\\n# [1] TRUE\\n\\nnames(m@model$varimp)\\n# [1] \"Relative importance\" \"Scaled.Values\"       \"Percent.Influence\"\\n\\nrownames(m@model$varimp)\\n# [1] \"Petal.Width\"  \"Petal.Length\" \"Sepal.Length\" \"Sepal.Width\"\\n\\nm@model$varimp$\"Relative importance\"\\n# [1] 7.216290000 6.851120500 0.013625654 0.005484723\\n```\\n\\n-->\\n\\n\\n---\\n\\n**How does the `col.names` argument work in `group_by`?**\\n\\nYou need to add the `col.names` inside the `gb.control` list. Refer to the following example:\\n\\n```\\nnewframe <- h2o.group_by(dd, by=\"footwear_category\", nrow(\"email_event_click_ct\"), sum(\"email_event_click_ct\"), mean(\"email_event_click_ct\"),\\n    sd(\"email_event_click_ct\"), gb.control = list( col.names=c(\"count\", \"total_email_event_click_ct\", \"avg_email_event_click_ct\", \"std_email_event_click_ct\") ) )\\nnewframe$avg_email_event_click_ct2 = newframe$total_email_event_click_ct / newframe$count\\n```\\n\\n---\\n\\n**How are the results of `h2o.predict` displayed?**\\n\\n\\nThe order of the rows in the results for `h2o.predict` is the same as the order in which the data was loaded, even if some rows fail (for example, due to missing values or unseen factor levels). To bind a per-row identifier, use `cbind`.\\n\\n---\\n\\n**How do I view all the variable importances for a model?**\\n\\nBy default, H2O returns the top five and lowest five variable importances.\\nTo view all the variable importances, use the following:\\n\\n```\\nmodel <- h2o.getModel(model_id = \"my_H2O_modelID\",conn=localH2O)\\n\\nvarimp<-as.data.frame(h2o.varimp(model))\\n```\\n\\n\\n---\\n\\n**How do I add random noise to a column in an H2O frame?**\\n\\nTo add random noise to a column in an H2O frame, refer to the following example:\\n\\n```\\nh2o.init()\\n\\nfr <- as.h2o(iris)\\n\\n  |======================================================================| 100%\\n\\nrandom_column <- h2o.runif(fr)\\n\\nnew_fr <- h2o.cbind(fr,random_column)\\n\\nnew_fr\\n```\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': '## Sparkling Water\\n\\n**What are the advantages of using Sparkling Water compared with H2O?**\\n\\nSparkling Water contains the same features and functionality as H2O but provides a way to use H2O with [Spark](http://spark.apache.org/), a large-scale cluster framework.\\n\\nSparkling Water is ideal for H2O users who need to manage large clusters for their data processing needs and want to transfer data from Spark to H2O (or vice versa).\\n\\nThere is also a Python interface available to enable access to Sparkling Water directly from PySpark.\\n\\n---\\n\\n**How do I filter an H2OFrame using Sparkling Water?**\\n\\nFiltering columns is easy: just remove the unnecessary columns or create a new H2OFrame from the columns you want to include (`Frame(String[] names, Vec[] vec)`), then make the H2OFrame wrapper around it (`new H2OFrame(frame)`).\\n\\nFiltering rows is a little bit harder. There are two ways:\\n\\n- Create an additional binary vector holding `1/0` for the `in/out` sample (make sure to take this additional vector into account in your computations). This solution is quite cheap, since you do not duplicate data - just create a simple vector in a data walk.\\n\\n  or\\n\\n- Create a new frame with the filtered rows. This is a harder task, since you have to copy data. For reference, look at the #deepSlice call on Frame (`H2OFrame`)\\n\\n\\n---\\n\\n**How can I save and load a K-means model using Sparkling Water?**\\n\\n\\nThe following example code defines the save and load functions explicitly.\\n\\n```\\nimport water._\\nimport _root_.hex._\\nimport java.net.URI\\nimport water.serial.ObjectTreeBinarySerializer\\n// Save H2O model (as binary)\\ndef exportH2OModel(model : Model[_,_,_], destination: URI): URI = {\\n  val modelKey = model._key.asInstanceOf[Key[_ <: Keyed[_ <: Keyed[_ <: AnyRef]]]]\\n  val keysToExport = model.getPublishedKeys()\\n  // Prepend model key\\n  keysToExport.add(0, modelKey)\\n\\n  new ObjectTreeBinarySerializer().save(keysToExport, destination)\\n  destination\\n}\\n\\n// Get model from H2O DKV and Save to disk\\nval gbmModel: _root_.hex.tree.gbm.GBMModel = DKV.getGet(\"model\")\\nexportH2OModel(gbmModel, new File(\"../h2omodel.bin\").toURI)\\n\\n\\n\\ndef loadH2OModel[M <: Model[_, _, _]](source: URI) : M = {\\n    val l = new ObjectTreeBinarySerializer().load(source)\\n    l.get(0).get().asInstanceOf[M]\\n  }\\n// Load H2O model\\ndef loadH2OModel[M <: Model[_, _, _]](source: URI) : M = {\\n    val l = new ObjectTreeBinarySerializer().load(source)\\n    l.get(0).get().asInstanceOf[M]\\n  }\\n\\n// Load model\\nval h2oModel: Model[_, _, _] = loadH2OModel(new File(\"../h2omodel.bin\").toURI)\\n```\\n\\n\\n---\\n\\n**How do I inspect H2O using Flow while a droplet is running?**\\n\\nIf your droplet execution time is very short, add a simple sleep statement to your code:\\n\\n`Thread.sleep(...)`\\n\\n---\\n\\n**How do I change the memory size of the executors in a droplet?**\\n\\nThere are two ways to do this:\\n\\n- Change your default Spark setup in `$SPARK_HOME/conf/spark-defaults.conf`\\n\\n  or\\n\\n- Pass `--conf` via spark-submit when you launch your droplet (e.g., `$SPARK_HOME/bin/spark-submit --conf spark.executor.memory=4g --master $MASTER --class org.my.Droplet $TOPDIR/assembly/build/libs/droplet.jar`\\n\\n---\\n\\n**I received the following error while running Sparkling Water using multiple nodes, but not when using a single node - what should I do?**\\n\\n```\\nonExCompletion for water.parser.ParseDataset$MultiFileParseTask@31cd4150\\nwater.DException$DistributedException: from /10.23.36.177:54321; by class water.parser.ParseDataset$MultiFileParseTask; class water.DException$DistributedException: from /10.23.36.177:54325; by class water.parser.ParseDataset$MultiFileParseTask; class water.DException$DistributedException: from /10.23.36.178:54325; by class water.parser.ParseDataset$MultiFileParseTask$DistributedParse; class java.lang.NullPointerException: null\\n\\tat water.persist.PersistManager.load(PersistManager.java:141)\\n\\tat water.Value.loadPersist(Value.java:226)\\n\\tat water.Value.memOrLoad(Value.java:123)\\n\\tat water.Value.get(Value.java:137)\\n\\tat water.fvec.Vec.chunkForChunkIdx(Vec.java:794)\\n\\tat water.fvec.ByteVec.chunkForChunkIdx(ByteVec.java:18)\\n\\tat water.fvec.ByteVec.chunkForChunkIdx(ByteVec.java:14)\\n\\tat water.MRTask.compute2(MRTask.java:426)\\n\\tat water.MRTask.compute2(MRTask.java:398)\\n```\\n\\nThis error output displays if the input file is not present on all nodes. Because of the way that Sparkling Water distributes data, the input file is required on all nodes (including remote), not just the primary node. Make sure there is a copy of the input file on all the nodes, then try again.\\n\\n---\\n\\n**Are there any drawbacks to using Sparkling Water compared to standalone H2O?**\\n\\nThe version of H2O embedded in Sparkling Water is the same as the standalone version.\\n\\n---\\n\\n**How do I use Sparkling Water from the Spark shell?**\\n\\nThere are two methods:\\n\\n- Use `$SPARK_HOME/bin/spark-shell --packages ai.h2o:sparkling-water-core_2.10:1.3.3`\\n\\n  or\\n\\n- `bin/sparkling-shell`\\n\\nThe software distribution provides example scripts in the `examples/scripts` directory:\\n\\n`bin/sparkling-shell -i examples/scripts/chicagoCrimeSmallShell.script.scala`\\n\\nFor either method, initialize H2O as shown in the following example:\\n\\n```\\nimport org.apache.spark.h2o._\\nval h2oContext = new H2OContext(sc).start()\\n```\\n\\nAfter successfully launching H2O, the following output displays:\\n\\n```\\nSparkling Water Context:\\n * number of executors: 3\\n * list of used executors:\\n  (executorId, host, port)\\n  ------------------------\\n  (1,Michals-MBP.0xdata.loc,54325)\\n  (0,Michals-MBP.0xdata.loc,54321)\\n  (2,Michals-MBP.0xdata.loc,54323)\\n  ------------------------\\n\\n  Open H2O Flow in browser: http://172.16.2.223:54327 (CMD + click in Mac OSX)\\n\\n```\\n\\n---\\n\\n**How do I use H2O with Spark Submit?**\\n\\nSpark Submit is for submitting self-contained applications. For more information, refer to the [Spark documentation](https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications).\\n\\nFirst, initialize H2O:\\n\\n```\\nimport org.apache.spark.h2o._\\nval h2oContext = new H2OContext(sc).start()\\n```\\n\\nThe Sparkling Water distribution provides several examples of self-contained applications built with Sparkling Water. To run the examples:\\n\\n`bin/run-example.sh ChicagoCrimeAppSmall`\\n\\nThe \"magic\" behind `run-example.sh` is a regular Spark Submit:\\n\\n`$SPARK_HOME/bin/spark-submit ChicagoCrimeAppSmall --packages ai.h2o:sparkling-water-core_2.10:1.3.3 --packages ai.h2o:sparkling-water-examples_2.10:1.3.3`\\n\\n---\\n\\n**How do I use Sparkling Water with Databricks cloud?**\\n\\nSparkling Water compatibility with Databricks cloud is still in development.\\n\\n\\n\\n---\\n\\n**How do I develop applications with Sparkling Water?**\\n\\nFor a regular Spark application (a self-contained application as described in the [Spark documentation](https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications)), the app needs to initialize `H2OServices` via `H2OContext`:\\n\\n```\\nimport org.apache.spark.h2o._\\nval h2oContext = new H2OContext(sc).start()\\n```\\n\\nFor more information, refer to the [Sparkling Water development documentation](https://github.com/h2oai/sparkling-water/blob/master/DEVEL.md).\\n\\n---\\n\\n**How do I connect to Sparkling Water from R or Python?**\\n\\nAfter starting `H2OServices` by starting `H2OContext`, point your client to the IP address and port number specified in `H2OContext`.\\n\\n---\\n**I\\'m getting a `java.lang.ArrayIndexOutOfBoundsException` when I try to run Sparkling Water - what do I need to do to resolve this error?**\\n\\nThis error message displays if you have not set up the `H2OContext` before running Sparkling Water. To set up the `H2OContext`:\\n\\n```\\nimport org.apache.spark.h2o._\\nval h2oContext = new H2OContext(sc)\\n```\\nAfter setting up `H2OContext`, try to run Sparkling Water again.\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/FAQ.md', 'section': \"## Tunneling between servers with H2O\\n\\nTo tunnel between servers (for example, due to firewalls):\\n\\n1. Use ssh to log in to the machine where H2O will run.\\n2. Start an instance of H2O by locating the working directory and calling a java command similar to the following example.\\n\\n The port number chosen here is arbitrary; yours may be different.\\n\\n `$ java -jar h2o.jar -port  55599`\\n\\n This returns output similar to the following:\\n\\n\\t```\\n\\tirene@mr-0x3:~/target$ java -jar h2o.jar -port 55599\\n\\t04:48:58.053 main      INFO WATER: ----- H2O started -----\\n\\t04:48:58.055 main      INFO WATER: Build git branch: master\\n\\t04:48:58.055 main      INFO WATER: Build git hash: 64fe68c59ced5875ac6bac26a784ce210ef9f7a0\\n\\t04:48:58.055 main      INFO WATER: Build git describe: 64fe68c\\n\\t04:48:58.055 main      INFO WATER: Build project version: 1.7.0.99999\\n\\t04:48:58.055 main      INFO WATER: Built by: 'Irene'\\n\\t04:48:58.055 main      INFO WATER: Built on: 'Wed Sep  4 07:30:45 PDT 2013'\\n\\t04:48:58.055 main      INFO WATER: Java availableProcessors: 4\\n\\t04:48:58.059 main      INFO WATER: Java heap totalMemory: 0.47 gb\\n\\t04:48:58.059 main      INFO WATER: Java heap maxMemory: 6.96 gb\\n\\t04:48:58.060 main      INFO WATER: ICE root: '/tmp'\\n\\t04:48:58.081 main      INFO WATER: Internal communication uses port: 55600\\n\\t+                                  Listening for HTTP and REST traffic on\\n\\t+                                  http://192.168.1.173:55599/\\n\\t04:48:58.109 main      INFO WATER: H2O cloud name: 'irene'\\n\\t04:48:58.109 main      INFO WATER: (v1.7.0.99999) 'irene' on\\n\\t/192.168.1.173:55599, discovery address /230 .252.255.19:59132\\n\\t04:48:58.111 main      INFO WATER: Cloud of size 1 formed [/192.168.1.173:55599]\\n\\t04:48:58.247 main      INFO WATER: Log dir: '/tmp/h2ologs'\\n\\t```\\n\\n3. Log into the remote machine where the running instance of H2O will be forwarded using a command similar to the following (your specified port numbers and IP address will be different)\\n\\n \\t`ssh -L 55577:localhost:55599 irene@192.168.1.173`\\n\\n4. Check the cluster status.\\n\\nYou are now using H2O from localhost:55577, but the\\ninstance of H2O is running on the remote server (in this\\ncase the server with the ip address 192.168.1.xxx) at port number 55599.\\n\\nTo see this in action note that the web UI is pointed at\\nlocalhost:55577, but that the cluster status shows the cluster running\\non 192.168.1.173:55599\\n\\n\\n---\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## JVM Options\\n\\n- `-version`: Display Java version info. \\n- `-Xmx<Heap Size>`: To set the total heap size for an H2O node, configure the memory allocation option `-Xmx`. By default, this option is set to 1 Gb (`-Xmx1g`). When launching nodes, we recommend allocating a total of four times the memory of your data. \\n\\n> **Note**: Do not try to launch H2O with more memory than you have available.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## H2O Options\\n\\n- `-h` or `-help`: Display this information in the command line output. \\n- `-name <H2OCloudName>`: Assign a name to the H2O instance in the cloud (where `<H2OCloudName>` is the name of the cloud). Nodes with the same cloud name will form an H2O cloud (also known as an H2O cluster). \\n- `-flatfile <FileName>`: Specify a flatfile of IP address for faster cloud formation (where `<FileName>` is the name of the flatfile). \\n- `-ip <IPnodeAddress>`: specifies IP for the machine other than the default `localhost`, for example:\\n    - IPv4: `-ip 178.16.2.223` \\n    - IPv6: `-ip 2001:db8:1234:0:0:0:0:1` (Short version of IPv6 with `::` is not supported.) Note: If you are selecting a link-local address fe80::/96, it is necessary to specify _zone index_ (e.g., `%en0` for `fe80::2acf:e9ff:fe15:e0f3%en0`) to select the right interface.\\n- `-port <#>`: Specify a PORT used for REST API. The communication port will be the port with value +1 higher.\\n- `-baseport` specifies starting port to find a free port for REST API, the internal communication port will be port with value +1 higher.\\n- `-network <ip_address/subnet_mask>`: Specify an IP addresses with a subnet mask. The IP address discovery code binds to the first interface that matches one of the networks in the comma-separated list; to specify an IP address, use `-network`. To specify a range, use a comma to separate the IP addresses: `-network 123.45.67.0/22,123.45.68.0/24`. For example, `10.1.2.0/24` supports 256 possibilities. IPv4 and IPv6 addresses are supported. \\n    - IPv4: `-network 178.0.0.0/8`\\n    - IPv6: `-network 2001:db8:1234:0:0:0:0:0/48` (short version of IPv6 with `::` is not supported.)\\n- `-ice_root <fileSystemPath>`: Specify a directory for H2O to spill temporary data to disk (where `<fileSystemPath>` is the file path). \\n- `-flow_dir <server-side or HDFS directory>`: Specify a directory for saved flows. The default is `/Users/h2o-<H2OUserName>/h2oflows` (where `<H2OUserName>` is your user name). \\n- `-nthreads <#ofThreads>`: Specify the maximum number of threads in the low-priority batch work queue (where `<#ofThreads>` is the number of threads). The default is 99. \\n- `-client`: Launch H2O node in client mode. This is used mostly for running Sparkling Water.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## H2O Internal Communication\\n\\nBy default, H2O selects the IP and PORT for internal communication automatically using the following this process (if not specified):\\n\\n  1. Retrieve a list of available interfaces (which are up).\\n  2. Sort them with \"bond\" interfaces put on the top.\\n  3. For each interface, extract associated IPs.\\n  4. Pick only reachable IPs (that filter IPs provided by interfaces such as awdl):\\n\\n   - If there is a site IP, use it.\\n   - Otherwise, if there is a link local IP, use it. (For IPv6, the link IP 0xfe80/96 is associated with each interface.)\\n   - Or finally, try to find a local IP. (Use loopback or try to use Google DNS to find IP for this machine.)\\n\\n>***Notes***: The port is selected by looking for a free port starting with port 54322.\\n\\n> The IP, PORT and network selection can be changed by the following options:\\n\\n  - `-ip` \\n  - `network`\\n  - `-port`\\n  - `-baseport`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## Cloud Formation Behavior\\n\\nNew H2O nodes join to form a cloud during launch. After a job has started on the cloud, it  prevents new members from joining. \\n\\n- To start an H2O node with 4GB of memory and a default cloud name: \\n  `java -Xmx4g -jar h2o.jar`\\n\\n- To start an H2O node with 6GB of memory and a specific cloud name: \\n  `java -Xmx6g -jar h2o.jar -name MyCloud`\\n\\n- To start an H2O cloud with three 2GB nodes using the default cloud names: \\n\\n  ```\\n  java -Xmx2g -jar h2o.jar &\\n  java -Xmx2g -jar h2o.jar &\\n  java -Xmx2g -jar h2o.jar &\\n  ```\\n\\nWait for the `INFO: Registered: # schemas in: #mS` output before entering the above command again to add another node (the number for # will vary).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## Clouding Up: Cluster Creation\\n\\nH2O provides two modes for cluster creation:\\n\\n  1. Multicast based\\n  2. Flatfile based\\n\\n### Multicast \\nIn this mode, H2O is using IP multicast to announce existence of H2O nodes. Each node selects the same multicast group and port based on specified shared cloud name (see `-name` option). For example, for IPv4/PORT a generated multicast group is `228.246.114.236:58614` (for cloud name `michal`), \\nfor IPv6/PORT a generated multicast group is `ff05:0:3ff6:72ec:0:0:3ff6:72ec:58614` (for cloud name `michal` and link-local address which enforce link-local scope).\\n\\nFor IPv6 the scope of multicast address is enforced by a selected node IP. For example, if IP the selection process selects link-local address, then the scope of multicast will be link-local. This can be modified by specifying JVM variable `sys.ai.h2o.network.ipv6.scope` which enforces addressing scope use in multicast group address (for example, `-Dsys.ai.h2o.network.ipv6.scope=0x0005000000000000` enforces the site local scope. For more details please consult the\\nclass `water.util.NetworkUtils`).\\n\\nFor more information about scopes, see <a href=\"http://www.tcpipguide.com/free/diagrams/ipv6scope.png\" target=\"_blank\">http://www.tcpipguide.com/free/diagrams/ipv6scope.png</a>. \\n\\n### Flatfile\\nThe flatfile describes a topology of a H2O cluster. The flatfile definition is passed via `-flatfile` option. It needs to be passed at each node in the cluster, but definition does not be the same at each node. However, transitive closure of all definitions should contains all nodes. For example, for the following definition\\n\\nNodes    | nodeA | nodeB | nodeC \\n---------|-------|-------|-------\\nFlatfile | A,B   | A, B  | B, C  \\n\\nThe resulting cluster will be formed by nodes A, B, C. The node A transitively sees node C via node B flatfile definition, and vice versa.\\n\\nThe flatfile contains a list of nodes in the form `IP:PORT` (each node on separated line, everything prefixed by `#` is ignored) that are going to compose a resulting cluster. For example:\\n\\n**IPv4**:\\n\\n```\\n# run two nodes on 108\\n10.10.65.108:54322\\n10.10.65.108:54325\\n```\\n**IPv6**:\\n\\n```\\n0:0:0:0:0:0:0:1:54321\\n0:0:0:0:0:0:0:1:54323\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## Web Server\\n\\nThe web server IP is auto-configured in the same way as internal communication IP, nevertheless the created socket listens on all available interfaces. A specific API can be specified with the `-web_ip` option.\\n\\n### Options\\n  - `-web_ip`: specifies IP for web server to expose REST API'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': '## Dual Stacks\\n\\nDual stack machines support IPv4 and IPv6 network stacks.\\nRight now, H2O always prefer IPV4, however the preference can be changed via JVM system options `java.net.preferIPv4Addresses` and `java.net.preferIPv6Addresses`.\\n\\nFor example:\\n\\n  - `-Djava.net.preferIPv6Addresses=true -Djava.net.preferIPv4Addresses=true` - H2O will try to select IPv4\\n\\n  - `-Djava.net.preferIPv6Addresses=true -Djava.net.preferIPv4Addresses=false` - H2O will try to select IPv6'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevCmdLine.md', 'section': \"## Run to embed into iframe\\n\\nBy default you can't embed web into `iframe`, `object` or similar way because\\n`X-Frame-Options` HTTP header is `deny` by default. To change it to `sameorigin`\\nstart H2O-3 with `java -Dsys.ai.h2o.enable.xframe.samerorigin=true -jar build/h2o.jar`\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevDocker.md', 'section': '## Walkthrough\\n\\n**Prerequisites**\\n\\n  * Linux kernel version 3.8+\\n\\n  or\\n\\n  * Mac OS X 10.6+\\n  * VirtualBox\\n  * Latest version of Docker is installed and configured\\n  * Docker daemon is running - enter all commands below in the Docker daemon window\\n  * Using `User` directory (not `root`)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevDocker.md', 'section': '## Notes\\n\\n- Older Linux kernel versions are known to cause kernel panics that break Docker; there are ways around it, but these should be attempted at your own risk. To check the version of your kernel, run `uname -r` at the command prompt. The following walkthrough has been tested on a Mac OS X 10.10.1.\\n- The Dockerfile always pulls the latest H2O release. \\n- The Docker image only needs to be built once. \\n\\n**Step 1 - Install and Launch Docker**\\n\\nDepending on your OS, select the appropriate installation method:\\n\\n  * [Mac Installation](https://docs.docker.com/installation/mac/#installation)\\n  * [Ubuntu Installation](https://docs.docker.com/installation/ubuntulinux/)\\n  * [Other OS Installations](https://docs.docker.com/installation/)\\n\\n\\n**Step 2 - Create or Download Dockerfile**\\n\\n  >**Note**: If the following commands do not work, prepend with `sudo`. \\n\\nCreate a folder on the Host OS to host your Dockerfile by running:\\n\\n```\\nmkdir -p /data/h2o-{{branch_name}}\\n```\\n\\nNext, either download or create a Dockerfile, which is a build recipe that builds the container.\\n\\nDownload and use our [Dockerfile template](https://github.com/h2oai/h2o-3/blob/master/Dockerfile) by running:\\n\\n```\\ncd /data/h2o-{{branch_name}}\\nwget https://raw.githubusercontent.com/h2oai/h2o-3/master/Dockerfile\\n```\\n\\nThe Dockerfile:\\n\\n  * obtains and updates the base image (Ubuntu 14.04) \\n  * installs Java 7\\n  * obtains and downloads the H2O build from H2O\\'s S3 repository\\n  * exposes ports 54321 and 54322 in preparation for launching H2O on those ports\\n\\n**Step 3 - Build Docker image from Dockerfile**\\n\\nFrom the /data/h2o-{{branch_name}} directory, run:\\n\\n```\\ndocker build -t \"h2oai/{{branch_name}}:v5\" .\\n```\\n\\n>**Note**: `v5` represents the current version number. \\n\\nBecause it assembles all the necessary parts for the image, this process can take a few minutes.\\n\\n**Step 4 - Run Docker Build**\\n\\nOn a Mac, use the argument *-p 54321:54321* to expressly map the port 54321. This is not necessary on Linux.\\n\\n```\\ndocker run -ti -p 54321:54321 h2o.ai/{{branch_name}}:v5 /bin/bash\\n```\\n\\n>**Note**: `v5` represents the version number. \\n\\n**Step 5 - Launch H2O**\\n\\nNavigate to the `/opt` directory and launch H2O. Change the value of `-Xmx` to the amount of memory you want to allocate to the H2O instance. By default, H2O launches on port 54321.\\n\\n```\\ncd /opt\\njava -Xmx1g -jar h2o.jar\\n```\\n\\n**Step 6 - Access H2O from the web browser or R**\\n\\n  * *On Linux*: After H2O launches, copy and paste the IP address and port of the H2O instance into the address bar of your browser. In the following example, the IP is `172.17.0.5:54321`.\\n\\n```\\n03:58:25.963 main      INFO WATER: Cloud of size 1 formed [/172.17.0.5:54321 (00:00:00.000)]\\n```\\n\\n  * *On OSX*: Locate the IP address of the Docker\\'s network (`192.168.59.103` in the following examples) that bridges to your Host OS by opening a new Terminal window (not a bash for your container) and running ```boot2docker ip```.\\n\\n```\\n$ boot2docker ip\\n192.168.59.103\\n```\\n\\nYou can also view the IP address (`192.168.99.100` in the example below) by scrolling to the top of the Docker daemon window: \\n\\n```\\n\\n                        ##         .\\n                  ## ## ##        ==\\n               ## ## ## ## ##    ===\\n           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\\\___/ ===\\n      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~\\n           \\\\______ o           __/\\n             \\\\    \\\\         __/\\n              \\\\____\\\\_______/\\n\\n\\ndocker is configured to use the default machine with IP 192.168.99.100\\nFor help getting started, check out the docs at https://docs.docker.com\\n\\n\\n```\\n\\nAfter obtaining the IP address, point your [browser](localhost:54321) to the specified ip address and port. In R, you can access the instance by installing the latest version of the H2O R package and running:\\n\\n``` \\nlibrary(h2o)\\ndockerH2O <- h2o.init(ip = \"192.168.59.103\", port = 54321)\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevLogs.md', 'section': '## Accessing Logs\\n\\nDepending on whether you are using Hadoop with H2O and whether the job is currently running, there are different ways of obtaining the logs for H2O. \\n\\nCopy and email the logs to support@h2o.ai or submit them to h2ostream@googlegroups.com with a brief description of your Hadoop environment, including the Hadoop distribution and version.\\n\\n### Without Running Jobs\\n\\n- If you are using Hadoop and the job is not running, view the logs by using the `yarn logs -applicationId` command. When you start an H2O instance, the complete command displays in the output: \\n\\n```\\n\\tjessica@mr-0x8:~/h2o-3.1.0.3008-cdh5.2$ hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output hdfsOutputDirName\\nDetermining driver host interface for mapper->driver callback...\\n    [Possible callback IP address: 172.16.2.178]\\n    [Possible callback IP address: 127.0.0.1]\\nUsing mapper->driver callback IP address and port: 172.16.2.178:52030\\n(You can override these with -driverif and -driverport.)\\nMemory Settings:\\n    mapreduce.map.java.opts:     -Xms1g -Xmx1g -Dlog4j.defaultInitOverride=true\\n    Extra memory percent:        10\\n    mapreduce.map.memory.mb:     1126\\n15/05/06 17:11:50 INFO client.RMProxy: Connecting to ResourceManager at mr-0x10.0xdata.loc/172.16.2.180:8032\\n15/05/06 17:11:52 INFO mapreduce.JobSubmitter: number of splits:1\\n15/05/06 17:11:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1430127035640_0075\\n15/05/06 17:11:52 INFO impl.YarnClientImpl: Submitted application application_1430127035640_0075\\n15/05/06 17:11:52 INFO mapreduce.Job: The url to track the job: http://mr-0x10.0xdata.loc:8088/proxy/application_1430127035640_0075/\\nJob name \\'H2O_29570\\' submitted\\nJobTracker job ID is \\'job_1430127035640_0075\\'\\nFor YARN users, logs command is \\'yarn logs -applicationId application_1430127035640_0075\\'\\nWaiting for H2O cluster to come up...\\n\\n```\\n\\n\\nIn the above example, the command is specified in the next to last line (`For YARN users, logs command is...`). The command is unique for each instance. In Terminal, enter `yarn logs -applicationId application_<UniqueID>` to view the logs (where `<UniqueID>` is the number specified in the next to last line of the output that displayed when you created the cluster). \\n\\t\\n---\\n\\nUse YARN to obtain the `stdout` and `stderr` logs that are used for troubleshooting. To learn how to access YARN based on management software, version, and job status, see [Accessing YARN](#AccessYARN). \\n\\n1. Click the **Applications** link to view all jobs, then click the **History** link for the job.\\n \\n  ![YARN - History](images/YARN_AllApps_History.png)\\n\\n2. Click the **logs** link. \\n\\t\\n\\t![YARN - History](images/YARN_History_Logs.png)\\n\\t\\n3. \\tCopy the information that displays and send it in an email to support@h2o.ai. \\n\\t\\n\\t![YARN - History](images/YARN_History_Logs2.png)\\n \\n---\\n\\n### With Running Jobs\\n\\n\\nIf you are using Hadoop and the job is still running: \\n\\n- Use YARN to obtain the `stdout` and `stderr` logs that are used for troubleshooting. To learn how to access YARN based on management software, version, and job status, see [Accessing YARN](#AccessYARN).\\n\\n 1. Click the **Applications** link to view all jobs, then click the **ApplicationMaster** link for the job. \\n\\t\\n\\t ![YARN - Application Master](images/YARN_AllApps_AppMaster.png)\\n\\n 2. Select the job from the list of active jobs. \\n\\t\\n\\t ![YARN - Application Master](images/YARN_AppMaster_Job.png)\\n\\t\\n 3. Click the **logs** link. \\n\\t\\n\\t  ![YARN - Application Master](images/YARN_AppMaster_Logs.png)\\n\\t\\n 4. Send the contents of the displayed files to support@h2o.ai. \\n\\t\\n\\t ![YARN - Application Master](images/YARN_AppMaster_Logs2.png)\\n\\t\\n---\\n\\n- Go to the H2O web UI and select **Admin** > **View Log**. To filter the results select a node or log file type from the drop-down menus. To download the logs, click the **Download Logs** button. \\n\\n When you view the log, the output displays the location of log directory after `Log dir:` (as shown in the last line in the following example): \\n\\n```\\n05-06 17:12:15.610 172.16.2.179:54321    26336  main      INFO: ----- H2O started  -----\\n05-06 17:12:15.731 172.16.2.179:54321    26336  main      INFO: Build git branch: master\\n05-06 17:12:15.731 172.16.2.179:54321    26336  main      INFO: Build git hash: 41d039196088df081ad77610d3e2d6550868f11b\\n05-06 17:12:15.731 172.16.2.179:54321    26336  main      INFO: Build git describe: jenkins-master-1187\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Build project version: 0.3.0.1187\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Built by: \\'jenkins\\'\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Built on: \\'2015-05-05 23:31:12\\'\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java availableProcessors: 8\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java heap totalMemory: 982.0 MB\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java heap maxMemory: 982.0 MB\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java version: Java 1.7.0_80 (from Oracle Corporation)\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: OS   version: Linux 3.13.0-51-generic (amd64)\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Machine physical memory: 31.30 GB\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: X-h2o-cluster-id: 1430957535344\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Possible IP Address: virbr0 (virbr0), 192.168.122.1\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Possible IP Address: br0 (br0), 172.16.2.179\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Possible IP Address: lo (lo), 127.0.0.1\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Multiple local IPs detected:\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO:   /192.168.122.1  /172.16.2.179\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Attempting to determine correct address...\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Using /172.16.2.179\\n05-06 17:12:15.734 172.16.2.179:54321    26336  main      INFO: Internal communication uses port: 54322\\n05-06 17:12:15.734 172.16.2.179:54321    26336  main      INFO: Listening for HTTP and REST traffic on  http://172.16.2.179:54321/\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO: H2O cloud name: \\'H2O_29570\\' on /172.16.2.179:54321, discovery address /237.61.246.13:60733\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO:   1. Open a terminal and run \\'ssh -L 55555:localhost:54321 yarn@172.16.2.179\\'\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO:   2. Point your browser to http://localhost:55555\\n05-06 17:12:15.979 172.16.2.179:54321    26336  main      INFO: Log dir: \\'/home2/yarn/nm/usercache/jessica/appcache/application_1430127035640_0075/h2ologs\\'\\n``` \\n\\n---\\n\\n-  In Terminal, enter `cd /tmp/h2o-<UserName>/h2ologs` (where `<UserName>` is your computer user name), then enter `ls -l` to view a list of the log files. The `httpd` log contains the request/response status of all REST API transactions. \\n  The rest of the logs use the format `h2o_\\\\<IPaddress>\\\\_<Port>-<LogLevel>-<LogLevelName>.log`, where `<IPaddress>` is the bind address of the H2O instance, `<Port>` is the port number, `<LogLevel>` is the numerical log level (1-6, with 6 as the highest severity level), and `<LogLevelName>` is the name of the log level (trace, debug, info, warn, error, or fatal). \\n\\n---\\n\\n- Download the logs using R. In R, enter the command `h2o.downloadAllLogs(filename = \"logs.zip\")` (where `filename` is the specified filename for the logs).\\n\\n---\\n\\n<a name=\"AccessYARN\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevLogs.md', 'section': '## Accessing YARN\\n\\nMethods for accessing YARN vary depending on the default management software and version, as well as job status. \\n\\n\\n---\\n \\n### Cloudera 5 & 5.2\\n\\n\\n1. In Cloudera Manager, click the **YARN** link in the cluster section.\\n\\n  ![Cloudera Manager](images/Logs_cloudera5_1.png)\\n  \\n2. In the Quick Links section, select **ResourceManager Web UI** if the job is running or select **HistoryServer Web UI** if the job is not running. \\n\\n ![Cloudera Manager](images/Logs_cloudera5_2.png)\\n\\n---\\n \\n### Ambari\\n\\n\\n1. From the Ambari Dashboard, select **YARN**. \\n\\n  ![Ambari](images/Logs_ambari1.png)\\n\\n2. From the Quick Links drop-down menu, select **ResourceManager UI**.   \\n\\n  ![Ambari](images/Logs_ambari2.png)\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevLogs.md', 'section': '## For Non-Hadoop Users\\n\\n### Without Current Jobs\\n\\n\\nIf you are not using Hadoop and the job is not running: \\n\\n- In Terminal, enter `cd /tmp/h2o-<UserName>/h2ologs` (where `<UserName>` is your computer user name), then enter `ls -l` to view a list of the log files. The `httpd` log contains the request/response status of all REST API transactions. \\n The rest of the logs use the format `h2o_\\\\<IPaddress>\\\\_<Port>-<LogLevel>-<LogLevelName>.log`, where `<IPaddress>` is the bind address of the H2O instance, `<Port>` is the port number, `<LogLevel>` is the numerical log level (1-6, with 6 as the highest severity level), and `<LogLevelName>` is the name of the log level (trace, debug, info, warn, error, or fatal). \\n\\n---\\n\\n### With Current Jobs\\n\\nIf you are not using Hadoop and the job is still running: \\n\\n- Go to the H2O web UI and select **Admin** > **Inspect Log** or go to http://localhost:54321/LogView.html.\\n\\t![Logs](images/Logsdownload.png)\\n\\n  To download the logs, click the **Download Logs** button. \\n\\n When you view the log, the output displays the location of log directory after `Log dir:` (as shown in the last line in the following example):\\n\\n```\\n05-06 17:12:15.610 172.16.2.179:54321    26336  main      INFO: ----- H2O started  -----\\n05-06 17:12:15.731 172.16.2.179:54321    26336  main      INFO: Build git branch: master\\n05-06 17:12:15.731 172.16.2.179:54321    26336  main      INFO: Build git hash: 41d039196088df081ad77610d3e2d6550868f11b\\n05-06 17:12:15.731 172.16.2.179:54321    26336  main      INFO: Build git describe: jenkins-master-1187\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Build project version: 0.3.0.1187\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Built by: \\'jenkins\\'\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Built on: \\'2015-05-05 23:31:12\\'\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java availableProcessors: 8\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java heap totalMemory: 982.0 MB\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java heap maxMemory: 982.0 MB\\n05-06 17:12:15.732 172.16.2.179:54321    26336  main      INFO: Java version: Java 1.7.0_80 (from Oracle Corporation)\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: OS   version: Linux 3.13.0-51-generic (amd64)\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Machine physical memory: 31.30 GB\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: X-h2o-cluster-id: 1430957535344\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Possible IP Address: virbr0 (virbr0), 192.168.122.1\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Possible IP Address: br0 (br0), 172.16.2.179\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Possible IP Address: lo (lo), 127.0.0.1\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Multiple local IPs detected:\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO:   /192.168.122.1  /172.16.2.179\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Attempting to determine correct address...\\n05-06 17:12:15.733 172.16.2.179:54321    26336  main      INFO: Using /172.16.2.179\\n05-06 17:12:15.734 172.16.2.179:54321    26336  main      INFO: Internal communication uses port: 54322\\n05-06 17:12:15.734 172.16.2.179:54321    26336  main      INFO: Listening for HTTP and REST traffic on  http://172.16.2.179:54321/\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO: H2O cloud name: \\'H2O_29570\\' on /172.16.2.179:54321, discovery address /237.61.246.13:60733\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO:   1. Open a terminal and run \\'ssh -L 55555:localhost:54321 yarn@172.16.2.179\\'\\n05-06 17:12:15.744 172.16.2.179:54321    26336  main      INFO:   2. Point your browser to http://localhost:55555\\n05-06 17:12:15.979 172.16.2.179:54321    26336  main      INFO: Log dir: \\'/home2/yarn/nm/usercache/jessica/appcache/application_1430127035640_0075/h2ologs\\'\\n```\\n\\n---\\n\\n- In Terminal, enter `cd /tmp/h2o-<UserName>/h2ologs` (where `<UserName>` is your computer user name), then enter `ls -l` to view a list of the log files. The `httpd` log contains the request/response status of all REST API transactions. \\n The rest of the logs use the format `h2o_\\\\<IPaddress>\\\\_<Port>-<LogLevel>-<LogLevelName>.log`, where `<IPaddress>` is the bind address of the H2O instance, `<Port>` is the port number, `<LogLevel>` is the numerical log level (1-6, with 6 as the highest severity level), and `<LogLevelName>` is the name of the log level (trace, debug, info, warn, error, or fatal). \\n\\n---\\n\\n- To view the REST API logs from R: \\n\\n  1. In R, enter `h2o.startLogging()`. The output displays the location of the REST API logs: \\n\\n\\t\\t```\\t\\t\\n\\t\\t> h2o.startLogging()\\n\\t\\tAppending REST API transactions to log file /var/folders/ylcq5nhky53hjcl9wrqxt39kz80000gn/T//RtmpE7X8Yv/rest.log \\n\\t\\t```\\n\\t\\t\\n  2. Copy the displayed file path. \\n\\t  In Terminal, enter `less` and paste the file path. \\n  3. Press Enter. A time-stamped log of all REST API transactions displays. \\n\\n```\\t\\t\\n\\t\\t------------------------------------------------------------\\n\\n\\t\\tTime:     2015-01-06 15:46:11.083\\n\\t\\n\\t\\tGET       http://172.16.2.20:54321/3/Cloud.json\\n\\t\\tpostBody: \\n\\n\\t\\tcurlError:         FALSE\\n\\t\\tcurlErrorMessage:  \\n\\t\\thttpStatusCode:    200\\n\\t\\thttpStatusMessage: OK\\n\\t\\tmillis:            3\\n\\n\\t\\t{\"__meta\":{\"schema_version\":\\t1,\"schema_name\":\"CloudV1\",\"schema_type\":\"Iced\"},\"version\":\"0.1.17.1009\",\"cloud_name\":...[truncated]}\\n\\t\\t-------------------------------------------------------------\\n```\\t\\n\\t\\n---\\n\\n- Download the logs using R. In R, enter the command `h2o.downloadAllLogs(filename = \"logs.zip\")` (where `filename` is the specified filename for the logs).\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevS3Creds.md', 'section': '## On EC2\\n\\n>Tested on Redhat AMI, Amazon Linux AMI, and Ubuntu AMI\\n\\nTo use the Amazon Web Services (AWS) S3 storage solution, you will need to pass your S3 access credentials to H2O. This will allow you to access your data on S3 when importing data frames with path prefixes `s3n://...`.\\n\\nFor security reasons, we recommend writing a script to read the access credentials that are stored in a separate file. This will not only keep your credentials from propagating to other locations, but it will also make it easier to change the credential information later.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevS3Creds.md', 'section': '## Standalone Instance\\n\\nWhen running H2O in standalone mode using the simple Java launch command, we can pass in the S3 credentials in three ways. \\n\\n- You can pass in credentials in standalone mode by creating a `core-site.xml` file and pass it in with the flag `-hdfs_config`. For an example `core-site.xml` file, refer to [Core-site.xml](#Example). \\n\\n 1. Edit the properties in the core-site.xml file to include your Access Key ID and Access Key as shown in the following example:\\n   \\n\\t   ```\\n\\t   <property>\\n\\t      <name>fs.s3n.awsAccessKeyId</name>\\n\\t      <value>[AWS SECRET KEY]</value>\\n\\t    </property>\\n\\n\\t    <property>\\n\\t      <name>fs.s3n.awsSecretAccessKey</name>\\n\\t      <value>[AWS SECRET ACCESS KEY]</value>\\n\\t    </property>\\n\\t    ```\\n 2. Launch with the configuration file `core-site.xml` by entering the following in the command line:\\n\\n\\t\\tjava -jar h2o.jar -hdfs_config core-site.xml\\n\\n 3. Set the credentials dynamically before accessing the bucket. (where ``AWS_ACCESS_KEY`` represents your user name, and ``AWS_SECRET_KEY`` represents your password).\\n\\n - To set the credentials dynamically using R API:\\n\\n```python\\nh2o.set_s3_credentials(\"AWS_ACCESS_KEY\", \"AWS_SECRET_KEY\")\\nh2o.importFile(path = \"s3://bucket/path/to/file.csv\")\\n```\\n\\n  - To set the credentials dynamically using Python API:\\n\\n```python\\n\\nfrom h2o.persist import set_s3_credentials\\nset_s3_credentials(\"AWS_ACCESS_KEY\", \"AWS_SECRET_KEY\")\\nh2o.import_file(path = \"s3://bucket/path/to/file.csv\")\\n```\\n\\nPassing credentials in the URL, e.g. `h2o.importFile(path = \"s3://<AWS_ACCESS_KEY>:<AWS_SECRET_KEY>@bucket/path/to/file.csv\")` is considered security risk and is deprecated.\\n  \\n---\\n<a name=\"Multi\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevS3Creds.md', 'section': '## Multi-Node Instance\\n\\n>[Python](http://www.amazon.com/Python-and-AWS-Cookbook-ebook/dp/B005ZTO0UW/ref=sr_1_1?ie=UTF8&qid=1379879111&sr=8-1&keywords=python+aws) and the [`boto`](http://boto.readthedocs.org/en/latest/) Python library are required to launch a multi-node instance of H2O on EC2. Confirm these dependencies are installed before proceeding. \\n\\nFor more information, refer to the [H2O EC2 repo](https://github.com/h2oai/h2o-3/tree/master/ec2). \\n\\nBuild a cluster of EC2 instances by running the following commands on the host that can access the nodes using a public DNS name. \\n\\n1. Edit `h2o-cluster-launch-instances.py` to include your SSH key name and security group name, as well as any other environment-specific variables. \\n        \\n   ```\\t\\t\\n   ./h2o-cluster-launch-instances.py\\n   ./h2o-cluster-distribute-h2o.sh  \\n   ```\\t\\t\\n \\n    --OR--\\n\\t\\t\\n   ```\\t\\t  \\n   ./h2o-cluster-launch-instances.py\\n   ./h2o-cluster-download-h2o.sh\\n   ```\\t\\t\\n\\n   **Note**: The second method may be faster than the first, since download pulls from S3. \\n\\n2. Distribute the credentials using `./h2o-cluster-distribute-aws-credentials.sh`. \\n  >**Note**: If you are running H2O using an IAM role, it is not necessary to distribute the AWS credentials to all the nodes in the cluster. The latest version of H2O can access the temporary access key. \\n\\n  >**Caution**: Distributing the AWS credentials copies the Amazon `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to the instances to enable S3 and S3N access. Use caution when adding your security keys to the cloud. \\n\\n3. Start H2O by launching one H2O node per EC2 instance: \\n  `./h2o-cluster-start-h2o.sh`\\n  \\n  Wait 60 seconds after entering the command before entering it on the next node. \\n  \\n4. In your internet browser, substitute any of the public DNS node addresses for `IP_ADDRESS` in the following example:\\n  `http://IP_ADDRESS:54321`\\n\\n  - To start H2O: `./h2o-cluster-start-h2o.sh`\\n  - To stop H2O: `./h2o-cluster-stop-h2o.sh`\\n  - To shut down the cluster, use your [Amazon AWS console](http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_TerminateJobFlow.html) to shut down the cluster manually. \\n\\n >**Note**: To successfully import data, the data must reside in the same location on all nodes. \\n\\n---\\n\\n\\n<a name=\"Example\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevS3Creds.md', 'section': '## Core-site.xml Example\\n\\nThe following is an example core-site.xml file: \\n\\n\\n    <?xml version=\"1.0\"?>\\n    <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\\n\\n    <!-- Put site-specific property overrides in this file. -->\\n\\n    <configuration>\\n    \\n        <!--\\n        <property>\\n        <name>fs.default.name</name>\\n        <value>s3n://<your s3 bucket></value>\\n        </property>\\n        -->\\n    \\n        <property>\\n            <name>fs.s3n.awsAccessKeyId</name>\\n            <value>insert access key here</value>\\n        </property>\\n    \\n        <property>\\n            <name>fs.s3n.awsSecretAccessKey</name>\\n            <value>insert secret key here</value>\\n        </property>\\n        </configuration> \\n    \\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-DevS3Creds.md', 'section': '## Launching H2O\\n\\n**Note**: Before launching H2O on an EC2 cluster, verify that ports `54321` and `54322` are both accessible by TCP. \\n\\n### Selecting the Operating System and Virtualization Type\\n\\nSelect your operating system and the virtualization type of the prebuilt AMI on Amazon. If you are using Windows, you will need to use a hardware-assisted virtual machine (HVM). If you are using Linux, you can choose between para-virtualization (PV) and HVM. These selections determine the type of instances you can launch. \\n\\n  ![EC2 Systems](images/ec2_system.png)\\n\\nFor more information about virtualization types, refer to [Amazon](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html).\\n\\n--- \\n\\n### Configuring the Instance\\n\\n1. Select the IAM role and policy to use to launch the instance. H2O detects the temporary access keys associated with the instance, so you don\\'t need to copy your AWS credentials to the instances. \\n\\n  ![EC2 Configuration](images/ec2_config.png)\\n\\n2. When launching the instance, select an accessible key pair. \\n\\n  ![EC2 Key Pair](images/ec2_key_pair.png)\\n\\n---\\n\\n#### (Windows Users) Tunneling into the Instance\\n\\nFor Windows users that do not have the ability to use `ssh` from the terminal, either download Cygwin or a Git Bash that has the capability to run `ssh`:\\n\\n`ssh -i amy_account.pem ec2-user@54.165.25.98`\\n\\nOtherwise, download PuTTY and follow these instructions:\\n\\n1. Launch the PuTTY Key Generator. \\n2. Load your downloaded AWS pem key file. \\n   **Note:** To see the file, change the browser file type to \"All\". \\n3. Save the private key as a .ppk file. \\n\\n  ![Private Key](images/ec2_putty_key.png)\\n\\n4. Launch the PuTTY client. \\n5. In the *Session* section, enter the host name or IP address. For Ubuntu users, the default host name is `ubuntu@<ip-address>`. For Linux users, the default host name is `ec2-user@<ip-address>`.  \\n\\n  ![Configuring Session](images/ec2_putty_connect_1.png)\\n\\n6. Select *SSH*, then *Auth* in the sidebar, and click the **Browse** button to select the private key file for authentication. \\n\\n  ![Configuring SSH](images/ec2_putty_connect_2.png)\\n7. Start a new session and click the **Yes** button to confirm caching of the server\\'s rsa2 key fingerprint and continue connecting. \\n\\n  ![PuTTY Alert](images/ec2_putty_alert.png)\\n\\n---\\n\\n### Downloading Java and H2O\\n\\n\\n1. Download [Java](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements) (JDK 1.8 or later) if it is not already available on the instance. \\n\\n2. To download H2O, run the `wget` command with the link to the zip file available on our [website](http://h2o.ai/download/) by copying the link associated with the **Download** button for the selected H2O build. \\n\\t\\n\\t\\twget http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/index.html\\n\\t\\tunzip h2o-{{project_version}}.zip\\n\\t\\tcd h2o-{{project_version}}\\n\\t\\tjava -Xmx4g -jar h2o.jar\\n\\n3. From your browser, navigate to `<Private_IP_Address>:54321` or `<Public_DNS>:54321` to use H2O\\'s web interface.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-Networking.md', 'section': '## Internal Communication\\n\\nBy default, H2O selects the IP and PORT for internal communication automatically using the following this process (if not specified):\\n\\n  1. Retrieve a list of available interfaces (which are up).\\n  2. Sort them with \"bond\" interfaces put on the top.\\n  3. For each interface, extract associated IPs.\\n  4. Pick only reachable IPs (that filter IPs provided by interfaces such as awdl):\\n\\n   - If there is a site IP, use it.\\n   - Otherwise, if there is a link local IP, use it. (For IPv6, the link IP 0xfe80/96 is associated with each interface.)\\n   - Or finally, try to find a local IP. (Use loopback or try to use Google DNS to find IP for this machine.)\\n\\nThe port is selected by looking for a free port starting with port 54322. This can be modified using the `-port` and `-baseport` options.\\n\\n### Options\\nThe IP, PORT and network selection can be changed by the following options\\n\\n  - `-ip` specifies IP for the machine, for example:\\n    - IPv4: `-ip 178.16.2.223` \\n    - IPv6: `-ip 2001:db8:1234:0:0:0:0:1` (short version of IPv6 with `::` is not supported). Note: if you are selecting a link-local address fe80::/96 it is necessary to specify _zone index_ - e.g., `%en0` for `fe80::2acf:e9ff:fe15:e0f3%en0` to select the right interface.\\n  - `network` limits selection of IP to a specified subnet\\n    - IPv4: `-network 178.0.0.0/8`\\n    - IPv6: `-network 2001:db8:1234:0:0:0:0:0/48` (short version of IPv6 with `::` is not supported)\\n  - `-port` specifies PORT used for REST API, the communication port will be the port with value +1 higher\\n  - `-baseport` specifies starting port to find a free port for REST API, the internal communication port will be port with value +1 higher'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-Networking.md', 'section': '## Clouding Up: Cluster Creation\\n\\nH2O provides two modes for cluster creation:\\n\\n  1. Multicast based\\n  2. Flatfile based\\n\\n### Multicast \\nIn this mode, H2O is using IP multicast to announce existence of H2O nodes. Each node selects the same multicast group and port based on specified shared cloud name (see `-name` option). For example, for IPv4/PORT a generated multicast group is `228.246.114.236:58614` (for cloud name `michal`), \\nfor IPv6/PORT a generated multicast group is `ff05:0:3ff6:72ec:0:0:3ff6:72ec:58614` (for cloud name `michal` and link-local address which enforce link-local scope).\\n\\nFor IPv6 the scope of multicast address is enforced by a selected node IP. For example, if IP the selection process selects link-local address, then the scope of multicast will be link-local. This can be modified by specifying JVM variable `sys.ai.h2o.network.ipv6.scope` which enforces addressing scope use in multicast group address (for example, `-Dsys.ai.h2o.network.ipv6.scope=0x0005000000000000` enforces the site local scope. For more details please consult the\\nclass `water.util.NetworkUtils`).\\n\\nFor more information about scopes, see <a href=\"http://www.tcpipguide.com/free/diagrams/ipv6scope.png\" target=\"_blank\">http://www.tcpipguide.com/free/diagrams/ipv6scope.png</a>. \\n\\n### Flatfile\\nThe flatfile describes a topology of a H2O cluster. The flatfile definition is passed via `-flatfile` option. \\nIt needs to be passed at each node in the cluster, but definition does not be the same at each node. However, transitive closure of all definitions should contains all nodes. For example, for the following definition\\n\\nNodes    | nodeA | nodeB | nodeC \\n---------|-------|-------|-------\\nFlatfile | A,B   | A, B  | B, C  \\n\\nThe resulting cluster will be formed by nodes A, B, C. The node A transitively sees node C via node B flatfile definition, and vice versa.\\n\\nThe flatfile contains a list of nodes in the form `IP:PORT` (each node on separated line, everythin prefixed by `#` is ignored) which are going to compose a resulting cluster.\\nFor example:\\n\\n**IPv4**:\\n\\n```\\n# run two nodes on 108\\n10.10.65.108:54322\\n10.10.65.108:54325\\n```\\n**IPv6**:\\n\\n```\\n0:0:0:0:0:0:0:1:54321\\n0:0:0:0:0:0:0:1:54323\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-Networking.md', 'section': '## Web Server\\n\\nThe web server IP is auto-configured in the same way as internal communication IP, nevertheless the created socket listens on all available interfaces. A specific API can be specified by the `-web_ip` option.\\n\\n### Options\\n  - `-web_ip`: specifies IP for web server to expose REST API'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/H2O-Networking.md', 'section': '## Dual Stacks\\n\\nDual stack machines support IPv4 and IPv6 network stacks.\\nRight now, H2O always prefer IPV4, however the preference can be changed via JVM system options `java.net.preferIPv4Addresses` and `java.net.preferIPv6Addresses` options.\\n\\nFor example:\\n\\n  - `-Djava.net.preferIPv6Addresses=true -Djava.net.preferIPv4Addresses=true` - H2O will try to select IPv4\\n\\n  - `-Djava.net.preferIPv6Addresses=true -Djava.net.preferIPv4Addresses=false` - H2O will try to select IPv6'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/Videos.md', 'section': '## H2O Quick Start with Flow\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_HVx9Jqr34Q?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0\" frameborder=\"0\" allowfullscreen></iframe>\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/Videos.md', 'section': '## H2O Quick Start with Python\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/K8J3dPBEz1s?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0\" frameborder=\"0\" allowfullscreen></iframe>\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/Videos.md', 'section': '## H2O Quick Start on Hadoop\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/B1ax_k_sSoY?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0\" frameborder=\"0\" allowfullscreen></iframe>\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/Videos.md', 'section': '## H2O Quick Start with Sparkling Water\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ypka6eX1G14?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0\" frameborder=\"0\" allowfullscreen></iframe>\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/Videos.md', 'section': '## H2O Quick Start with R\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zzV1kTCnmR0?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0\" frameborder=\"0\" allowfullscreen></iframe>\\n\\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': '## Using H2O with YARN\\n\\nWhen you launch H2O on Hadoop using the `hadoop jar` command, YARN allocates the necessary resources to launch the requested number of nodes. H2O launches as a MapReduce (V2) task, where each mapper is an H2O node of the specified size. \\n\\n`hadoop jar h2odriver.jar -nodes 1 -mapperXmx 6g -output hdfsOutputDirName`\\n\\nOccasionally, YARN may reject a job request. This usually occurs because either there is not enough memory to launch the job or because of an incorrect configuration. \\n\\nIf YARN rejects the job request, try launching the job with less memory to see if that is the cause of the failure. Specify smaller values for `-mapperXmx` (we recommend a minimum of `2g`) and `-nodes` (start with `1`) to confirm that H2O can launch successfully.\\n\\nTo resolve configuration issues, adjust the maximum memory that YARN will allow when launching each mapper. If the cluster manager settings are configured for the default maximum memory size but the memory required for the request exceeds that amount, YARN will not launch and H2O will time out. If you are using the default configuration, change the configuration settings in your cluster manager to specify memory allocation when launching mapper tasks. To calculate the amount of memory required for a successful launch, use the following formula: \\n\\n>YARN container size (`mapreduce.map.memory.mb`) = `-mapperXmx` value + (`-mapperXmx` * `-extramempercent` [default is 10%])\\n\\nThe `mapreduce.map.memory.mb` value must be less than the YARN memory configuration values for the launch to succeed.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': '## Configuring YARN\\n\\n**For Cloudera, configure the settings in Cloudera Manager. Depending on how the cluster is configured, you may need to change the settings for more than one role group.**\\n\\t\\n1. Click **Configuration** and enter the following search term in quotes: **yarn.nodemanager.resource.memory-mb**.\\n\\n2. Enter the amount of memory (in GB) to allocate in the **Value** field. If more than one group is listed, change the values for all listed groups.\\n\\t\\n\\t![Cloudera Configuration](images/TroubleshootingHadoopClouderayarnnodemgr.png)\\n\\t\\n3. Click the **Save Changes** button in the upper-right corner. \\n4. Enter the following search term in quotes: **yarn.scheduler.maximum-allocation-mb**\\n5. Change the value, click the **Save Changes** button in the upper-right corner, and redeploy.\\n\\t\\n ![Cloudera Configuration](images/TroubleshootingHadoopClouderayarnscheduler.png)\\n\\t\\t\\n\\t\\n**For Hortonworks,** [configure](http://docs.hortonworks.com/HDPDocuments/Ambari-1.6.0.0/bk_Monitoring_Hadoop_Book/content/monitor-chap2-3-3_2x.html) **the settings in Ambari.**\\n\\n1. Select **YARN**, then click the **Configs** tab. \\n2. Select the group. \\n3. In the **Node Manager** section, enter the amount of memory (in MB) to allocate in the **yarn.nodemanager.resource.memory-mb** entry field. \\n\\t\\n ![Ambari Configuration](images/TroubleshootingHadoopAmbariNodeMgr.png)\\n\\t  \\n4. In the **Scheduler** section, enter the amount of memory (in MB)to allocate in the **yarn.scheduler.maximum-allocation-mb** entry field. \\n\\t\\n ![Ambari Configuration](images/TroubleshootingHadoopAmbariyarnscheduler.png)\\n\\n5. \\tClick the **Save** button at the bottom of the page and redeploy the cluster. \\n\\t\\n**For MapR:**\\n\\n1. Edit the **yarn-site.xml** file for the node running the ResourceManager. \\n2. Change the values for the `yarn.nodemanager.resource.memory-mb` and `yarn.scheduler.maximum-allocation-mb` properties.\\n3. Restart the ResourceManager and redeploy the cluster. \\n\\t\\n\\nTo verify the values were changed, check the values for the following properties:\\n \\t\\n\\t - <name>yarn.nodemanager.resource.memory-mb</name>\\n\\t - <name>yarn.scheduler.maximum-allocation-mb</name>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': '## Limiting CPU Usage\\n\\nTo limit the number of CPUs used by H2O, use the `-nthreads` option and specify the maximum number of CPUs for a single container to use. The following example limits the number of CPUs to four:  \\n\\n`hadoop jar h2odriver.jar -nthreads 4 -nodes 1 -mapperXmx 6g -output hdfsOutputDirName`\\n \\n**Note**: The default is 4*the number of CPUs. You must specify at least four CPUs; otherwise, the following error message displays: \\n`ERROR: nthreads invalid (must be >= 4)`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': '## Specifying Queues\\n\\nIf you do not specify a queue when launching H2O, H2O jobs are submitted to the default queue. Jobs submitted to the default queue have a lower priority than jobs submitted to a specific queue. \\n\\nTo specify a queue with Hadoop, enter `-Dmapreduce.job.queuename=<my-h2o-queue>` \\n\\n(where `<my-h2o-queue>` is the name of the queue) when launching Hadoop. \\n\\nFor example, \\n\\n`hadoop jar h2odriver.jar -Dmapreduce.job.queuename=<my-h2o-queue> -nodes <num-nodes> -mapperXmx 6g -output hdfsOutputDirName`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': \"## Specifying Output Directories\\n\\nTo prevent overwriting multiple users' files, each job must have a unique output directory name. Change the `-output hdfsOutputDir` argument (where `hdfsOutputDir` is the name of the directory. \\n\\nAlternatively, you can delete the directory (manually or by using a script) instead of creating a unique directory each time you launch H2O.\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': '## Customizing YARN\\n\\nMost of the configurable YARN variables are stored in `yarn-site.xml`. To prevent settings from being overridden, you can mark a config as \"final.\" If you change any values in `yarn-site.xml`, you must restart YARN to confirm the changes.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/howto/YARN_BP.md', 'section': '## Accessing Logs\\n\\nTo learn how to access logs in YARN, refer to [Downloading Logs](http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-docs/index.html#Downloading%20Logs).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GainsLift.md', 'section': '## Requirements:\\n\\n- The training frame dataset must contain actual binary class labels.\\n- The prediction column used as the response must contain probabilities.\\n- For GLM, the visualization displays only when using `nfolds` (for example, `nfolds=2`).\\n- The model type cannot be K-means or PCA.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GainsLift.md', 'section': '## Creating a Gains/Lift table\\n\\n1. Import a binary classification dataset. \\n2. Select the model type (DL, DRF, GBM, GLM, or Naive Bayes)\\n3. Select the imported dataset from the drop-down *training_frame* list. \\n4. Select a binomial column from the drop-down *response_column* list. \\n5. Click the **Build Model** button, then click the **View** button after the model is complete. \\n6. Scroll down to view the Gains/Lift chart (as shown in the example screenshot below). \\n\\n  ![Gains/Lift chart](images/GainsLift.png)\\n\\nA table that lists the following values is also provided: \\n\\n  - lower threshold\\n  - cumulative data fraction\\n  - response rate\\n  - cumulative response rate\\n  - capture rate\\n  - cumulative capture rate\\n  - lift\\n  - cumulative lift\\n  - gain \\n  - cumulative gain\\n\\n  ![Gains/Lift table](images/GainsLiftTable.png)\\n\\n\\nThe quantiles column defines the group for the row. The response rate column lists the likelihood of response, the lift column lists the lift rate, and the cumulative lift column provides the percentage of increase in response based on the lift.\\n\\nAll rows containing NA values in either the label (response) or the prediction probability are ignored.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## REST\\n\\nThe current implementation of the grid search REST API exposes the following endpoints:\\n\\n- `GET /<version>/Grids`: List available grids, with optional parameters to sort the list by model metric such as MSE\\n- `GET /<version>/Grids/<grid_id>`: Return specified grid\\n- `POST /<version>/Grid/<algo_name>`: Start a new grid search\\n\\t- `<algo_name>`: Supported algorithm values are `{glm, gbm, drf, kmeans, deeplearning}`\\n\\nEndpoints accept model-specific parameters (e.g., [GBMParametersV3](https://github.com/h2oai/h2o-3/blob/master/h2o-algos/src/main/java/hex/schemas/GBMV3.java) and an additional parameter called `hyper_parameters` which contains a dictionary of the hyper parameters which will be searched. In this dictionary an array of values is specified for each searched hyperparameter.\\n\\n```json\\n{\\n  \"ntrees\":[1,5],\\n  \"learn_rate\":[0.1,0.01]\\n}\\n```\\n\\nAn optional `search_criteria` dictionary specifies options for controlling more advanced search strategies.  Currently, full `Cartesian` is the default.  `RandomDiscrete` allows a random search over the hyperparameter space, with three ways of specifying when to stop the search: max number of models, max time, and metric-based early stopping (e.g., stop if MSE hasn\\'t improved by 0.0001 over the 5 best models). An example is:\\n\\n```json\\n{\\n  \"strategy\": \"RandomDiscrete\",\\n  \"max_runtime_secs\": 600,\\n  \"max_models\": 100,\\n  \"stopping_metric\": \"AUTO\",\\n  \"stopping_tolerance\": 0.00001,\\n  \"stopping_rounds\": 5,\\n  \"seed\": 123456\\n}\\n```\\n\\nWith grid search, each model is built sequentially, allowing users to view each model as it is built.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Supported Grid Search Hyperparameters\\n\\nThe following hyperparameters are supported by grid search.\\n\\n### Common Hyperparameters Supported by Grid Search\\n\\n- `validation_frame`\\n- `response_column`\\n- `weights_column`\\n- `offset_column`\\n- `fold_column`\\n- `fold_assignment`\\n- `stopping_rounds`\\n- `max_runtime_secs`\\n- `stopping_metric`\\n- `stopping_tolerance`\\n\\n### Shared Tree Hyperparameters Supported by Grid Search\\n\\n>***Note***: The Shared Tree hyperparameters apply to DRF and GBM.\\n\\n- `balance_classes`\\n- `class_sampling_factors`\\n- `max_after_balance_size`\\n- `ntrees`\\n- `max_depth`\\n- `min_rows`\\n- `nbins`\\n- `nbins_top_level`\\n- `nbins_cats`\\n- `r2_stopping`\\n- `seed`\\n- `build_tree_one_node`\\n- `sample_rate`\\n- `sample_rate_per_class`\\n- `col_sample_rate_per_tree`\\n- `col_sample_rate_change_per_level`\\n- `score_tree_interval`\\n- `min_split_improvement`\\n- `histogram_type`\\n\\n### DRF Hyperparameters Supported by Grid Search\\n\\n- `mtries`\\n\\n### GBM Hyperparameters Supported by Grid Search\\n\\n- `learn_rate`\\n- `learn_rate_annealing`\\n- `distribution`\\n- `quantile_alpha`\\n- `tweedie_power`\\n- `col_sample_rate`\\n- `max_abs_leafnode_pred`\\n\\n### K-Means Hyperparameters Supported by Grid Search\\n\\n- `max_iterations`\\n- `standardize`\\n- `seed`\\n- `init`\\n\\n### GLRM Hyperparameters Supported by Grid Search\\n\\n- `transform`\\n- `k`\\n- `loss`\\n- `multi_loss`\\n- `loss_by_col`\\n- `period`\\n- `regularization_x`\\n- `regularization_y`\\n- `gamma_x`\\n- `gamma_y`\\n- `max_iterations`\\n- `max_updates`\\n- `missing_values_handling`\\n- `init_step_size`\\n- `min_step_size`\\n- `seed`\\n- `init`\\n- `svd_method`\\n\\n### Nave Bayes Hyperparameters Supported by Grid Search\\n\\n- `laplace`\\n- `min_sdev`\\n- `eps_sdev`\\n- `min_prob`\\n- `eps_prob`\\n- `compute_metrics`\\n- `seed`\\n\\n### PCA Hyperparameters Supported by Grid Search\\n\\n- `transform`\\n- `k`\\n- `max_iterations`\\n\\n### Deep Learning Hyperparameters Supported by Grid Search\\n\\n- `balance_classes`\\n- `class_sampling_factors`\\n- `max_after_balance_size`\\n- `max_confusion_matrix_size`\\n- `overwrite_with_best_model`\\n- `use_all_factor_levels`\\n- `standardize`\\n- `activation`\\n- `hidden`\\n- `epochs`\\n- `train_samples_per_iteration`\\n- `target_ratio_comm_to_comp`\\n- `seed`\\n- `adaptive_rate`\\n- `rho`\\n- `epsilon`\\n- `rate`\\n- `rate_annealing`\\n- `rate_decay`\\n- `momentum_start`\\n- `momentum_ramp`\\n- `momentum_stable`\\n- `nesterov_accelerated_gradient`\\n- `input_dropout_ratio`\\n- `hidden_dropout_ratios`\\n- `l1`\\n- `l2`\\n- `max_w2`\\n- `initial_weight_distribution`\\n- `initial_weight_scale`\\n- `initial_weights`\\n- `initial_biases`\\n- `loss`\\n- `distribution`\\n- `tweedie_power`\\n- `quantile_alpha`\\n- `score_interval`\\n- `score_training_samples`\\n- `score_validation_samples`\\n- `score_duty_cycle`\\n- `classification_stop`\\n- `regression_stop`\\n- `quiet_mode`\\n- `score_validation_sampling`\\n- `variable_importances`\\n- `fast_mode`\\n- `force_load_balance`\\n- `replicate_training_data`\\n- `single_node_mode`\\n- `shuffle_training_data`\\n- `missing_values_handling`\\n- `sparse`\\n- `col_major`\\n- `average_activation`\\n- `sparsity_beta`\\n- `max_categorical_features`\\n- `reproducible`\\n- `elastic_averaging`\\n- `elastic_averaging_moving_rate`\\n- `elastic_averaging_regularization`\\n\\n### Aggregator Hyperparameters Supported by Grid Search\\n\\n- `radius_scale`\\n- `transform`\\n- `pca_method`\\n- `k`\\n- `max_iterations`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Example\\n\\nInvoke a new GBM model grid search by POSTing the following request to `/99/Grid/gbm`:\\n\\n```json\\nparms:{hyper_parameters={\"ntrees\":[1,5],\"learn_rate\":[0.1,0.01]}, training_frame=\"filefd41fe7ac0b_csv_1.hex_2\", grid_id=\"gbm_grid_search\", response_column=\"Species\"\", ignored_columns=[\"\"]}\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Grid Search in R\\n\\nGrid search in R provides the following capabilities:\\n\\n- `H2OGrid class`: Represents the results of the grid search\\n- `h2o.getGrid(<grid_id>, sort_by, decreasing)`: Display the specified grid\\n- `h2o.grid`: Start a new grid search parameterized by\\n\\t- model builder name (e.g., `gbm`)\\n\\t- model parameters (e.g., `ntrees=100`)\\n\\t- `hyper_parameters` attribute for passing a list of hyper parameters (e.g., `list(ntrees=c(1,100), learn_rate=c(0.1,0.001))`)\\n\\t- `search_criteria` optional attribute for specifying more a advanced search strategy\\n\\n### Example\\n\\n```r\\nntrees_opts = c(1, 5)\\nlearn_rate_opts = c(0.1, 0.01)\\nhyper_parameters = list(ntrees = ntrees_opts, learn_rate = learn_rate_opts)\\ngrid <- h2o.grid(\"gbm\", grid_id=\"gbm_grid_test\", x=1:4, y=5, training_frame=iris.hex, hyper_params = hyper_parameters)\\ngrid_models <- lapply(grid@model_ids, function(mid) {\\n    model = h2o.getModel(mid)\\n  })\\n```\\n\\n### Random Hyper-Parameter Grid Search Example\\n\\n```r\\n# The following two commands remove any previously installed H2O packages for R.\\nif (\"package:h2o\" %in% search()) { detach(\"package:h2o\", unload=TRUE) }\\nif (\"h2o\" %in% rownames(installed.packages())) { remove.packages(\"h2o\") }\\n\\n# Next, we download packages that H2O depends on.\\npkgs <- c(\"methods\",\"statmod\",\"stats\",\"graphics\",\"RCurl\",\"jsonlite\",\"tools\",\"utils\")\\nfor (pkg in pkgs) {\\n  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }\\n}\\n\\n# Now we download, install and initialize the H2O package for R.\\ninstall.packages(\"h2o\", type=\"source\", repos=(c(\"http://h2o-release.s3.amazonaws.com/h2o/rel-tukey/7/R\")))\\n\\n\\nlibrary(h2o)\\nh2o.init(nthreads=-1)\\ntrain <- h2o.importFile(\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/arrhythmia.csv.gz\")\\ndim(train)\\nresponse <- 1\\npredictors <- c(2:ncol(train))\\n\\nsplits<-h2o.splitFrame(train, 0.9, destination_frames = c(\"trainSplit\",\"validSplit\"), seed = 123456)\\ntrainSplit <- splits[[1]]\\nvalidSplit <- splits[[2]]'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Hyper-Parameter Search'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Construct a large Cartesian hyper-parameter space\\n\\nntrees_opts <- c(10000) ## early stopping will stop earlier\\nmax_depth_opts <- seq(1,20)\\nmin_rows_opts <- c(1,5,10,20,50,100)\\nlearn_rate_opts <- seq(0.001,0.01,0.001)\\nsample_rate_opts <- seq(0.3,1,0.05)\\ncol_sample_rate_opts <- seq(0.3,1,0.05)\\ncol_sample_rate_per_tree_opts = seq(0.3,1,0.05)\\n#nbins_cats_opts = seq(100,10000,100) ## no categorical features in this dataset\\n\\nhyper_params = list( ntrees = ntrees_opts,\\n                     max_depth = max_depth_opts,\\n                     min_rows = min_rows_opts,\\n                     learn_rate = learn_rate_opts,\\n                     sample_rate = sample_rate_opts,\\n                     col_sample_rate = col_sample_rate_opts,\\n                     col_sample_rate_per_tree = col_sample_rate_per_tree_opts\\n                     #,nbins_cats = nbins_cats_opts\\n)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Search a random subset of these hyper-parmameters (max runtime and max models are enforced, and the search will stop after we don\\'t improve much over the best 5 random models)\\n\\nsearch_criteria = list(strategy = \"RandomDiscrete\", max_runtime_secs = 600, max_models = 100, stopping_metric = \"AUTO\", stopping_tolerance = 0.00001, stopping_rounds = 5, seed = 123456)\\n\\ngbm.grid <- h2o.grid(\"gbm\",\\n                     grid_id = \"mygrid\",\\n                     x = predictors,\\n                     y = response,\\n\\n                     # faster to use a 80/20 split\\n                     training_frame = trainSplit,\\n                     validation_frame = validSplit,\\n                     nfolds = 0,\\n\\n                     # alternatively, use N-fold cross-validation\\n                     #training_frame = train,\\n                     #nfolds = 5,\\n\\n                     distribution=\"gaussian\", ## best for MSE loss, but can try other distributions (\"laplace\", \"quantile\")\\n\\n                     ## stop as soon as mse doesn\\'t improve by more than 0.1% on the validation set,\\n                     ## for 2 consecutive scoring events\\n                     stopping_rounds = 2,\\n                     stopping_tolerance = 1e-3,\\n                     stopping_metric = \"MSE\",\\n\\n                     score_tree_interval = 100, ## how often to score (affects early stopping)\\n                     seed = 123456, ## seed to control the sampling of the Cartesian hyper-parameter space\\n                     hyper_params = hyper_params,\\n                     search_criteria = search_criteria)\\n\\ngbm.sorted.grid <- h2o.getGrid(grid_id = \"mygrid\", sort_by = \"mse\")\\nprint(gbm.sorted.grid)\\n\\nbest_model <- h2o.getModel(gbm.sorted.grid@model_ids[[1]])\\nsummary(best_model)\\n\\nscoring_history <- as.data.frame(best_model@model$scoring_history)\\nplot(scoring_history$number_of_trees, scoring_history$training_MSE, type=\"p\") #training mse\\npoints(scoring_history$number_of_trees, scoring_history$validation_MSE, type=\"l\") #validation mse'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## get the actual number of trees\\n\\nntrees <- best_model@model$model_summary$number_of_trees\\nprint(ntrees)\\n```\\n\\nFor more information, refer to the [R grid search code](https://github.com/h2oai/h2o-3/blob/master/h2o-r/h2o-package/R/grid.R) and [runit_GBMGrid_airlines.R](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/gbm/runit_GBMGrid_airlines.R).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Grid Search in Python\\n\\n- Class is `H2OGridSearch`\\n- `<grid_name>.show()`: Display a list of models (including model IDs, hyperparameters, and MSE) explored by grid search  (where `<grid_name>` is an instance of an `H2OGridSearch` class)\\n- `grid_search = H2OGridSearch(<model_type), hyper_params=hyper_parameters)`: Start a new grid search parameterized by:\\n\\t- `model_type` is the type of H2O estimator model with its unchanged parameters\\n\\t- `hyper_params` in Python is a dictionary of string parameters (keys) and a list of values to be explored by grid search (values) (e.g., `{\\'ntrees\\':[1,100], \\'learn_rate\\':[0.1, 0.001]}`\\n\\t- `search_criteria` optional dictionary for specifying more a advanced search strategy\\n\\n\\n\\n### Example\\n\\n\\n```python\\n  hyper_parameters = {\\'ntrees\\':[10,50], \\'max_depth\\':[20,10]}\\n  grid_search = H2OGridSearch(H2ORandomForestEstimator, hyper_params=hyper_parameters)\\n  grid_search.train(x=[\"x1\", \"x2\"], y=\"y\", training_frame=train)\\n  grid_search.show()\\n\\n```\\n\\nFor more information, refer to the [Python grid search code](https://github.com/h2oai/h2o-3/blob/master/h2o-py/h2o/grid/grid_search.py) and [pyunit_benign_glm_grid.py](https://github.com/h2oai/h2o-3/blob/master/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Grid Search Java API\\n\\nEach parameter exposed by the schema can specify if it is supported by grid search by specifying the attribute `gridable=true` in the schema @API annotation. In any case, the Java API does not restrict the parameters supported by grid search.\\n\\nThere are two core entities: `Grid` and `GridSearch`. `GridSeach` is a job-building `Grid` object and is defined by the user\\'s model factory and the [hyperspace walk strategy](https://en.wikipedia.org/wiki/Hyperparameter_optimization).  The model factory must be defined for each supported model type (DRF, GBM, DL, and K-means). The hyperspace walk strategy specifies how the user-defined space of hyper parameters is traversed. The space definition is not limited. For each point in hyperspace, model parameters of the specified type are produced.\\n\\nThe implementation supports a simple Cartesian grid search as well as random search with several different stopping criteria. Grid build triggers a new model builder job for each hyperspace point returned by the walk strategy. If the model builder job fails, the resulting model is ignored; however, it can still be tracked in the job list, and errors are returned in the grid build result.\\n\\nModel builder jobs are run serially in sequential order. More advanced job scheduling schemes are under development.  Note that in cases of true big data sequential scheduling will yield the highest performance.  It is only with a large cluster and small data that concurrent scheduling will improve performance.\\n\\nThe grid object contains the results of the grid search: a list of model keys produced by the grid search as well as any errors, and a table of metrics for each succesful model. The grid object publishes a simple API to get the models.\\n\\nLaunch the grid search by specifying:\\n\\n- the common model hyperparameters (parameter values which will be common across all models in the search)\\n- the search hyperparameters (a map `<parameterName, listOfValues>` that defines the parameter spaces to traverse)\\n- optionally, search criteria (an instance of `HyperSpaceSearchCriteria`)\\n\\nThe Java API can grid search any parameters defined in the model parameter\\'s class (e.g., `GBMParameters`). Paramters that are appropriate for gridding are marked by the @API parameter, but this is not enforced by the framework.\\n\\nAdditional methods are available in the model builder to support creation of model parameters and configuration. This eliminates the requirement of the previous implementation where each gridable value was represented as a `double`. This also allows users to specify different building strategies for model parameters. For example, the REST layer uses a builder that validates parameters against the model parameter\\'s schema, where the Java API uses a simple reflective builder. Additional reflections support is provided by PojoUtils (methods `setField`, `getFieldValue`).\\n\\n### Example\\n\\n```java\\nHashMap<String, Object[]> hyperParms = new HashMap<>();\\nhyperParms.put(\"_ntrees\", new Integer[]{1, 2});\\nhyperParms.put(\"_distribution\", new DistributionFamily[]{DistributionFamily.multinomial});\\nhyperParms.put(\"_max_depth\", new Integer[]{1, 2, 5});\\nhyperParms.put(\"_learn_rate\", new Float[]{0.01f, 0.1f, 0.3f});\\n\\n// Setup common model parameters\\nGBMModel.GBMParameters params = new GBMModel.GBMParameters();\\nparams._train = fr._key;\\nparams._response_column = \"cylinders\";\\n// Trigger new grid search job, block for results and get the resulting grid object\\nGridSearch gs =\\n GridSearch.startGridSearch(params, hyperParms, GBM_MODEL_FACTORY, new HyperSpaceSearchCriteria.CartesianSearchCriteria());\\nGrid grid = (Grid) gs.get();\\n```\\n\\n### Exposing grid search end-point for a new algorithm\\n\\nIn the following example, the PCA algorithm has been implemented and we would like to expose the algorithm via REST API. The following aspects are assumed:\\n\\n  - The PCA model builder is called `PCA`\\n  - The PCA parameters are defined in a class called `PCAParameters`\\n  - The PCA parameters schema is called `PCAParametersV3`\\n\\nTo add support for PCA grid search:\\n\\n1. Add the PCA model build factory into the `hex.grid.ModelFactories` class:\\n\\n\\t  ```java\\n\\t  class ModelFactories {\\n\\t    /* ... */\\n\\t    public static ModelFactory<PCAModel.PCAParameters>\\n\\t      PCA_MODEL_FACTORY =\\n\\t      new ModelFactory<PCAModel.PCAParameters>() {\\n\\t        @Override\\n\\t        public String getModelName() {\\n\\t          return \"PCA\";\\n\\t        }\\n\\n\\t        @Override\\n\\t        public ModelBuilder buildModel(PCAModel.PCAParameters params) {\\n\\t          return new PCA(params);\\n\\t        }\\n\\t      };\\n\\t  }\\n\\t  ```\\n\\n2. Add the PCA REST end-point schema:\\n\\n\\t  ```java\\n\\t  public class PCAGridSearchV99 extends GridSearchSchema<PCAGridSearchHandler.PCAGrid,\\n\\t    PCAGridSearchV99,\\n\\t    PCAModel.PCAParameters,\\n\\t    PCAV3.PCAParametersV3> {\\n\\n\\t  }\\n\\t  ```\\n\\n3. Add the PCA REST end-point handler:\\n\\n\\t  ```java\\n\\t  public class PCAGridSearchHandler\\n\\t    extends GridSearchHandler<PCAGridSearchHandler.PCAGrid,\\n\\t    PCAGridSearchV99,\\n\\t    PCAModel.PCAParameters,\\n\\t    PCAV3.PCAParametersV3> {\\n\\n\\t    public PCAGridSearchV99 train(int version, PCAGridSearchV99 gridSearchSchema) {\\n\\t      return super.do_train(version, gridSearchSchema);\\n\\t    }\\n\\n\\t    @Override\\n\\t    protected ModelFactory<PCAModel.PCAParameters> getModelFactory() {\\n\\t      return ModelFactories.PCA_MODEL_FACTORY;\\n\\t    }\\n\\n\\t    @Deprecated\\n\\t    public static class PCAGrid extends Grid<PCAModel.PCAParameters> {\\n\\n\\t      public PCAGrid() {\\n\\t        super(null, null, null, null);\\n\\t      }\\n\\t    }\\n\\t  }\\n\\t  ```\\n\\n4. Register the REST end-point in the register factory `hex.api.Register`:\\n\\n\\t  ```java\\n\\t  public class Register extends AbstractRegister {\\n\\t      @Override\\n\\t      public void register() {\\n\\t          // ...\\n\\t          H2O.registerPOST(\"/99/Grid/pca\", PCAGridSearchHandler.class, \"train\", \"Run grid search for PCA model.\");\\n\\t          // ...\\n\\t       }\\n\\t  }\\n\\t  ```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': \"## Grid Testing\\n\\nThe current test infrastructure includes:\\n\\n**R Tests**\\n\\n- GBM grids using wine, airlines, and iris datasets verify the consistency of results\\n- DL grid using the `hidden` parameter verifying the passing of structured parameters as a list of values\\n- Minor R testing support verifying equality of the model's parameters against  a given list of hyper parameters.\\n\\n**JUnit Test**\\n\\n- Basic tests verifying consistency of the results for DRF, GBM, and KMeans\\n- JUnit test assertions for grid results\\n\\nThere are tests for the `RandomDiscrete` search criteria in [runit_GBMGrid_airlines.R](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/gbm/runit_GBMGrid_airlines.R) and [pyunit_benign_glm_grid.py](https://github.com/h2oai/h2o-3/blob/master/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py).\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Caveats/In Progress\\n\\n- Currently, the schema system requires specific classes instead of parameterized classes. For example, the schema definition `Grid<GBMParameters>` is not supported unless your define the class `GBMGrid extends Grid<GBMParameters>`.\\n- Grid Job scheduler is sequential only; schedulers for concurrent builds are under development.  Note that in cases of true big data sequential scheduling will yield the highest performance.  It is only with a large cluster and small data that concurrent scheduling will improve performance.\\n- The model builder job and grid jobs are not associated.\\n- There is no way to list the hyper space parameters that caused a model builder job failure.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/GridSearch.md', 'section': '## Documentation\\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-core/javadoc/index.html\" target=\"_blank\">H2O Core Java Developer Documentation</a>: The definitive Java API guide for the core components of H2O.\\n\\n- <a href=\"http://h2o-release.s3.amazonaws.com/h2o/{{branch_name}}/{{build_number}}/docs-website/h2o-algos/javadoc/index.html\" target=\"_blank\">H2O Algos Java Developer Documentation</a>: The definitive Java API guide for the algorithms used by H2O.\\n\\n- <a href=\"https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/random%20hyperparmeter%20search%20and%20roadmap.md\">Hyperparameter Optimization in H2O </a>: A guide to Grid Search and Random Search in H2O.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/Interactions.md', 'section': '## Example\\n\\n```\\nlibrary(h2o)\\nlocalH2O <- h2o.init()\\n\\n# Create some random data\\nmyframe = h2o.createFrame(localH2O, \\'framekey\\', rows = 20, cols = 5,\\n                         seed = -12301283, randomize = TRUE, value = 0,\\n                         categorical_fraction = 0.8, factors = 10, real_range = 1,\\n                         integer_fraction = 0.2, integer_range = 10,\\n                         binary_fraction = 0, binary_ones_fraction = 0.5,\\n                         missing_fraction = 0.2,\\n                         response_factors = 1)\\n# Turn integer column into a categorical\\nmyframe[,5] <- as.factor(myframe[,5])\\nhead(myframe, 20)\\n\\n# Create pairwise interactions\\npairwise <- h2o.interaction(myframe, destination_frame = \\'pairwise\\',\\n                            factors = list(c(1,2),c(\"C2\",\"C3\",\"C4\")),\\n                            pairwise=TRUE, max_factors = 10, min_occurrence = 1)\\nhead(pairwise, 20)\\nh2o.levels(pairwise,2)\\n\\n# Create 5-th order interaction\\nhigherorder <- h2o.interaction(myframe, destination_frame = \\'higherorder\\', factors = c(1,2,3,4,5),\\n                               pairwise=FALSE, max_factors = 10000, min_occurrence = 1)\\nhead(higherorder, 20)\\n\\n# Limit the number of factors of the \"categoricalized\" integer column\\n# to at most 3 factors, and only if they occur at least twice\\nhead(myframe[,5], 20)\\ntrim_integer_levels <- h2o.interaction(myframe, destination_frame = \\'trim_integers\\', factors = \"C5\",\\n                                       pairwise = FALSE, max_factors = 3, min_occurrence = 2)\\nhead(trim_integer_levels, 20)\\n\\n# Put all together\\nmyframe <- h2o.cbind(myframe, pairwise, higherorder, trim_integer_levels)\\nmyframe\\nhead(myframe,20)\\nsummary(myframe)\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## Commonalities\\n\\n### Quantiles\\n\\n\\n**Note**: The quantile results in Flow are computed lazily on-demand and cached. It is a fast approximation (max - min / 1024) that is very accurate for most use cases. \\nIf the distribution is skewed, the quantile results may not be as accurate as the results obtained using `h2o.quantile` in R or `H2OFrame.quantile` in Python.  \\n\\n<a name=\"Kmeans\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## K-Means\\n\\n### Introduction\\n\\nK-Means falls in the general category of clustering algorithms.\\n\\n### Defining a K-Means Model\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: (Optional) Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **k***: Specify the number of clusters.  \\n\\n- **user_points**: Specify a vector of initial cluster centers. The user-specified points must have the same number of columns as the training observations. The number of rows must equal the number of clusters. \\n\\n- **max_iterations**: Specify the maximum number of training iterations. The range is 0 to 1e6.\\n\\n- **init**: Select the initialization mode. The options are Random, Furthest, PlusPlus, or User. **Note**: If PlusPlus is selected, the initial Y matrix is chosen by the final cluster centers from the K-Means PlusPlus algorithm. \\n\\n- **fold_assignment**: (Applicable only if a value for **nfolds** is specified and **fold_column** is not selected) Select the cross-validation fold assignment scheme. The available options are AUTO (which is Random), Random, or [Modulo](https://en.wikipedia.org/wiki/Modulo_operation). \\n\\n- **fold_column**: Select the column that contains the cross-validation fold index assignment per observation. \\n\\n- **score\\\\_each\\\\_iteration**: (Optional) Check this checkbox to score during each iteration of the model training. \\n\\n- **standardize**: To standardize the numeric columns to have mean of zero and unit variance, check this checkbox. Standardization is highly recommended; if you do not use standardization, the results can include components that are dominated by variables that appear to have larger variances relative to other attributes as a matter of scale, rather than true contribution. This option is selected by default. \\n\\n >**Note**: If standardization is enabled, each column of numeric data is centered and scaled so that its mean is zero and its standard deviation is one before the algorithm is used. At the end of the process, the cluster centers on both the standardized scale (`centers_std`) and the de-standardized scale (`centers`) are displayed. \\n >To de-standardize the centers, the algorithm multiplies by the original standard deviation of the corresponding column and adds the original mean. Enabling standardization is mathematically equivalent to using `h2o.scale` in R with `center` = TRUE and `scale` = TRUE on the numeric columns. Therefore, there will be no discernible difference if standardization is enabled or not for K-Means, since H2O calculates unstandardized centroids. \\n\\n- **keep\\\\_cross\\\\_validation\\\\_predictions**: To keep the cross-validation predictions, check this checkbox. \\n \\n- **seed**: Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n### Interpreting a K-Means Model\\n\\nBy default, the following output displays:\\n\\n- A graph of the scoring history (number of iterations vs. average within the cluster\\'s sum of squares) \\n- Output (model category, validation metrics if applicable, and centers std)\\n- Model Summary (number of clusters, number of categorical columns, number of iterations, avg. within sum of squares, avg. sum of squares, avg. between the sum of squares)\\n- Scoring history (number of iterations, avg. change of standardized centroids, avg. within cluster sum of squares)\\n- Training metrics (model name, checksum name, frame name, frame checksum name, description if applicable, model category, duration in ms, scoring time, predictions, MSE, avg. within sum of squares, avg. between sum of squares)\\n- Centroid statistics (centroid number, size, within sum of squares)\\n- Cluster means (centroid number, column)\\n\\nK-Means randomly chooses starting points and converges to a local minimum of centroids. The number of clusters is arbitrary, and should be thought of as a tuning parameter.\\nThe output is a matrix of the cluster assignments and the coordinates of the cluster centers in terms of the originally chosen attributes. Your cluster centers may differ slightly from run to run as this problem is Non-deterministic Polynomial-time (NP)-hard.\\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during training?**\\n   \\n  Missing values are automatically imputed by the column mean.  K-means also handles missing values by assuming that missing feature distance contributions are equal to the average of all other distance term contributions.\\n\\n- **How does the algorithm handle missing values during testing?**\\n   \\n  Missing values are automatically imputed by the column mean of the training data.\\n\\n- **Does it matter if the data is sorted?** \\n  \\n  No.\\n\\n- **Should data be shuffled before training?**\\n  \\n  No.\\n\\n- **What if there are a large number of columns?**\\n  \\n  K-Means suffers from the curse of dimensionality: all points are roughly at the same distance from each other in high dimensions, making the algorithm less and less useful.\\n\\n- **What if there are a large number of categorical factor levels?**\\n\\n  This can be problematic, as categoricals are one-hot encoded on the fly, which can lead to the same problem as datasets with a large number of columns.\\n\\n\\n\\n### K-Means Algorithm\\n\\nThe number of clusters \\\\(K\\\\) is user-defined and is determined a priori. \\n\\n1. Choose \\\\(K\\\\) initial cluster centers \\\\(m_{k}\\\\) according to one of\\n   the following:\\n\\n    - **Randomization**: Choose \\\\(K\\\\) clusters from the set of \\\\(N\\\\) observations at random so that each observation has an equal chance of being chosen.\\n\\n    - **Plus Plus**  \\n\\n      a. Choose one center \\\\(m_{1}\\\\) at random. \\n\\n      2.  Calculate the difference between \\\\(m_{1}\\\\) and each of the remaining \\\\(N-1\\\\) observations \\\\(x_{i}\\\\). \\n  \\\\(d(x_{i}, m_{1}) = ||(x_{i}-m_{1})||^2\\\\)\\n\\n      3. Let \\\\(P(i)\\\\) be the probability of choosing \\\\(x_{i}\\\\) as \\\\(m_{2}\\\\). Weight \\\\(P(i)\\\\) by \\\\(d(x_{i}, m_{1})\\\\) so that those \\\\(x_{i}\\\\) furthest from \\\\(m_{2}\\\\) have  a higher probability of being selected than those \\\\(x_{i}\\\\) close to \\\\(m_{1}\\\\).\\n\\n      4. Choose the next center \\\\(m_{2}\\\\) by drawing at random according to the weighted probability distribution. \\n\\n      5.  Repeat until \\\\(K\\\\) centers have been chosen.\\n\\n   - **Furthest**\\n\\n       a. Choose one center \\\\(m_{1}\\\\) at random. \\n\\n       2. Calculate the difference between \\\\(m_{1}\\\\) and each of the remaining \\\\(N-1\\\\) observations \\\\(x_{i}\\\\). \\n       \\\\(d(x_{i}, m_{1}) = ||(x_{i}-m_{1})||^2\\\\)\\n\\n       3. Choose \\\\(m_{2}\\\\) to be the \\\\(x_{i}\\\\) that maximizes \\\\(d(x_{i}, m_{1})\\\\).\\n\\n       4. Repeat until \\\\(K\\\\) centers have been chosen. \\n\\n2. Once \\\\(K\\\\) initial centers have been chosen calculate the difference between each observation \\\\(x_{i}\\\\) and each of the centers \\\\(m_{1},...,m_{K}\\\\), where difference is the squared Euclidean distance taken over \\\\(p\\\\) parameters.  \\n  \\n   \\\\(d(x_{i}, m_{k})=\\\\)\\n   \\\\(\\\\sum_{j=1}^{p}(x_{ij}-m_{k})^2=\\\\)\\n \\\\(\\\\lVert(x_{i}-m_{k})\\\\rVert^2\\\\)\\n\\n\\n3. Assign \\\\(x_{i}\\\\) to the cluster \\\\(k\\\\) defined by \\\\(m_{k}\\\\) that minimizes \\\\(d(x_{i}, m_{k})\\\\)\\n\\n4. When all observations \\\\(x_{i}\\\\) are assigned to a cluster calculate the mean of the points in the cluster. \\n\\n\\t\\\\(\\\\bar{x}(k)=\\\\lbrace\\\\bar{x_{i1}},\\\\bar{x_{ip}}\\\\rbrace\\\\)\\n\\n5. Set the \\\\(\\\\bar{x}(k)\\\\) as the new cluster centers \\\\(m_{k}\\\\). Repeat steps 2 through 5 until the specified number of max iterations is reached or cluster assignments of the \\\\(x_{i}\\\\) are stable.\\n\\n\\n\\n### References\\n\\n[Hastie, Trevor, Robert Tibshirani, and J Jerome H Friedman. The Elements of Statistical Learning. Vol.1. N.p., Springer New York, 2001.](http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf)\\n\\nXiong, Hui, Junjie Wu, and Jian Chen. K-means Clustering Versus Validation Measures: A Data- distribution Perspective. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on 39.2 (2009): 318-331.\\n\\n---\\n\\n<a name=\"GLM\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## GLM\\n\\n### Introduction\\n\\nGeneralized Linear Models (GLM) estimate regression models for outcomes following exponential distributions. In addition to the Gaussian (i.e. normal) distribution, these include Poisson, binomial, and gamma distributions. Each serves a different purpose, and depending on distribution and link function choice, can be used either for prediction or classification.\\n\\nThe GLM suite includes:\\n\\n- Gaussian regression\\n- Poisson regression\\n- Binomial regression (classification)\\n- Multinomial classification\\n- Gamma regression\\n\\n\\n### Defining a GLM Model\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **nfolds**: Specify the number of folds for cross-validation.\\n\\n- **response_column**: (Required) Select the column to use as the independent variable.\\n\\t\\n\\t- For a regression model, this column must be numeric (**Real** or **Int**).\\n\\t- For a classification model, this column must be categorical (**Enum** or **String**).  If the family is **Binomial**, the dataset cannot contain more than two levels. \\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **family**: Select the model type.\\n\\t> - If the family is **gaussian**, the data must be numeric (**Real** or **Int**).\\n\\t> - If the family is **binomial**, the data must be categorical 2 levels/classes or binary (**Enum** or **Int**).\\n\\t> - If the family is **multinomial**, the data can be categorical with more than two levels/classes (**Enum**).\\n\\t> - If the family is **poisson**, the data must be numeric and non-negative (**Int**).\\n\\t> - If the family is **gamma**, the data must be numeric and continuous and positive (**Real** or **Int**).\\n\\t> - If the family is **tweedie**, the data must be numeric and continuous (**Real**) and non-negative.\\n\\n- **tweedie_variance_power**: (Only applicable if *Tweedie* is selected for **Family**) Specify the Tweedie variance power. \\n\\n- **tweedie_link_power**: (Only applicable if *Tweedie* is selected for **Family**) Specify the Tweedie link power. \\n\\n- **solver**: Select the solver to use (AUTO, IRLSM, L\\\\_BFGS, COORDINATE\\\\_DESCENT\\\\_NAIVE, or COORDINATE\\\\_DESCENT). IRLSM is fast on on problems with a small number of predictors and for lambda-search with L1 penalty, while [L_BFGS](http://cran.r-project.org/web/packages/lbfgs/vignettes/Vignette.pdf) scales better for datasets with many columns.  COORDINATE\\\\_DESCENT is IRLSM with the covariance updates version of cyclical coordinate descent in the innermost loop. COORDINATE\\\\_DESCENT\\\\_NAIVE is IRLSM with the naive updates version of cyclical coordinate descent in the innermost loop.\\n\\n- **alpha**: Specify the regularization distribution between L2 and L2.  \\n\\n- **lambda**:  Specify the regularization strength. \\n\\n- **lambda_search**: Check this checkbox to enable lambda search, starting with lambda max. The given lambda is then interpreted as lambda min.\\n\\n- **nlambdas**: (Applicable only if **lambda\\\\_search** is enabled) Specify the number of lambdas to use in the search. The default is 100. \\n\\n- **standardize**: To standardize the numeric columns to have a mean of zero and unit variance, check this checkbox. Standardization is highly recommended; if you do not use standardization, the results can include components that are dominated by variables that appear to have larger variances relative to other attributes as a matter of scale, rather than true contribution. This option is selected by default.\\n\\n- **remove_collinear_columns**: Automatically remove collinear columns during model-building. Collinear columns will be dropped from the model and will have 0 coefficient in the returned model. Can only be set if there is no regularization (lambda=0)\\n\\n- **compute_p_values**: Request computation of p-values. Only applicable with no penalty (lambda = 0 and no beta constraints). Setting remove_collinear_columns is recommended. H2O will return an error if p-values are requested and there are collinear columns and remove_collinear_columns flag is not set.\\n\\n- **non-negative**: To force coefficients to have non-negative values, check this checkbox. \\n\\n- **beta_constraints**: To use beta constraints, select a dataset from the drop-down menu. The selected frame is used to constraint the coefficient vector to provide upper and lower bounds. The dataset must contain a names column with valid coefficient names. \\n\\n- **fold_assignment**: (Applicable only if a value for **nfolds** is specified and **fold_column** is not selected) Select the cross-validation fold assignment scheme. The available options are AUTO (which is Random), Random, or [Modulo](https://en.wikipedia.org/wiki/Modulo_operation). \\n\\n- **fold_column**: Select the column that contains the cross-validation fold index assignment per observation. \\n\\n- **score\\\\_each\\\\_iteration**: (Optional) Check this checkbox to score during each iteration of the model training. \\n\\n- **offset_column**: Select a column to use as the offset; the value cannot be the same as the value for the `weights_column`. \\n\\t>*Note*: Offsets are per-row \"bias values\" that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. For more information, refer to the following [link](http://www.idg.pl/mirrors/CRAN/web/packages/gbm/vignettes/gbm.pdf). \\n\\n- **weights_column**: Select a column to use for the observation weights, which are used for bias correction. The specified `weights_column` must be included in the specified `training_frame`. *Python only*: To use a weights column when passing an H2OFrame to `x` instead of a list of column names, the specified `training_frame` must contain the specified `weights_column`. \\n\\t>*Note*: Weights are per-row observation weights and do not increase the size of the data frame. This is typically the number of times a row is repeated, but non-integer values are supported as well. During training, rows with higher weights matter more, due to the larger loss function pre-factor.  \\n\\n- **max_iterations**: Specify the number of training iterations.   \\n\\n- **link**: Select a link function (Identity, Family_Default, Logit, Log, Inverse, or Tweedie).\\n\\n\\t> - If the family is **Gaussian**, **Identity**, **Log**, and **Inverse** are supported. \\n\\t>  - If the family is **Binomial**, **Logit** is supported. \\n\\t>  - If the family is **Poisson**, **Log** and **Identity** are supported. \\n\\t>  - If the family is **Gamma**, **Inverse**, **Log**, and **Identity** are supported. \\n\\t>  - If the family is **Tweedie**, only **Tweedie** is supported. \\t \\t \\n\\n- **max\\\\_confusion\\\\_matrix\\\\_size**: Specify the maximum size (number of classes) for the confusion matrices printed in the logs. \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: (Applicable for classification only) Specify the maximum number (top K) of predictions to use for hit ratio computation. Applicable to multi-class only. To disable, enter `0`. \\n\\n- **keep\\\\_cross\\\\_validation\\\\_predictions**: To keep the cross-validation predictions, check this checkbox. \\n\\n- **intercept**: To include a constant term in the model, check this checkbox. This option is selected by default. \\n\\n- **objective_epsilon**: Specify a threshold for convergence. If the objective value is less than this threshold, the model is converged. \\n\\n- **beta_epsilon**: Specify the beta epsilon value. If the L1 normalization of the current beta change is below this threshold, consider using convergence. \\n\\n- **gradient_epsilon**: (For L-BFGS only) Specify a threshold for convergence. If the objective value (using the L-infinity norm) is less than this threshold, the model is converged. \\n\\n- **prior**: Specify prior probability for p(y==1). Use this parameter for logistic regression if the data has been sampled and the mean of response does not reflect reality. Note: this is simple method affecting only the intercept, you may want to use weights and offset for better fit.\\n\\n- **lambda\\\\_min\\\\_ratio**: Specify the minimum lambda to use for lambda search (specified as a ratio of **lambda\\\\_max**). \\n\\n- **max\\\\_active\\\\_predictors**: Specify the maximum number of active predictors during computation. This value is used as a stopping criterium to prevent expensive model building with many predictors. \\n\\n- **missing\\\\_values\\\\_handling**: Specify how to handle missing values (Skip or MeanImputation). This defaults to MeanImputation. \\n\\n- **seed**: Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n### Interpreting a GLM Model\\n\\nBy default, the following output displays:\\n\\n- A graph of the normalized coefficient magnitudes\\n- Output (model category, model summary, scoring history, training metrics, validation metrics, best lambda, threshold, residual deviance, null deviance, residual degrees of freedom, null degrees of freedom, AIC, AUC, binomial, rank)\\n- Coefficients\\n- Coefficient magnitudes\\n\\n### Handling of Categorical Variables\\nGLM auto-expands categorical variables into one-hot encoded binary variables (i.e. if variable has levels \"cat\",\"dog\", \"mouse\", cat is encoded as 1,0,0, mouse is 0,1,0 and dog is 0,0,1).\\nIt is generally more efficient to let GLM perform auto-expansion instead of expanding data manually and it also adds the benefit of correct handling of different categorical mappings between different datasets as welll as handling of unseen categorical levels.\\nUnlike binary numeric columns, auto-expanded variables are not standardized.\\n\\nIt is common to skip one of the levels during the one-hot encoding to prevent linear dependency between the variable and the intercept.\\nH3O follows the convention of skipping the first level.\\nThis behavior can be controlled by setting use_all_factor_levels_flag (no level is going to be skipped if the flag is true).\\nThe default depends on regularization parameter - it is set to false if no regularization and to true otherwise.\\nThe reference level which is skipped is always the first level, you can change which level is the reference level by calling h2o.relevel function prior to building the model.\\n\\n\\n### Lambda Search and Full Regularization Path\\nIf lambda_search option is set, GLM will compute models for full regularization path similar to glmnet (see glmnet paper).\\nRegularziation path starts at lambda max (highest lambda values which makes sense - i.e. lowest value driving all coefficients to zero) and goes down to lambda min on log scale, decreasing regularization strength at each step.\\nThe returned model will have coefficients corresponding to the \"optimal\" lambda value as decided during training.\\n\\nIt can sometimes be useful to see the coefficients for all lambda values. Or to override default lambda selection.\\nFull regularization path can be extracted from both R and python clients (currently not from Flow). It returns coefficients (and standardized coefficients)\\nfor all computed lambda values and also explained deviances on both train and validation.\\nSubsequently, makeGLMModel call can be used to create h2o glm model with selected coefficients.\\n\\nTo extract the regularization path from R or python:\\n - R: call h2o.getGLMFullRegularizationPath, takes the model as an argument\\n - pyton: H2OGeneralizedLinearEstimator.getGLMRegularizationPath (static method), takes the model as an rgument\\n\\n### Modifying or Creating Custom GLM Model\\nIn R and python, makeGLMModel call can be used to create h2o model from given coefficients.\\nIt needs a source glm model trained on the same dataset to extract dataset information.\\nTo make custom GLM model from R or python:\\n - R: call h2o.makeGLMModel, takes a model and a vector of coefficients and (optional) decision threshold as parameters.\\n - pyton: H2OGeneralizedLinearEstimator.makeGLMModel (static method), takes a model, dictionary containing coefficients and (optional) decision threshold as parameters.\\n\\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during training?**\\n\\n  Depending on the selected missing value handling policy, they are either imputed mean or the whole row is skipped.  \\n  The default behavior is mean imputation. Note that categorical variables are imputed by adding an extra \"missing\" level.   \\n  Optionally, glm can skip all rows with any missing values. \\n\\n- **How does the algorithm handle missing values during testing?**\\n  Same as during training. If the missing value handling is set to skip and we are generating predictions, skipped rows will have Na (missing) prediction.\\n\\n- **What happens if the response has missing values?**\\n\\n  The rows with missing response are ignored during model training and validation.\\n\\n- **What happens during prediction if the new sample has categorical levels not seen in training?**\\n  The value will be filled with either special missing level (if trained with missing values and missing_value_handling was set to MeanImputation) or 0.\\n\\n- **Does it matter if the data is sorted?** \\n\\n  No.\\n\\n- **Should data be shuffled before training?**\\n\\n  No.\\n\\n- **How does the algorithm handle highly imbalanced data in a response column?**\\n\\n  GLM does not require special handling for imbalanced data.\\n\\n- **What if there are a large number of columns?**\\n\\n  IRLS will get quadratically slower with the number of columns. Try L-BFGS for datasets with more than 5-10 thousand columns.\\n\\n- **What if there are a large number of categorical factor levels?**\\n\\n  GLM internally one-hot encodes the categorical factor levels; the same limitations as with a high column count will apply.\\n\\n- **When building the model, does GLM use all features or a selection of the best features?**\\n\\n  Typically, GLM picks the best predictors, especially if lasso is used (`alpha = 1`). By default, the GLM model includes an L1 penalty and will pick only the most predictive predictors. \\n\\n- **When running GLM, is it better to create a cluster that uses many smaller nodes or fewer larger nodes?** \\n\\nA rough heuristic would be: \\n\\n  nodes ~=M*N^2/(p*1e8)\\n\\nwhere M is the number of observations, N is the number of columns (categorical columns count as a single column in this case), and p is the number of CPU cores per node. \\n\\nFor example, a dataset with 250 columns and 1M rows would optimally use about 20 nodes with 32 cores each (following the formula 250^2*1000000/(32*1e8)  = 19.5 ~= 20). \\n\\n- **How is variable importance calculated for GLM?**\\n\\nFor GLM, the variable importance represents the coefficient magnitudes. \\n\\n\\n\\n### GLM Algorithm\\n\\nFollowing the definitive text by P. McCullagh and J.A. Nelder (1989) on the generalization of linear models to non-linear distributions of the response variable Y, H2O fits GLM models based on the maximum likelihood estimation via iteratively reweighed least squares. \\n\\nLet \\\\(y_{1},,y_{n}\\\\) be n observations of the independent, random response variable \\\\(Y_{i}\\\\).\\n\\nAssume that the observations are distributed according to a function from the exponential family and have a probability density function of the form:\\n\\n\\\\(f(y_{i})=exp[\\\\frac{y_{i}\\\\theta_{i} - b(\\\\theta_{i})}{a_{i}(\\\\phi)} + c(y_{i}; \\\\phi)]\\\\)\\nwhere \\\\(\\\\theta\\\\) and \\\\(\\\\phi\\\\) are location and scale parameters,\\nand \\\\(\\\\: a_{i}(\\\\phi), \\\\:b_{i}(\\\\theta_{i}),\\\\: c_{i}(y_{i}; \\\\phi)\\\\) are known functions.\\n\\n\\\\(a_{i}\\\\) is of the form \\\\(\\\\:a_{i}=\\\\frac{\\\\phi}{p_{i}}; p_{i}\\\\) is a known prior weight.\\n\\nWhen \\\\(Y\\\\) has a pdf from the exponential family: \\n\\n\\\\(E(Y_{i})=\\\\mu_{i}=b^{\\\\prime}\\\\)\\n\\\\(var(Y_{i})=\\\\sigma_{i}^2=b^{\\\\prime\\\\prime}(\\\\theta_{i})a_{i}(\\\\phi)\\\\)\\n\\nLet \\\\(g(\\\\mu_{i})=\\\\eta_{i}\\\\) be a monotonic, differentiable transformation of the expected value of \\\\(y_{i}\\\\). The function \\\\(\\\\eta_{i}\\\\) is the link function and follows a linear model.\\n\\n\\\\(g(\\\\mu_{i})=\\\\eta_{i}=\\\\mathbf{x_{i}^{\\\\prime}}\\\\beta\\\\)\\n\\nWhen inverted: \\n\\\\(\\\\mu=g^{-1}(\\\\mathbf{x_{i}^{\\\\prime}}\\\\beta)\\\\)\\n\\n**Maximum Likelihood Estimation**\\n\\nFor an initial rough estimate of the parameters \\\\(\\\\hat{\\\\beta}\\\\), use the estimate to generate fitted values: \\n\\\\(\\\\mu_{i}=g^{-1}(\\\\hat{\\\\eta_{i}})\\\\)\\n\\nLet \\\\(z\\\\) be a working dependent variable such that \\n\\\\(z_{i}=\\\\hat{\\\\eta_{i}}+(y_{i}-\\\\hat{\\\\mu_{i}})\\\\frac{d\\\\eta_{i}}{d\\\\mu_{i}}\\\\),\\n\\nwhere \\\\(\\\\frac{d\\\\eta_{i}}{d\\\\mu_{i}}\\\\) is the derivative of the link function evaluated at the trial estimate. \\n\\nCalculate the iterative weights:\\n\\\\(w_{i}=\\\\frac{p_{i}}{[b^{\\\\prime\\\\prime}(\\\\theta_{i})\\\\frac{d\\\\eta_{i}}{d\\\\mu_{i}}^{2}]}\\\\)\\n\\nWhere \\\\(b^{\\\\prime\\\\prime}\\\\) is the second derivative of \\\\(b(\\\\theta_{i})\\\\) evaluated at the trial estimate. \\n\\n\\nAssume \\\\(a_{i}(\\\\phi)\\\\) is of the form \\\\(\\\\frac{\\\\phi}{p_{i}}\\\\). The weight \\\\(w_{i}\\\\) is inversely proportional to the variance of the working dependent variable \\\\(z_{i}\\\\) for current parameter estimates and proportionality factor \\\\(\\\\phi\\\\).\\n\\nRegress \\\\(z_{i}\\\\) on the predictors \\\\(x_{i}\\\\) using the weights \\\\(w_{i}\\\\) to obtain new estimates of \\\\(\\\\beta\\\\). \\n\\\\(\\\\hat{\\\\beta}=(\\\\mathbf{X}^{\\\\prime}\\\\mathbf{W}\\\\mathbf{X})^{-1}\\\\mathbf{X}^{\\\\prime}\\\\mathbf{W}\\\\mathbf{z}\\\\) \\n\\nWhere \\\\(\\\\mathbf{X}\\\\) is the model matrix, \\\\(\\\\mathbf{W}\\\\) is a diagonal matrix of \\\\(w_{i}\\\\), and \\\\(\\\\mathbf{z}\\\\) is a vector of the working response variable \\\\(z_{i}\\\\).\\n\\nThis process is repeated until the estimates \\\\(\\\\hat{\\\\beta}\\\\) change by less than the specified amount. \\n\\n**Cost of computation**\\n\\n\\nH2O can process large data sets because it relies on parallel processes. Large data sets are divided into smaller data sets and processed simultaneously and the results are communicated between computers as needed throughout the process. \\n\\nIn GLM, data are split by rows but not by columns, because the predicted Y values depend on information in each of the predictor variable vectors. If O is a complexity function, N is the number of observations (or rows), and P is the number of predictors (or columns) then \\n\\n\\n   &nbsp;&nbsp;&nbsp;&nbsp;\\\\(Runtime\\\\propto p^3+\\\\frac{(N*p^2)}{CPUs}\\\\)\\n\\nDistribution reduces the time it takes an algorithm to process because it decreases N.\\n \\n\\nRelative to P, the larger that (N/CPUs) becomes, the more trivial p becomes to the overall computational cost. However, when p is greater than (N/CPUs), O is dominated by p.\\n\\n\\n\\n   &nbsp;&nbsp;&nbsp;&nbsp;\\\\(Complexity = O(p^3 + N*p^2)\\\\) \\n\\nFor more information about how GLM works, refer to the [Generalized Linear Modeling booklet](http://h2o.ai/resources). \\n\\n\\n### References\\n\\nBreslow, N E. Generalized Linear Models: Checking Assumptions and Strengthening Conclusions. Statistica Applicata 8 (1996): 23-41.\\n\\n[Frome, E L. The Analysis of Rates Using Poisson Regression Models. Biometrics (1983): 665-674.](http://www.csm.ornl.gov/~frome/BE/FP/FromeBiometrics83.pdf)\\n\\n[Goldberger, Arthur S. Best Linear Unbiased Prediction in the Generalized Linear Regression Model. Journal of the American Statistical Association 57.298 (1962): 369-375.](http://people.umass.edu/~bioep740/yr2009/topics/goldberger-jasa1962-369.pdf)\\n\\n[Guisan, Antoine, Thomas C Edwards Jr, and Trevor Hastie. Generalized Linear and Generalized Additive Models in Studies of Species Distributions: Setting the Scene. Ecological modeling 157.2 (2002): 89-100.](http://www.stanford.edu/~hastie/Papers/GuisanEtAl_EcolModel-2003.pdf)\\n\\n[Nelder, John A, and Robert WM Wedderburn. Generalized Linear Models. Journal of the Royal Statistical Society. Series A (General) (1972): 370-384.](http://biecek.pl/MIMUW/uploads/Nelder_GLM.pdf)\\n\\n[Niu, Feng, et al. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. Advances in Neural Information Processing Systems 24 (2011): 693-701.*implemented algorithm on p.5](http://www.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf)\\n\\n[Pearce, Jennie, and Simon Ferrier. Evaluating the Predictive Performance of Habitat Models Developed Using Logistic Regression. Ecological modeling 133.3 (2000): 225-245.](http://www.whoi.edu/cms/files/Ecological_Modelling_2000_Pearce_53557.pdf)\\n\\n[Press, S James, and Sandra Wilson. Choosing Between Logistic Regression and Discriminant Analysis. Journal of the American Statistical Association 73.364 (April, 2012): 699705.](http://www.statpt.com/logistic/press_1978.pdf)\\n\\nSnee, Ronald D. Validation of Regression Models: Methods and Examples. Technometrics 19.4 (1977): 415-428.\\n\\n---\\n\\n\\n<a name=\"DRF\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## DRF\\n\\n### Introduction\\n\\nDistributed Random Forest (DRF) is a powerful classification and regression tool. When given a set of data, DRF generates a forest of classification (or regression) trees, rather than a single classification (or regression) tree. Each of these trees is a weak learner built on a subset of rows and columns. More trees will reduce the variance. Both classification and regression take the average prediction over all of their trees to make a final prediction, whether predicting for a class or numeric value (note: for a categorical response column, DRF maps factors  (e.g. \\'dog\\', \\'cat\\', \\'mouse) in lexicographic order to a name lookup array with integer indices (e.g. \\'cat ->0, \\'dog\\' -> 1, \\'mouse\\' ->2).\\n\\nThe current version of DRF is fundamentally the same as in previous versions of H2O (same algorithmic steps, same histogramming techniques), with the exception of the following changes: \\n\\n- Improved ability to train on categorical variables (using the `nbins_cats` parameter)\\n- Minor changes in histogramming logic for some corner cases\\n- By default, DRF builds half as many trees for binomial problems, similar to GBM: it uses a single tree to estimate class 0 (probability \"p0\"), and then computes the probability of class 0 as ``1.0 - p0``. For multiclass problems, a tree is used to estimate the probability of each class separately. \\n\\nThere was some code cleanup and refactoring to support the following features:\\n\\n- Per-row observation weights\\n- Per-row offsets\\n- N-fold cross-validation\\n\\nDRF no longer has a special-cased histogram for classification or regression (class DBinomHistogram has been superseded by DRealHistogram) since it was not applicable to cases with observation weights or for cross-validation. \\n\\n\\n### Defining a DRF Model\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **nfolds**: Specify the number of folds for cross-validation. \\n\\n- **response_column**: (Required) Select the column to use as the independent variable. The data can be numeric or categorical. \\n\\n- **Ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **ntrees**: Specify the number of trees.  \\n\\n- **max\\\\_depth**: Specify the maximum tree depth.\\n\\n- **min\\\\_rows**: Specify the minimum number of observations for a leaf (`nodesize` in R).  \\n\\n- **nbins**: (Numerical/real/int only) Specify the number of bins for the histogram to build, then split at the best point.  \\n\\n- **nbins_cats**: (Categorical/enums only) Specify the maximum number of bins for the histogram to build, then split at the best point. Higher values can lead to more overfitting.  The levels are ordered alphabetically; if there are more levels than bins, adjacent levels share bins. This value has a more significant impact on model fitness than **nbins**. Larger values may increase runtime, especially for deep trees and large clusters, so tuning may be required to find the optimal value for your configuration.\\n\\n- **seed**: Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n- **mtries**: Specify the columns to randomly select at each level. If the default value of `-1` is used, the number of variables is the square root of the number of columns for classification and p/3 for regression (where p is the number of predictors). The range is -1 to >=1. \\n\\n- **sample_rate**: Specify the row sampling rate (x-axis). The range is 0.0 to 1.0. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled. For details, refer to \"Stochastic Gradient Boosting\" ([Friedman, 1999](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)). If this option is specified along with **sample\\\\_rate_per\\\\_class**, then only the first option that DRF encounters will be used.\\n\\n- **col\\\\_sample_rate**: Specify the column sampling rate (y-axis). The range is 0.0 to 1.0. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled. For details, refer to \"Stochastic Gradient Boosting\" ([Friedman, 1999](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)). \\n\\n- **score\\\\_each\\\\_iteration**: (Optional) Check this checkbox to score during each iteration of the model training. \\n\\n- **score\\\\_tree\\\\_interval**: Score the model after every so many trees. Disabled if set to 0.\\n\\n- **fold_assignment**: (Applicable only if a value for **nfolds** is specified and **fold_column** is not selected) Select the cross-validation fold assignment scheme. The available options are AUTO (which is Random), Random, or [Modulo](https://en.wikipedia.org/wiki/Modulo_operation). \\n\\n- **fold_column**: Select the column that contains the cross-validation fold index assignment per observation. \\n\\n- **offset_column**:  Select a column to use as the offset. \\n\\t>*Note*: Offsets are per-row \"bias values\" that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. For more information, refer to the following [link](http://www.idg.pl/mirrors/CRAN/web/packages/gbm/vignettes/gbm.pdf). \\n\\n- **weights_column**: Select a column to use for the observation weights, which are used for bias correction. The specified `weights_column` must be included in the specified `training_frame`. *Python only*: To use a weights column when passing an H2OFrame to `x` instead of a list of column names, the specified `training_frame` must contain the specified `weights_column`. \\n\\t>*Note*: Weights are per-row observation weights and do not increase the size of the data frame. This is typically the number of times a row is repeated, but non-integer values are supported as well. During training, rows with higher weights matter more, due to the larger loss function pre-factor.  \\n\\n- **balance_classes**: Oversample the minority classes to balance the class distribution. This option is not selected by default and can increase the data frame size. This option is only applicable for classification. \\n\\n- **max\\\\_confusion\\\\_matrix\\\\_size**: Specify the maximum size (in number of classes) for confusion matrices to be printed in the Logs. \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: Specify the maximum number (top K) of predictions to use for hit ratio computation. Applicable to multi-class only. To disable, enter 0. \\n\\n- **r2_stopping**: Specify a threshold for the coefficient of determination (\\\\(r^2\\\\)) metric value. When this threshold is met or exceeded, H2O stops making trees. \\n\\n- **stopping\\\\_rounds**: Stops training when the option selected for **stopping\\\\_metric** doesn\\'t improve for the specified number of training rounds, based on a simple moving  average. To disable this feature, specify `0`. The metric is computed on the validation data (if provided); otherwise, training data is used. When used with **overwrite\\\\_with\\\\_best\\\\_model**, the final model is the best model generated for the given **stopping\\\\_metric** option.   \\n\\t>**Note**: If cross-validation is enabled: \\n\\t1. All cross-validation models stop training when the validation metric doesn\\'t improve. \\n    2. The main model runs for the mean number of epochs. \\n    3. N+1 models do *not* use **overwrite\\\\_with\\\\_best\\\\_model**\\n    4. N+1 models may be off by the number specified for **stopping\\\\_rounds** from the best model, but the cross-validation metric estimates the performance of the main model for the resulting number of epochs (which may be fewer than the specified number of epochs). \\n\\n- **stopping\\\\_metric**: Select the metric to use for early stopping. The available options are: \\n\\t\\n    - **AUTO**: Logloss for classification; deviance for regression\\n    - **deviance**\\n    - **logloss**\\n    - **MSE**\\n    - **AUC**\\n    - **r2**\\n    - **misclassification**\\n    - **mean\\\\_per\\\\_class\\\\_error**\\n\\n- **stopping\\\\_tolerance**: Specify the relative tolerance for the metric-based stopping to stop training if the improvement is less than this value. \\n\\n- **max\\\\_runtime\\\\_secs**: Maximum allowed runtime in seconds for model training. Use 0 to disable.\\n\\n- **build\\\\_tree\\\\_one\\\\_node**: To run on a single node, check this checkbox. This is suitable for small datasets as there is no network overhead but fewer CPUs are used. \\n\\n- **sample\\\\_rate\\\\_per\\\\_class**: When building models from imbalanced datasets, this option specifies that each tree in the ensemble should sample from the full training dataset using a per-class-specific sampling rate rather than a global sample factor (as with `sample_rate`). The range for this option is 0.0 to 1.0. If this option is specified along with **sample_rate**, then only the first option that DRF encounters will be used.\\n\\n- **binomial\\\\_double\\\\_trees**: (Binary classification only) Build twice as many trees (one per class). Enabling this option can lead to higher accuracy, while disabling can result in faster model building. This option is disabled by default. \\n\\n- **checkpoint**: Enter a model key associated with a previously-trained model. Use this option to build a new model as a continuation of a previously-generated model.\\n\\n- **col\\\\_sample_rate\\\\_change\\\\_per\\\\_level**: This option specifies to change the column sampling rate as a function of the depth in the tree. For example:\\n\\t>level 1: **col\\\\_sample_rate**\\n\\t\\n\\t>level 2: **col\\\\_sample_rate** * **factor**\\n\\t\\n\\t>level 3: **col\\\\_sample_rate** * **factor^2**\\n\\t\\n\\t>level 4: **col\\\\_sample_rate** * **factor^3**\\n\\t\\n\\t>etc. \\n\\t\\n- **col\\\\_sample\\\\_rate\\\\_per\\\\_tree**: Specifies the column sample rate per tree. This can be a value from 0.0 to 1.0. \\n\\t\\n- **min\\\\_split_improvement**: The value of this option specifies the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10...1e-3 range.\\n\\n- **histogram_type**: By default (AUTO) DRF bins from min...max in steps of (max-min)/N. Random split points or quantile-based split points can be selected as well. RoundRobin can be specified to cycle through all histogram types (one per tree). Use this option to specify the type of histogram to use for finding optimal split points:\\n\\n  - AUTO\\n  - UniformAdaptive\\n  - Random\\n  - QuantilesGlobal\\n  - RoundRobin\\n\\n  >**Note**: H2O supports extremely randomized trees via ``histogram_type=\"Random\"``. In extremely randomized trees (Extra-Trees), randomness goes one step further in the way splits are computed. As in Random Forests, a random subset of candidate features is used, but instead of looking for the best split, thresholds (for the split) are drawn at random for each candidate feature, and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.\\n\\n- **keep\\\\_cross\\\\_validation\\\\_predictions**: To keep the cross-validation predictions, check this checkbox. \\n\\n- **class\\\\_sampling\\\\_factors**: Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance.  \\n\\n- **max\\\\_after\\\\_balance\\\\_size**: Specify the maximum relative size of the training data after balancing class counts (**balance\\\\_classes** must be enabled). The value can be less than 1.0. \\n\\n- **nbins\\\\_top\\\\_level**: (For numerical/real/int columns only) Specify the minimum number of bins at the root level to use to build the histogram. This number will then be decreased by a factor of two per level.  \\n\\n\\n### Interpreting a DRF Model\\n\\nBy default, the following output displays:\\n\\n- Model parameters (hidden)  \\n- A graph of the scoring history (number of trees vs. training MSE)\\n- A graph of the ROC curve (TPR vs. FPR)\\n- A graph of the variable importances\\n- Output (model category, validation metrics, initf)\\n- Model summary (number of trees, min. depth, max. depth, mean depth, min. leaves, max. leaves, mean leaves)\\n- Scoring history in tabular format\\n- Training metrics (model name, checksum name, frame name, frame checksum name, description, model category, duration in ms, scoring time, predictions, MSE, R2, logloss, AUC, GINI)\\n- Training metrics for thresholds (thresholds, F1, F2, F0Points, Accuracy, Precision, Recall, Specificity, Absolute MCC, min. per-class accuracy, TNS, FNS, FPS, TPS, IDX)\\n- Maximum metrics (metric, threshold, value, IDX)\\n- Variable importances in tabular format\\n\\n\\n### Leaf Node Assignment\\nTrees cluster observations into leaf nodes, and this information can be useful for feature engineering or model interpretability. Use **h2o.predict\\\\_leaf\\\\_node\\\\_assignment\\\\(model, frame\\\\)** to get an H2OFrame with the leaf node assignments, or click the checkbox when making predictions from Flow. Those leaf nodes represent decision rules that can be fed to other models (i.e., GLM with lambda search and strong rules) to obtain a limited set of the most important rules.\\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during training?**\\n\\n  Missing values are interpreted as containing information (i.e., missing for a reason), rather than missing at random. During tree building, split decisions for every node are found by minimizing the loss function and treating missing values as a separate category that can go either left or right.\\n\\n- **How does the algorithm handle missing values during testing?**\\n\\n  During scoring, missing values follow the optimal path that was determined for them during training (minimized loss function).\\n\\n- **What happens if the response has missing values?**\\n \\n  No errors will occur, but nothing will be learned from rows containing missing the response.\\n\\n- **Does it matter if the data is sorted?** \\n\\n  No.\\n\\n- **Should data be shuffled before training?**\\n  \\n  No.\\n\\n- **How does the algorithm handle highly imbalanced data in a response column?**\\n\\n Specify `balance_classes`, `class_sampling_factors` and `max_after_balance_size` to control over/under-sampling.\\n\\n- **What if there are a large number of columns?**\\n\\n  DRFs are best for datasets with fewer than a few thousand columns.\\n\\n- **What if there are a large number of categorical factor levels?**\\n\\n  Large numbers of categoricals are handled very efficiently - there is never any one-hot encoding.\\n\\n- **How is variable importance calculated for DRF?**\\n\\nVariable importance is determined by calculating the relative influence of each variable: whether that variable was selected during splitting in the tree building process and how much the squared error (over all trees) improved as a result. \\n\\n- **How is column sampling implemented for DRF?**\\n\\nFor an example model using: \\n\\n- 100 columns\\n- `col_sample_rate_per_tree` is 0.602\\n- `mtries` is -1 or 7 (refers to the number of active predictor columns for the dataset)\\n\\nFor each tree, the floor is used to determine the number - for this example, (0.602*100)=60 out of the 100 - of columns that are randomly picked. For classification cases where `mtries=-1`, the square root - for this example, (100)=10 columns - are then randomly chosen for each split decision (out of the total 60). \\n\\nFor regression, the floor - in this example, (100/3)=33 columns - is used for each split by default. If `mtries=7`, then 7 columns are picked for each split decision (out of the 60). \\n\\n`mtries` is configured independently of `col_sample_rate_per_tree`, but it can be limited by it. For example, if `col_sample_rate_per_tree=0.01`, then there\\'s only one column left for each split, regardless of how large the value for `mtries` is.\\n\\n\\n### DRF Algorithm \\n\\n\\n<iframe src=\"//www.slideshare.net/slideshow/embed_code/key/tASzUyJ19dtJsQ\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" style=\"border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;\" allowfullscreen> </iframe> <div style=\"margin-bottom:5px\"> <strong> <a href=\"//www.slideshare.net/0xdata/rf-brighttalk\" title=\"Building Random Forest at Scale\" target=\"_blank\">Building Random Forest at Scale</a> </strong> from <strong><a href=\"//www.slideshare.net/0xdata\" target=\"_blank\">Sri Ambati</a></strong> </div>\\n\\n### References\\n\\n<a href=\"http://link.springer.com/article/10.1007%2Fs10994-006-6226-1\" target=\"_blank\">P. Geurts, D. Ernst., and L. Wehenkel, Extremely randomized trees, Machine Learning, 63(1), 3-42, 2006.</a>\\n\\n---\\n\\n<a name=\"NB\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## Nave Bayes\\n\\n### Introduction \\n\\nNave Bayes (NB) is a classification algorithm that relies on strong assumptions of the independence of covariates in applying Bayes Theorem. NB models are commonly used as an alternative to decision trees for classification problems.\\n\\n### Defining a Nave Bayes Model\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **response_column**: (Required) Select the column to use as the independent variable. The data must be categorical and must contain at least two unique categorical levels. \\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **laplace**: Specify the Laplace smoothing parameter. The value must be an integer >= 0. \\n\\n- **min\\\\_sdev**: Specify the minimum standard deviation to use for observations without enough data. The value must be at least 1e-10.   \\n\\n- **eps\\\\_sdev**: Specify the threshold for standard deviation. The value must be positive. If this threshold is not met, the **min\\\\_sdev** value is used.  \\n\\n- **min\\\\_prob**: Specify the minimum probability to use for observations without enough data.   \\n\\n- **eps\\\\_prob**: Specify the threshold for standard deviation. If this threshold is not met, the **min\\\\_sdev** value is used.  \\n\\n- **compute_metrics**: To compute metrics on training data, check this checkbox. The Nave Bayes classifier assumes independence between predictor variables conditional on the response, and a Gaussian distribution of numeric predictors with mean and standard deviation computed from the training dataset. When building a Nave Bayes classifier, every row in the training dataset that contains at least one NA will be skipped completely. If the test dataset has missing values, then those predictors are omitted in the probability calculation during prediction.\\n\\n- **score\\\\_each\\\\_iteration**: (Optional) Check this checkbox to score during each iteration of the model training. \\n\\n- **max\\\\_confusion\\\\_matrix\\\\_size**: Specify the maximum size (in number of classes) for confusion matrices to be printed in the Logs. \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: Specify the maximum number (top K) of predictions to use for hit ratio computation. Applicable to multi-class only. To disable, enter 0. \\n\\n- **max\\\\_runtime\\\\_secs**: Maximum allowed runtime in seconds for model training. Use 0 to disable.\\n\\n\\n\\n### Interpreting a Nave Bayes Model\\n\\nThe output from Nave Bayes is a list of tables containing the a-priori and conditional probabilities of each class of the response. The a-priori probability is the estimated probability of a particular class before observing any of the predictors. Each conditional probability table corresponds to a predictor column. The row headers are the classes of the response and the column headers are the classes of the predictor. Thus, in the table below, the probability of survival (y) given a person is male (x) is 0.91543624.\\n\\n```\\n        \\t\\tSex\\nSurvived       Male     Female\\n     No  0.91543624 0.08456376\\n     Yes 0.51617440 0.48382560\\n```\\n\\n\\nWhen the predictor is numeric, Nave Bayes assumes it is sampled from a Gaussian distribution given the class of the response. The first column contains the mean and the second column contains the standard deviation of the distribution.\\n\\nBy default, the following output displays:\\n\\n- Output (model category, model summary, scoring history, training metrics, validation metrics)\\n- Y-Levels (levels of the response column)\\n- P-conditionals \\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during training?**\\n  \\n  All rows with one or more missing values (either in the predictors or the response) will be skipped during model building. \\n\\n- **How does the algorithm handle missing values during testing?**\\n  \\n  If a predictor is missing, it will be skipped when taking the product of conditional probabilities in calculating the joint probability conditional on the response.\\n\\n- **What happens if the response domain is different in the training and test datasets?**\\n  \\n  The response column in the test dataset is not used during scoring, so any response categories absent in the training data will not be predicted.\\n\\n- **What happens during prediction if the new sample has categorical levels not seen in training?**\\n  \\n  The conditional probability of that predictor level will be set according to the Laplace smoothing factor. If Laplace smoothing is disabled (set to zero), the joint probability will be zero. See pgs. 13-14 of Andrew Ngs \"Generative learning algorithms\" in the References section for mathematical details.\\n\\n- **Does it matter if the data is sorted?**\\n\\n  No. \\n\\n- **Should data be shuffled before training?**\\n\\n  This does not affect model building. \\n\\n- **How does the algorithm handle highly imbalanced data in a response column?**\\n\\n  Unbalanced data will not affect the model. However, if one response category has very few observations compared to the total, the conditional probability may be very low. A cutoff (`eps_prob`) and minimum value (`min_prob`) are available for the user to set a floor on the calculated probability.\\n\\n\\n- **What if there are a large number of columns?**\\n\\n   More memory will be allocated on each node to store the joint frequency counts and sums.\\n\\n- **What if there are a large number of categorical factor levels?**\\n\\n  More memory will be allocated on each node to store the joint frequency count of each categorical predictor level with the responses level.\\n\\n- **When running PCA, is it better to create a cluster that uses many smaller nodes or fewer larger nodes?** \\n\\nFor Nave Bayes, we recommend using many smaller nodes because the distributed task doesn\\'t require intensive computation. \\n\\n\\n### Nave Bayes Algorithm \\n\\nThe algorithm is presented for the simplified binomial case without loss of generality.\\n\\nUnder the Naive Bayes assumption of independence, given a training set\\nfor a set of discrete valued features X \\n\\\\({(X^{(i)},\\\\ y^{(i)};\\\\ i=1,...m)}\\\\)\\n\\nThe joint likelihood of the data can be expressed as: \\n\\n\\\\(\\\\mathcal{L} \\\\: (\\\\phi(y),\\\\: \\\\phi_{i|y=1},\\\\:\\\\phi_{i|y=0})=\\\\Pi_{i=1}^{m} p(X^{(i)},\\\\: y^{(i)})\\\\)\\n\\nThe model can be parameterized by:\\n\\n\\\\(\\\\phi_{i|y=0}=\\\\ p(x_{i}=1|\\\\ y=0);\\\\: \\\\phi_{i|y=1}=\\\\ p(x_{i}=1|y=1);\\\\: \\\\phi(y)\\\\)\\n\\nWhere \\\\(\\\\phi_{i|y=0}=\\\\ p(x_{i}=1|\\\\ y=0)\\\\) can be thought of as the fraction of the observed instances where feature \\\\(x_{i}\\\\) is observed, and the outcome is \\\\(y=0, \\\\phi_{i|y=1}=p(x_{i}=1|\\\\ y=1)\\\\) is the fraction of the observed instances where feature \\\\(x_{i}\\\\) is observed, and the outcome is \\\\(y=1\\\\), and so on.\\n\\nThe objective of the algorithm is to maximize with respect to\\n\\\\(\\\\phi_{i|y=0}, \\\\ \\\\phi_{i|y=1},\\\\ and \\\\ \\\\phi(y)\\\\)\\n\\nWhere the maximum likelihood estimates are: \\n\\n\\\\(\\\\phi_{j|y=1}= \\\\frac{\\\\Sigma_{i}^m 1(x_{j}^{(i)}=1 \\\\ \\\\bigcap y^{i} = 1)}{\\\\Sigma_{i=1}^{m}(y^{(i)}=1}\\\\)\\n\\n\\\\(\\\\phi_{j|y=0}= \\\\frac{\\\\Sigma_{i}^m 1(x_{j}^{(i)}=1 \\\\ \\\\bigcap y^{i} = 0)}{\\\\Sigma_{i=1}^{m}(y^{(i)}=0}\\\\)\\n\\n\\\\(\\\\phi(y)= \\\\frac{(y^{i} = 1)}{m}\\\\)\\n\\n\\nOnce all parameters \\\\(\\\\phi_{j|y}\\\\) are fitted, the model can be used to predict new examples with features \\\\(X_{(i^*)}\\\\). \\n\\nThis is carried out by calculating: \\n\\n\\\\(p(y=1|x)=\\\\frac{\\\\Pi p(x_i|y=1) p(y=1)}{\\\\Pi p(x_i|y=1)p(y=1) \\\\: +\\\\: \\\\Pi p(x_i|y=0)p(y=0)}\\\\)\\n\\n\\\\(p(y=0|x)=\\\\frac{\\\\Pi p(x_i|y=0) p(y=0)}{\\\\Pi p(x_i|y=1)p(y=1) \\\\: +\\\\: \\\\Pi p(x_i|y=0)p(y=0)}\\\\)\\n\\nand predicting the class with the highest probability. \\n\\n\\nIt is possible that prediction sets contain features not originally seen in the training set. If this occurs, the maximum likelihood estimates for these features predict a probability of 0 for all cases of y. \\n\\nLaplace smoothing allows a model to predict on out of training data features by adjusting the maximum likelihood estimates to be: \\n\\n\\n\\\\(\\\\phi_{j|y=1}= \\\\frac{\\\\Sigma_{i}^m 1(x_{j}^{(i)}=1 \\\\ \\\\bigcap y^{i} = 1) \\\\: + \\\\: 1}{\\\\Sigma_{i=1}^{m}(y^{(i)}=1 \\\\: + \\\\: 2}\\\\)\\n\\n\\\\(\\\\phi_{j|y=0}= \\\\frac{\\\\Sigma_{i}^m 1(x_{j}^{(i)}=1 \\\\ \\\\bigcap y^{i} = 0) \\\\: + \\\\: 1}{\\\\Sigma_{i=1}^{m}(y^{(i)}=0 \\\\: + \\\\: 2}\\\\)\\n\\nNote that in the general case where y takes on k values, there are k+1 modified parameter estimates, and they are added in when the denominator is k (rather than two, as shown in the two-level classifier shown here.)\\n\\nLaplace smoothing should be used with care; it is generally intended to allow for predictions in rare events. As prediction data becomes increasingly distinct from training data, train new models when possible to account for a broader set of possible X values. \\n\\n\\n### References\\n\\n\\n[Hastie, Trevor, Robert Tibshirani, and J Jerome H Friedman. The Elements of Statistical Learning. Vol.1. N.p., Springer New York, 2001.](http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf) \\n\\n[Ng, Andrew. \"Generative Learning algorithms.\" (2008).](http://cs229.stanford.edu/notes/cs229-notes2.pdf)\\n\\n---\\n\\n<a name=\"PCA\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## PCA\\n\\n### Introduction\\n\\nPrincipal Components Analysis (PCA) is closely related to Principal Components Regression. The algorithm is carried out on a set of possibly collinear features and performs a transformation to produce a new set of uncorrelated features.\\n\\nPCA is commonly used to model without regularization or perform dimensionality reduction. It can also be useful to carry out as a preprocessing step before distance-based algorithms such as K-Means since PCA guarantees that all dimensions of a manifold are orthogonal.\\n\\n### Defining a PCA Model\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons.\\n\\n- **ignore\\\\_const\\\\_cols**: Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default.   \\n\\n- **transform**: Select the transformation method for the training data: None, Standardize, Normalize, Demean, or Descale. The default is None. \\n\\n- **pca_method**: Select the algorithm to use for computing the principal components: \\n\\t- *GramSVD*: Uses a distributed computation of the Gram matrix, followed by a local SVD using the JAMA package\\n\\t- *Power*: Computes the SVD using the power iteration method (experimental)\\n\\t- *Randomized*: Uses randomized subspace iteration method \\n\\t- *GLRM*: Fits a generalized low-rank model with L2 loss function and no regularization and solves for the SVD using local matrix algebra (experimental)\\n\\n- **k***: Specify the rank of matrix approximation. The default is 1.  \\n\\n- **max_iterations**: Specify the number of training iterations. The value must be between 1 and 1e6 and the default is 1000.\\n\\n- **seed**: Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n- **use\\\\_all\\\\_factor\\\\_levels**: Check this checkbox to use all factor levels in the possible set of predictors; if you enable this option, sufficient regularization is required. By default, the first factor level is skipped. For PCA models, this option ignores the first factor level of each categorical column when expanding into indicator columns. \\n\\n- **compute\\\\_metrics**: Enable metrics computations on the training data. \\n\\n- **score\\\\_each\\\\_iteration**: (Optional) Check this checkbox to score during each iteration of the model training. \\n\\n- **max\\\\_runtime\\\\_secs**: Maximum allowed runtime in seconds for model training. Use 0 to disable.\\n\\n\\n\\n\\n### Interpreting a PCA Model\\n\\nPCA output returns a table displaying the number of components specified by the value for `k`.\\n\\nScree and cumulative variance plots for the components are returned as well. Users can access them by clicking on the black button labeled \"Scree and Variance Plots\" at the top left of the results page. A scree plot shows the variance of each component, while the cumulative variance plot shows the total variance accounted for by the set of components.\\n\\nThe output for PCA includes the following: \\n\\n- Model parameters (hidden)\\n- Output (model category, model summary, scoring history, training metrics, validation metrics, iterations)\\n- Archetypes\\n- Standard deviation\\n- Rotation \\n- Importance of components (standard deviation, proportion of variance, cumulative proportion) \\n\\n\\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during scoring?**\\n\\nFor the GramSVD and Power methods, all rows containing missing values are ignored during training. For the GLRM method, missing values are excluded from the sum over the loss function in the objective. For more information, refer to section 4 Generalized Loss Functions, equation (13), in [\"Generalized Low Rank Models\"](https://web.stanford.edu/~boyd/papers/pdf/glrm.pdf) by Boyd et al.\\n\\n  \\n- **How does the algorithm handle missing values during testing?**\\n\\n  During scoring, the test data is right-multiplied by the eigenvector matrix produced by PCA. Missing categorical values are skipped in the row product-sum. Missing numeric values propagate an entire row of NAs in the resulting projection matrix.\\n\\n\\n- **What happens during prediction if the new sample has categorical levels not seen in training?**\\n\\n  Categorical levels in the test data not present in the training data are skipped in the row product-sum.\\n\\n\\n- **Does it matter if the data is sorted?**\\n  \\n  No, sorting data does not affect the model. \\n  \\n- **Should data be shuffled before training?**\\n\\n  No, shuffling data does not affect the model. \\n\\n\\n- **What if there are a large number of columns?**\\n\\n  Calculating the SVD will be slower, since computations on the Gram matrix are handled locally. \\n\\n- **What if there are a large number of categorical factor levels?**\\n\\n  Each factor level (with the exception of the first, depending on whether **use\\\\_all\\\\_factor\\\\_levels** is enabled) is assigned an indicator column. The indicator column is 1 if the observation corresponds to a particular factor; otherwise, it is 0. As a result, many factor levels result in a large Gram matrix and slower computation of the SVD. \\n\\n- **How are categorical columns handled during model building?**\\n  \\n  If the GramSVD or Power methods are used, the categorical columns are expanded into 0/1 indicator columns for each factor level. The algorithm is then performed on this expanded training frame. For GLRM, the multidimensional loss function for categorical columns is discussed in Section 6.1 of [\"Generalized Low Rank Models\"](https://web.stanford.edu/~boyd/papers/pdf/glrm.pdf) by Boyd et al.\\n\\n- **When running PCA, is it better to create a cluster that uses many smaller nodes or fewer larger nodes?** \\n\\nFor PCA, this is dependent on the selected `pca_method` parameter: \\n\\n- For **GramSVD**, use fewer larger nodes for better performance. Forming the Gram matrix requires few intensive calculations and the main bottleneck is the JAMA library\\'s SVD function, which is not parallelized and runs on a single machine. We do not recommend selecting GramSVD for datasets with many columns and/or categorical levels in one or more columns. \\n- For **Randomized**, use many smaller nodes for better performance, since H2O calls a few different distributed tasks in a loop, where each task does fairly simple matrix algebra computations. \\n- For **GLRM**, the number of nodes depends on whether the dataset contains many categorical columns with many levels. If this is the case, we recommend using fewer larger nodes, since computing the loss function for categoricals is an intensive task. If the majority of the data is numeric and the categorical columns have only a small number of levels (~10-20), we recommend using many small nodes in the cluster.\\n- For **Power**, we recommend using fewer larger nodes because the intensive calculations are single-threaded. However, this method is only recommended for obtaining principal component values (such as `k << ncol(train))` because the other methods are far more efficient. \\n\\n- **I ran PCA on my dataset - how do I input the new parameters into a model?**\\n\\nAfter the PCA model has been built using `h2o.prcomp`, use `h2o.predict` on the original data frame and the PCA model to produce the dimensionality-reduced representation. Use `cbind` to add the predictor column from the original data frame to the data frame produced by the output of `h2o.predict`. At this point, you can build supervised learning models on the new data frame. \\n\\n\\n### PCA Algorithm\\n\\nLet \\\\(X\\\\) be an \\\\(M\\\\times N\\\\) matrix where\\n \\n- Each row corresponds to the set of all measurements on a particular \\n   attribute, and \\n\\n- Each column corresponds to a set of measurements from a given\\n   observation or trial\\n\\nThe covariance matrix \\\\(C_{x}\\\\) is\\n\\n\\\\(C_{x}=\\\\frac{1}{n}XX^{T}\\\\)\\n\\nwhere \\\\(n\\\\) is the number of observations. \\n\\n\\\\(C_{x}\\\\) is a square, symmetric \\\\(m\\\\times m\\\\) matrix, the diagonal entries of which are the variances of attributes, and the off-diagonal entries are covariances between attributes. \\n\\nPCA convergence is based on the method described by Gockenbach: \"The rate of convergence of the power method depends on the ratio \\\\(lambda_2|/|\\\\lambda_1\\\\). If this is small...then the power method converges rapidly. If the ratio is close to 1, then convergence is quite slow. The power method will fail if \\\\(lambda_2| = |\\\\lambda_1\\\\).\" (567). \\n\\n\\nThe objective of PCA is to maximize variance while minimizing covariance. \\n\\nTo accomplish this, for a new matrix \\\\(C_{y}\\\\) with off diagonal entries of 0, and each successive dimension of Y ranked according to variance, PCA finds an orthonormal matrix \\\\(P\\\\) such that \\\\(Y=PX\\\\) constrained by the requirement that \\\\(C_{y}=\\\\frac{1}{n}YY^{T}\\\\) be a diagonal matrix. \\n\\nThe rows of \\\\(P\\\\) are the principal components of X.\\n\\n\\\\(C_{y}=\\\\frac{1}{n}YY^{T}\\\\)\\n\\\\(=\\\\frac{1}{n}(PX)(PX)^{T}\\\\)\\n\\\\(C_{y}=PC_{x}P^{T}.\\\\)\\n\\nBecause any symmetric matrix is diagonalized by an orthogonal matrix of its eigenvectors, solve matrix \\\\(P\\\\) to be a matrix where each row is an eigenvector of \\n\\\\(\\\\frac{1}{n}XX^{T}=C_{x}\\\\)\\n\\nThen the principal components of \\\\(X\\\\) are the eigenvectors of \\\\(C_{x}\\\\), and the \\\\(i^{th}\\\\) diagonal value of \\\\(C_{y}\\\\) is the variance of \\\\(X\\\\) along \\\\(p_{i}\\\\). \\n\\nEigenvectors of \\\\(C_{x}\\\\) are found by first finding the eigenvalues \\\\(\\\\lambda\\\\) of \\\\(C_{x}\\\\).\\n\\nFor each eigenvalue \\\\(\\\\lambda\\\\) \\\\((C-{x}-\\\\lambda I)x =0\\\\) where \\\\(x\\\\) is the eigenvector associated with \\\\(\\\\lambda\\\\). \\n\\nSolve for \\\\(x\\\\) by Gaussian elimination. \\n\\n#### Recovering SVD from GLRM\\n\\nGLRM gives \\\\(x\\\\)  and \\\\(y\\\\), where \\\\(x \\\\in \\\\rm \\\\Bbb I \\\\!\\\\Bbb R ^{n * k}\\\\) and \\\\( y \\\\in \\\\rm \\\\Bbb I \\\\!\\\\Bbb R ^{k*m} \\\\)\\n\\n&nbsp;&nbsp;&nbsp;- \\\\(n\\\\)= number of rows (A)\\n\\n&nbsp;&nbsp;&nbsp;- \\\\(m\\\\)= number of columns (A)\\n\\n&nbsp;&nbsp;&nbsp;- \\\\(k\\\\)= user-specified rank\\n&nbsp;&nbsp;&nbsp;- \\\\(A\\\\)= training matrix\\n\\nIt is assumed that the \\\\(x\\\\) and \\\\(y\\\\) columns are independent. \\n\\nFirst, perform QR decomposition of \\\\(x\\\\) and \\\\(y^T\\\\): \\n\\n&nbsp;&nbsp;&nbsp;\\\\(x = QR\\\\) \\n\\n&nbsp;&nbsp;&nbsp; \\\\(y^T = ZS\\\\), where \\\\(Q^TQ = I = Z^TZ\\\\)\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Call JAMA QR Decomposition directly on \\\\(y^T\\\\) to get \\\\( Z \\\\in \\\\rm \\\\Bbb I \\\\! \\\\Bbb R\\\\), \\\\( S \\\\in \\\\Bbb I \\\\! \\\\Bbb R \\\\)\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\\\( R \\\\) from QR decomposition of \\\\( x \\\\) is the upper triangular factor of Cholesky of \\\\(X^TX\\\\) Gram\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\\\( X^TX = LL^T, X = QR \\\\)\\n\\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\\\( X^TX= (R^TQ^T) QR = R^TR \\\\), since \\\\(Q^TQ=I \\\\) => \\\\(R=L^T\\\\) (transpose lower triangular)\\n\\n**Note**: In code, \\\\(X^TX \\\\over n\\\\) = \\\\( LL^T \\\\)\\n\\n&nbsp;&nbsp;&nbsp;\\\\( X^TX = (L \\\\sqrt{n})(L \\\\sqrt{n})^T =R^TR \\\\)\\n\\n&nbsp;&nbsp;&nbsp;\\\\( R = L^T \\\\sqrt{n} \\\\in \\\\rm \\\\Bbb I \\\\! \\\\Bbb R^{k * k} \\\\) reduced QR decomposition. \\n\\nFor more information, refer to the [Rectangular matrix](https://en.wikipedia.org/wiki/QR_decomposition#Rectangular_matrix) section of \"QR Decomposition\" on Wikipedia. \\n\\n\\\\( XY = QR(ZS)^T = Q(RS^T)Z^T \\\\)\\n\\n**Note**: \\\\( (RS^T) \\\\in \\\\rm \\\\Bbb I \\\\!\\\\Bbb R \\\\)\\n\\nFind SVD (locally) of \\\\( RS^T \\\\)\\n\\n\\\\( RS^T = U \\\\sum V^T, U^TU = I = V^TV \\\\) orthogonal \\n\\n\\\\( XY = Q(RS^T)Z^T = (QU \\\\sum (V^T Z^T) SVD \\\\)\\n\\n&nbsp;&nbsp;&nbsp;\\\\( (QU)^T(QU) = U^T Q^TQU U^TU = I\\\\)\\n\\n&nbsp;&nbsp;&nbsp;\\\\( (ZV)^T(ZV) = V^TZ^TZV = V^TV =I \\\\)\\n\\nRight singular vectors: \\\\( ZV \\\\in \\\\rm \\\\Bbb I \\\\!\\\\Bbb R^{m * k} \\\\)\\n\\nSingular values: \\\\( \\\\sum \\\\in \\\\rm \\\\Bbb I \\\\!\\\\Bbb R^{k * k} \\\\) diagonal\\n\\nLeft singular vectors: \\\\( (QU) \\\\in \\\\rm \\\\Bbb I \\\\!\\\\Bbb R^{n * k}\\\\)\\n\\n\\n\\n### References\\n\\nGockenbach, Mark S. \"Finite-Dimensional Linear Algebra (Discrete Mathematics and Its Applications).\" (2010): 566-567. \\n\\n\\n---\\n\\n<a name=\"GBM\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## GBM\\n\\n### Introduction\\n\\nGradient Boosted Regression and Gradient Boosted Classification are forward learning ensemble methods. The guiding heuristic is that good predictive results can be obtained through increasingly refined approximations. H2O\\'s GBM sequentially builds regression trees on all the features of the dataset in a fully distributed way - each tree is built in parallel.\\n\\nThe current version of GBM is fundamentally the same as in previous versions of H2O (same algorithmic steps, same histogramming techniques), with the exception of the following changes: \\n\\n- Improved ability to train on categorical variables (using the `nbins_cats` parameter)\\n- Minor changes in histogramming logic for some corner cases\\n\\nThere was some code cleanup and refactoring to support the following features:\\n\\n- Per-row observation weights\\n- Per-row offsets\\n- N-fold cross-validation\\n- Support for more distribution functions (such as Gamma, Poisson, and Tweedie)\\n\\n### Defining a GBM Model\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **nfolds**: Specify the number of folds for cross-validation. \\n\\n- **response_column**: (Required) Select the column to use as the independent variable. The data can be numeric or categorical. \\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **ntrees**: Specify the number of trees.  \\n\\n- **max\\\\_depth**: Specify the maximum tree depth.  \\n\\n- **min\\\\_rows**: Specify the minimum number of observations for a leaf (`nodesize` in R).   \\n\\n- **nbins**: (Numerical/real/int only) Specify the number of bins for the histogram to build, then split at the best point.\\n\\n- **max\\\\_abs\\\\_leafnode\\\\_pred**: When building a GBM classification model, this option reduces overfitting by limiting the maximum absolute value of a leaf node prediction. This option defaults to Double.MAX_VALUE.\\n\\n- **nbins_cats**: (Categorical/enums only) Specify the maximum number of bins for the histogram to build, then split at the best point. Higher values can lead to more overfitting.  The levels are ordered alphabetically; if there are more levels than bins, adjacent levels share bins. This value has a more significant impact on model fitness than **nbins**. Larger values may increase runtime, especially for deep trees and large clusters, so tuning may be required to find the optimal value for your configuration.  \\n\\n- **seed**: Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n- **learn_rate**: Specify the learning rate. The range is 0.0 to 1.0. \\n\\n- **learn\\\\_rate\\\\_annealing**: Specifies to reduce the **learn_rate** by this factor after every tree. So for *N* trees, GBM starts with **learn_rate** and ends with **learn_rate** * **learn\\\\_rate\\\\_annealing**^*N*. For example, instead of using **learn_rate=0.01**, you can now try **learn_rate=0.05** and **learn\\\\_rate\\\\_annealing=0.99**. This method would converge much faster with almost the same accuracy. Use caution not to overfit. \\n\\n- **distribution**: Select the loss function. The options are auto, bernoulli, multinomial, gaussian, poisson, gamma, or tweedie.  \\n\\n\\t> - If the distribution is **multinomial**, the response column must be categorical.\\n\\t> - If the distribution is **poisson**, the response column must be numeric.\\n\\t> - If the distribution is **gamma**, the response column must be  numeric. \\n\\t> - If the distribution is **tweedie**, the response column must be numeric. \\n\\t> - If the distribution is **gaussian**, the response column must be numeric. \\n\\t> - If the distribution is **laplace**, the data must be numeric and continuous (**Int**). \\n\\t> - If the distribution is **quantile**, the data must be numeric and continuous (**Int**). \\n\\n\\n- **sample_rate**: Specify the row sampling rate (x-axis). The range is 0.0 to 1.0. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled. For details, refer to \"Stochastic Gradient Boosting\" ([Friedman, 1999](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)). If this option is specified along with **sample\\\\_rate_per\\\\_class**, then only the first option that GBM encounters will be used. \\n\\n- **sample\\\\_rate_per\\\\_class**: When building models from imbalanced datasets, this option specifies that each tree in the ensemble should sample from the full training dataset using a per-class-specific sampling rate rather than a global sample factor (as with `sample_rate`). The range for this option is 0.0 to 1.0. If this option is specified along with **sample_rate**, then only the first option that GBM encounters will be used.\\n\\n- **col\\\\_sample_rate**: Specify the column sampling rate (y-axis). The range is 0.0 to 1.0. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled. For details, refer to \"Stochastic Gradient Boosting\" ([Friedman, 1999](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)). \\n\\n- **col\\\\_sample_rate\\\\_change\\\\_per\\\\_level**: This option specifies to change the column sampling rate as a function of the depth in the tree. For example:\\n\\t>level 1: **col\\\\_sample_rate**\\n\\t\\n\\t>level 2: **col\\\\_sample_rate** * **factor**\\n\\t\\n\\t>level 3: **col\\\\_sample_rate** * **factor^2**\\n\\t\\n\\t>level 4: **col\\\\_sample_rate** * **factor^3**\\n\\t\\n\\t>etc. \\n\\n- **min\\\\_split_improvement**: The value of this option specifies the minimum relative improvement in squared error reduction in order for a split to happen. When properly tuned, this option can help reduce overfitting. Optimal values would be in the 1e-10...1e-3 range.  \\n\\n- **histogram_type**: By default (AUTO) GBM bins from min...max in steps of (max-min)/N. Random split points or quantile-based split points can be selected as well. RoundRobin can be specified to cycle through all histogram types (one per tree). Use this option to specify the type of histogram to use for finding optimal split points:\\n\\n  - AUTO\\n  - UniformAdaptive\\n  - Random\\n  - QuantilesGlobal\\n  - RoundRobin\\n\\n- **score\\\\_each\\\\_iteration**: (Optional) Check this checkbox to score during each iteration of the model training. \\n\\n- **score\\\\_tree\\\\_interval**: Score the model after every so many trees. Disabled if set to 0.\\n\\n- **fold_assignment**: (Applicable only if a value for **nfolds** is specified and **fold_column** is not selected) Select the cross-validation fold assignment scheme. The available options are AUTO (which is Random), Random, or [Modulo](https://en.wikipedia.org/wiki/Modulo_operation).  \\n \\n- **fold_column**: Select the column that contains the cross-validation fold index assignment per observation. \\n\\n- **offset_column**: (Not applicable if the **distribution** is **multinomial**) Select a column to use as the offset. \\n\\t>*Note*: Offsets are per-row \"bias values\" that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. For more information, refer to the following [link](http://www.idg.pl/mirrors/CRAN/web/packages/gbm/vignettes/gbm.pdf). If the **distribution** is **Bernoulli**, the value must be less than one.\\n\\n- **weights_column**: Select a column to use for the observation weights, which are used for bias correction. The specified `weights_column` must be included in the specified `training_frame`. *Python only*: To use a weights column when passing an H2OFrame to `x` instead of a list of column names, the specified `training_frame` must contain the specified `weights_column`. \\n\\t>*Note*: Weights are per-row observation weights and do not increase the size of the data frame. This is typically the number of times a row is repeated, but non-integer values are supported as well. During training, rows with higher weights matter more, due to the larger loss function pre-factor.  \\n\\n- **balance_classes**: Oversample the minority classes to balance the class distribution. This option is not selected by default and can increase the data frame size. This option is only applicable for classification. Majority classes can be undersampled to satisfy the **Max\\\\_after\\\\_balance\\\\_size** parameter.\\n\\n- **max\\\\_confusion\\\\_matrix\\\\_size**: Specify the maximum size (in number of classes) for confusion matrices to be printed in the Logs. \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: Specify the maximum number (top K) of predictions to use for hit ratio computation. Applicable to multi-class only. To disable, enter 0. \\n\\n\\n- **r2_stopping**: Specify a threshold for the coefficient of determination (\\\\(r^2\\\\)) metric value. When this threshold is met or exceeded, H2O stops making trees.   \\n\\n- **stopping\\\\_rounds**: Stops training when the option selected for **stopping\\\\_metric** doesn\\'t improve for the specified number of training rounds, based on a simple moving  average. To disable this feature, specify `0`. The metric is computed on the validation data (if provided); otherwise, training data is used. When used with **overwrite\\\\_with\\\\_best\\\\_model**, the final model is the best model generated for the given **stopping\\\\_metric** option.   \\n\\t>**Note**: If cross-validation is enabled: \\n\\t1. All cross-validation models stop training when the validation metric doesn\\'t improve. \\n    2. The main model runs for the mean number of epochs. \\n    3. N+1 models do *not* use **overwrite\\\\_with\\\\_best\\\\_model**\\n    4. N+1 models may be off by the number specified for **stopping\\\\_rounds** from the best model, but the cross-validation metric estimates the performance of the main model for the resulting number of epochs (which may be fewer than the specified number of epochs). \\n\\n- **stopping\\\\_metric**: Select the metric to use for early stopping. The available options are: \\n\\t\\n    - **AUTO**: Logloss for classification; deviance for regression\\n    - **deviance**\\n    - **logloss**\\n    - **MSE**\\n    - **AUC**\\n    - **r2**\\n    - **misclassification**\\n    - **mean\\\\_per\\\\_class\\\\_error**\\n\\n- **stopping\\\\_tolerance**: Specify the relative tolerance for the metric-based stopping to stop training if the improvement is less than this value. \\n\\n- **max\\\\_runtime\\\\_secs**: Maximum allowed runtime in seconds for model training. Use 0 to disable.\\n\\n- **build\\\\_tree\\\\_one\\\\_node**: To run on a single node, check this checkbox. This is suitable for small datasets as there is no network overhead but fewer CPUs are used.\\n\\n- **quantile_alpha**: (Only applicable if *Quantile* is selected for **distribution**) Specify the quantile to be used for Quantile Regression.\\n\\n- **tweedie_power**: (Only applicable if *Tweedie* is selected for **distribution**) Specify the Tweedie power. The range is from 1 to 2. For a normal distribution, enter `0`. For Poisson distribution, enter `1`. For a gamma distribution, enter `2`. For a compound Poisson-gamma distribution, enter a value greater than 1 but less than 2. For more information, refer to [Tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution). \\n\\n- **checkpoint**: Enter a model key associated with a previously-trained model. Use this option to build a new model as a continuation of a previously-generated model.\\n\\n- **keep\\\\_cross\\\\_validation\\\\_models**: To keep the cross-validation models, check this checkbox. \\n\\n- **keep\\\\_cross\\\\_validation\\\\_predictions**: To keep the cross-validation predictions, check this checkbox. \\n\\n- **class\\\\_sampling\\\\_factors**: Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance. There is no default value. \\n\\n- **max\\\\_after\\\\_balance\\\\_size**: Specify the maximum relative size of the training data after balancing class counts (**balance\\\\_classes** must be enabled). The value can be less than 1.0. \\n\\n- **nbins\\\\_top\\\\_level**: (For numerical/real/int columns only) Specify the minimum number of bins at the root level to use to build the histogram. This number will then be decreased by a factor of two per level.  \\n\\n\\n### Interpreting a GBM Model\\n\\nThe output for GBM includes the following: \\n\\n- Model parameters (hidden)\\n- A graph of the scoring history (training MSE vs number of trees)\\n- A graph of the variable importances\\n- Output (model category, validation metrics, initf)\\n- Model summary (number of trees, min. depth, max. depth, mean depth, min. leaves, max. leaves, mean leaves)\\n- Scoring history in tabular format\\n- Training metrics (model name, model checksum name, frame name, description, model category, duration in ms, scoring time, predictions, MSE, R2)\\n- Variable importances in tabular format\\n\\n### Leaf Node Assignment\\nTrees cluster observations into leaf nodes, and this information can be useful for feature engineering or model interpretability. Use **h2o.predict\\\\_leaf\\\\_node\\\\_assignment\\\\(model, frame\\\\)** to get an H2OFrame with the leaf node assignments, or click the checkbox when making predictions from Flow. Those leaf nodes represent decision rules that can be fed to other models (i.e., GLM with lambda search and strong rules) to obtain a limited set of the most important rules.\\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during training?**\\n\\n  Missing values are interpreted as containing information (i.e., missing for a reason), rather than missing at random. During tree building, split decisions for every node are found by minimizing the loss function and treating missing values as a separate category that can go either left or right.\\n\\n- **How does the algorithm handle missing values during testing?**\\n\\n  During scoring, missing values follow the optimal path that was determined for them during training (minimized loss function).\\n\\n- **What happens if the response has missing values?**\\n\\n  No errors will occur, but nothing will be learned from rows containing missing the response.\\n\\n-  **What happens when you try to predict on a categorical level not seen during training?**\\n\\n  GBM converts a new categorical level to an \"undefined\" value in the test set, and then splits either left or right during scoring.  \\n\\n- **Does it matter if the data is sorted?** \\n\\n  No.\\n\\n- **Should data be shuffled before training?**\\n\\n  No.\\n\\n- **How does the algorithm handle highly imbalanced data in a response column?**\\n\\n  You can specify `balance_classes`, `class_sampling_factors` and `max_after_balance_size` to control over/under-sampling.\\n\\n- **What if there are a large number of columns?**\\n\\n  DRF models are best for datasets with fewer than a few thousand columns.\\n\\n- **What if there are a large number of categorical factor levels?**\\n\\n  Large numbers of categoricals are handled very efficiently - there is never any one-hot encoding.\\n\\n- **Given the same training set and the same GBM parameters, will GBM produce a different model with two different validation data sets, or the same model?**\\n\\n  The same model will be generated. \\n\\n- **How deterministic is GBM?**\\n\\n  The `nfolds` and `balance_classes` parameters use the seed directly. Otherwise, GBM is deterministic up to floating point rounding errors (out-of-order atomic addition of multiple threads during histogram building). Any observed variations in the AUC curve should be the same up to at least three to four significant digits. \\n\\n- **When fitting a random number between 0 and 1 as a single feature, the training ROC curve is consistent with `random` for low tree numbers and overfits as the number of trees is increased, as expected. However, when a random number is included as part of a set of hundreds of features, as the number of trees increases, the random number increases in feature importance. Why is this?**\\n \\n  This is a known behavior of GBM that is similar to its behavior in R. If, for example, it takes 50 trees to learn all there is to learn from a frame without the random features, when you add a random predictor and train 1000 trees, the first 50 trees will be approximately the same. The final 950 trees are used to make sense of the random number, which will take a long time since there\\'s no structure. The variable importance will reflect the fact that all the splits from the first 950 trees are devoted to the random feature. \\n\\n- **How is column sampling implemented for GBM?**\\n\\n  For an example model using: \\n\\n  - 100 columns\\n  - `col_sample_rate_per_tree=0.754`\\n  - `col_sample_rate=0.8` (refers to available columns after per-tree sampling)\\n\\n  For each tree, the floor is used to determine the number - in this example, (0.754*100)=75 out of the 100 - of columns that are randomly picked, and then the floor is used to determine the number - in this case, (0.754*0.8*100)=60 - of columns that are then randomly chosen for each split decision (out of the 75).\\n\\n- **I want to score multiple models on a huge dataset. Is it possible to score these models in parallel?**\\n\\n  The best way to score models in parallel is to use the in-H2O binary models. To do this, import the binary (non-POJO, previously exported) model into an H2O cluster; import the datasets into H2O as well; call the predict endpoint either from R, Python, Flow or the REST API directly; then export the predictions to file or download them from the server.\\n\\n- **Are there any tutorials for GBM?**\\n\\n You can find tutorials for using GBM with R, Python, and Flow at the following location: <a href=\"https://github.com/h2oai/h2o-3/tree/master/h2o-docs/src/product/tutorials/gbm\" target=\"_blank\">https://github.com/h2oai/h2o-3/tree/master/h2o-docs/src/product/tutorials/gbm</a>\\n\\n### GBM Algorithm \\n\\nH2O\\'s Gradient Boosting Algorithms follow the algorithm specified by Hastie et al (2001):\\n\\n\\nInitialize \\\\(f_{k0} = 0,\\\\: k=1,2,,K\\\\)\\n\\nFor \\\\(m=1\\\\) to \\\\(M:\\\\)\\n  \\n   &nbsp;&nbsp;(a) Set \\\\(p_{k}(x)=\\\\frac{e^{f_{k}(x)}}{\\\\sum_{l=1}^{K}e^{f_{l}(x)}},\\\\:k=1,2,,K\\\\)\\n\\n   &nbsp;&nbsp;(b) For \\\\(k=1\\\\) to \\\\(K\\\\):\\n\\n   &nbsp;&nbsp;&nbsp;&nbsp;i. Compute \\\\(r_{ikm}=y_{ik}-p_{k}(x_{i}),\\\\:i=1,2,,N.\\\\)\\n\\t&nbsp;&nbsp;&nbsp;&nbsp;ii. Fit a regression tree to the targets \\\\(r_{ikm},\\\\:i=1,2,,N\\\\), giving terminal regions \\\\(R_{jim},\\\\:j=1,2,,J_{m}.\\\\)\\n   \\\\(iii. Compute\\\\) \\\\(\\\\gamma_{jkm}=\\\\frac{K-1}{K}\\\\:\\\\frac{\\\\sum_{x_{i}\\\\in R_{jkm}}(r_{ikm})}{\\\\sum_{x_{i}\\\\in R_{jkm}}|r_{ikm}|(1-|r_{ikm})},\\\\:j=1,2,,J_{m}.\\\\)\\n\\t\\\\(\\\\:iv.\\\\:Update\\\\:f_{km}(x)=f_{k,m-1}(x)+\\\\sum_{j=1}^{J_{m}}\\\\gamma_{jkm}I(x\\\\in\\\\:R_{jkm}).\\\\)\\t      \\n\\nOutput \\\\(\\\\:\\\\hat{f_{k}}(x)=f_{kM}(x),\\\\:k=1,2,,K.\\\\) \\n\\nBe aware that the column type affects how the histogram is created and the column type depends on whether rows are excluded or assigned a weight of 0. For example:\\n\\nval weight\\n1      1\\n0.5    0\\n5      1\\n3.5    0\\n\\nThe above vec has a real-valued type if passed as a whole, but if the zero-weighted rows are sliced away first, the integer weight is used. The resulting histogram is either kept at full `nbins` resolution or potentially shrunk to the discrete integer range, which affects the split points. \\n\\nFor more information about the GBM algorithm, refer to the [Gradient Boosted Machines booklet](http://h2o.ai/resources). \\n\\n\\n### Binning In GBM\\n\\n**Is the binning range-based or percentile-based?**\\n\\nIt\\'s range based, and re-binned at each tree split.\\nNAs always \"go to the left\" (smallest) bin.\\nThere\\'s a minimum observations required value (default 10).\\nThere has to be at least 1 FP ULP improvement in error to split (all-constant predictors won\\'t split).\\nnbins is at least 1024 at the top-level, and divides by 2 down each level until you hit the nbins parameter (default: 20).\\nCategoricals use a separate, more aggressive, binning range.\\n\\nRe-binning means, eg, suppose your column C1 data is: {1,1,2,4,8,16,100,1000}.\\nThen a 20-way binning will use the range from 1 to 1000, bin by units of 50.\\nThe first binning will be a lumpy: {1,1,2,4,8,16},{100},{47_empty_bins},{1000}.  Suppose the split peels out the {1000} bin from the rest.\\n\\nNext layer in the tree for the left-split has value from 1 to 100 (not 1000!) and so re-bins in units of 5:  {1,1,2,4},{8},{},{16},{lots of empty bins}{100}\\n(the RH split has the single value 1000).\\n\\nAnd so on: important dense ranges with split essentially logrithmeticaly at each layer.\\n\\n**What should I do if my variables are long skewed in the tail and might have large outliers?**\\n\\nYou can try adding a new predictor column which is either pre-binned (e.g. as a categorical - \"small\", \"median\", and \"giant\" values), or a log-transform - plus keep the old column.\\n\\n### References\\n\\nDietterich, Thomas G, and Eun Bae Kong. \"Machine Learning Bias,\\nStatistical Bias, and Statistical Variance of Decision Tree\\nAlgorithms.\" ML-95 255 (1995).\\n\\nElith, Jane, John R Leathwick, and Trevor Hastie. \"A Working Guide to\\nBoosted Regression Trees.\" Journal of Animal Ecology 77.4 (2008): 802-813\\n\\nFriedman, Jerome H. \"Greedy Function Approximation: A Gradient\\nBoosting Machine.\" Annals of Statistics (2001): 1189-1232.\\n\\nFriedman, Jerome, Trevor Hastie, Saharon Rosset, Robert Tibshirani,\\nand Ji Zhu. \"Discussion of Boosting Papers.\" Ann. Statist 32 (2004): \\n102-107\\n\\n[Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. \"Additive\\nLogistic Regression: A Statistical View of Boosting (With Discussion\\nand a Rejoinder by the Authors).\" The Annals of Statistics 28.2\\n(2000): 337-407](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223)\\n\\n[Hastie, Trevor, Robert Tibshirani, and J Jerome H Friedman. The\\nElements of Statistical Learning.\\nVol.1. N.p., page 339: Springer New York, 2001.](http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf)\\n\\n---\\n\\n<a name=\"DL\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## Deep Learning\\n\\n### Introduction\\n\\nH2Os Deep Learning is based on a multi-layer feed-forward artificial neural network that is trained with stochastic gradient descent using back-propagation. The network can contain a large number of hidden layers consisting of neurons with tanh, rectifier and maxout activation functions. Advanced features such as adaptive learning rate, rate annealing, momentum training, dropout, L1 or L2 regularization, checkpointing and grid search enable high predictive accuracy. Each compute node trains a copy of the global model parameters on its local data with multi-threading (asynchronously), and contributes periodically to the global model via model averaging across the network.\\n\\n### Defining a Deep Learning Model\\n\\nH2O Deep Learning models have many input parameters, many of which are only accessible via the expert mode. For most cases, use the default values. Please read the following instructions before building extensive Deep Learning models. The application of grid search and successive continuation of winning models via checkpoint restart is highly recommended, as model performance can vary greatly.\\n\\n- **model_id**: (Optional) Enter a custom name for the model to use as a reference. By default, H2O automatically generates a destination key. \\n\\n- **training_frame**: (Required) Select the dataset used to build the model. \\n**NOTE**: If you click the **Build a model** button from the `Parse` cell, the training frame is entered automatically. \\n\\n- **validation_frame**: (Optional) Select the dataset used to evaluate the accuracy of the model. \\n\\n- **nfolds**: Specify the number of folds for cross-validation. \\n\\t>**Note**: Cross-validation is not supported when autoencoder is enabled.   \\n\\n- **response_column**: Select the column to use as the independent variable. The data can be numeric or categorical. \\n\\n- **ignored_columns**: (Optional) Click the checkbox next to a column name to add it to the list of columns excluded from the model. To add all columns, click the **All** button. To remove a column from the list of ignored columns, click the X next to the column name. To remove all columns from the list of ignored columns, click the **None** button. To search for a specific column, type the column name in the **Search** field above the column list. To only show columns with a specific percentage of missing values, specify the percentage in the **Only show columns with more than 0% missing values** field. To change the selections for the hidden columns, use the **Select Visible** or **Deselect Visible** buttons. \\n\\n- **ignore\\\\_const\\\\_cols**: Check this checkbox to ignore constant training columns, since no information can be gained from them. This option is selected by default. \\n\\n- **activation**: Select the activation function (Tanh, Tanh with dropout, Rectifier, Rectifier with dropout, Maxout, Maxout with dropout).\\n\\t> - **Maxout** is not supported when **autoencoder** is enabled.    \\n\\n- **hidden**: Specify the hidden layer sizes (e.g., 100,100). The value must be positive.   \\n\\n- **epochs**: Specify the number of times to iterate (stream) the dataset. The value can be a fraction.   \\n\\n- **variable_importances**: Check this checkbox to compute variable importance. This option is not selected by default. \\n\\n- **fold_assignment**: (Applicable only if a value for **nfolds** is specified and **fold_column** is not selected) Select the cross-validation fold assignment scheme. The available options are AUTO (which is Random), Random, or [Modulo](https://en.wikipedia.org/wiki/Modulo_operation).  \\n\\n- **fold_column**: Select the column that contains the cross-validation fold index assignment per observation. \\n\\n- **weights_column**: Select a column to use for the observation weights, which are used for bias correction. The specified `weights_column` must be included in the specified `training_frame`. *Python only*: To use a weights column when passing an H2OFrame to `x` instead of a list of column names, the specified `training_frame` must contain the specified `weights_column`. \\n\\t>*Note*: Weights are per-row observation weights. This is typically the number of times a row is repeated, but non-integer values are supported as well. During training, rows with higher weights matter more, due to the larger loss function pre-factor.  \\n\\n- **offset_column**: (Applicable for regression only) Select a column to use as the offset. \\n\\t>*Note*: Offsets are per-row \"bias values\" that are used during model training. For Gaussian distributions, they can be seen as simple corrections to the response (y) column. Instead of learning to predict the response (y-row), the model learns to predict the (row) offset of the response column. For other distributions, the offset corrections are applied in the linearized space before applying the inverse link function to get the actual response values. For more information, refer to the following [link](http://www.idg.pl/mirrors/CRAN/web/packages/gbm/vignettes/gbm.pdf). \\n\\n- **balance_classes**: (Applicable for classification only) Oversample the minority classes to balance the class distribution. This option is not selected by default and can increase the data frame size. This option is only applicable for classification. Majority classes can be undersampled to satisfy the **Max\\\\_after\\\\_balance\\\\_size** parameter. \\n\\n- **standardize**: If enabled, automatically standardize the data (mean 0, variance 0). If disabled, the user must provide properly scaled input data.\\n\\n- **max\\\\_confusion\\\\_matrix\\\\_size**: Specify the maximum size (in number of classes) for confusion matrices to be printed in the Logs. \\n\\n- **max\\\\_hit\\\\_ratio\\\\_k**: Specify the maximum number (top K) of predictions to use for hit ratio computation. Applicable to multi-class only. To disable, enter 0. \\n\\n- **checkpoint**: Enter a model key associated with a previously-trained Deep Learning model. Use this option to build a new model as a continuation of a previously-generated model.\\n\\t>**Note**: Cross-validation is not supported during checkpoint restarts.  \\n\\n- **use\\\\_all\\\\_factor\\\\_levels**: Check this checkbox to use all factor levels in the possible set of predictors; if you enable this option, sufficient regularization is required. By default, the first factor level is skipped. For Deep Learning models, this option is useful for determining variable importances and is automatically enabled if the autoencoder is selected. \\n\\n- **train\\\\_samples\\\\_per\\\\_iteration**: Specify the number of global training samples per MapReduce iteration. To specify one epoch, enter 0. To specify all available data (e.g., replicated training data), enter -1. To use the automatic values, enter -2.   \\n\\n- **adaptive_rate**: Check this checkbox to enable the adaptive learning rate (ADADELTA). This option is selected by default. \\n\\n- **input\\\\_dropout\\\\_ratio**: Specify the input layer dropout ratio to improve generalization. Suggested values are 0.1 or 0.2.  \\n\\n- **hidden\\\\_dropout\\\\_ratios**: (Applicable only if the activation type is **TanhWithDropout**, **RectifierWithDropout**, or **MaxoutWithDropout**) Specify the hidden layer dropout ratio to improve generalization. Specify one value per hidden layer. The range is >= 0 to <1 and the default is 0.5. \\n\\n- **l1**: Specify the L1 regularization to add stability and improve generalization; sets the value of many weights to 0. \\n\\n- **l2**: Specify the L2 regularization to add stability and improve generalization; sets the value of many weights to smaller values. \\n\\n- **loss**:  Select the loss function. The options are Automatic,  CrossEntropy, Quadratic, Huber, or Absolute and the default value is Automatic. \\n\\t> - Use **Absolute**, **Quadratic**, or **Huber** for regression\\n\\t> - Use  **Absolute**, **Quadratic**, **Huber**, or **CrossEntropy** for classification\\n\\n- **distribution**:  Select the distribution type from the drop-down list. The options are auto, bernoulli, multinomial, gaussian, poisson, gamma, laplace, quantile or tweedie.\\n\\n- **quantile_alpha**: (Only applicable if *Quantile* is selected for **distribution**) Specify the quantile to be used for Quantile Regression.\\n\\n- **tweedie_power**: (Only applicable if *Tweedie* is selected for **distribution**) Specify the Tweedie power. The range is from 1 to 2. For a normal distribution, enter `0`. For Poisson distribution, enter `1`. For a gamma distribution, enter `2`. For a compound Poisson-gamma distribution, enter a value greater than 1 but less than 2. For more information, refer to [Tweedie distribution](https://en.wikipedia.org/wiki/Tweedie_distribution). \\n\\n- **score_interval**: Specify the shortest time interval (in seconds) to wait between model scoring.  \\n\\n- **score\\\\_training\\\\_samples**: Specify the number of training set samples for scoring. The value must be >= 0. To use all training samples, enter 0.  \\n\\n- **score\\\\_validation\\\\_samples**: (Applicable only if **validation\\\\_frame** is specified) Specify the number of validation set samples for scoring. The value must be >= 0. To use all validation samples, enter 0.  \\n\\n- **score\\\\_duty\\\\_cycle**: Specify the maximum duty cycle fraction for scoring. A lower value results in more training and a higher value results in more scoring. \\n\\n- **stopping\\\\_rounds**: Stops training when the option selected for **stopping\\\\_metric** doesn\\'t improve for the specified number of training rounds, based on a simple moving  average. To disable this feature, specify `0`. The metric is computed on the validation data (if provided); otherwise, training data is used. When used with **overwrite\\\\_with\\\\_best\\\\_model**, the final model is the best model generated for the given **stopping\\\\_metric** option.   \\n\\t>**Note**: If cross-validation is enabled: \\n\\t1. All cross-validation models stop training when the validation metric doesn\\'t improve. \\n    2. The main model runs for the mean number of epochs. \\n    3. N+1 models do *not* use **overwrite\\\\_with\\\\_best\\\\_model**\\n    4. N+1 models may be off by the number specified for **stopping\\\\_rounds** from the best model, but the cross-validation metric estimates the performance of the main model for the resulting number of epochs (which may be fewer than the specified number of epochs). \\n\\n- **stopping\\\\_metric**: Select the metric to use for early stopping. The available options are: \\n\\t\\n    - **AUTO**: Logloss for classification; deviance for regression\\n    - **deviance**\\n    - **logloss**\\n    - **MSE**\\n    - **AUC**\\n    - **r2**\\n    - **misclassification** \\n    - **mean\\\\_per\\\\_class\\\\_error**\\n\\n- **stopping\\\\_tolerance**: Specify the relative tolerance for the metric-based stopping to stop training if the improvement is less than this value. \\n\\n- **max\\\\_runtime\\\\_secs**: Maximum allowed runtime in seconds for model training. Use 0 to disable.\\n\\n- **autoencoder**: Check this checkbox to enable the Deep Learning autoencoder. This option is not selected by default. \\n\\t>**Note**: Cross-validation is not supported when autoencoder is enabled.   \\n\\n- **keep\\\\_cross\\\\_validation\\\\_predictions**: To keep the cross-validation predictions, check this checkbox. \\n\\n- **class\\\\_sampling\\\\_factors**: (Applicable only for classification and when **balance\\\\_classes** is enabled) Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance.  \\n\\n- **max\\\\_after\\\\_balance\\\\_size**: Specify the maximum relative size of the training data after balancing class counts (**balance\\\\_classes** must be enabled). The value can be less than 1.0. \\n\\n- **overwrite\\\\_with\\\\_best\\\\_model**: Check this checkbox to overwrite the final model with the best model found during training, based on the option selected for **stopping\\\\_metric**. This option is selected by default. \\n\\n- **target\\\\_ratio\\\\_comm\\\\_to\\\\_comp**: Specify the target ratio of communication overhead to computation. This option is only enabled for multi-node operation and if **train\\\\_samples\\\\_per\\\\_iteration** equals -2 (auto-tuning).  \\n\\n- **seed**: Specify the random number generator (RNG) seed for algorithm components dependent on randomization. The seed is consistent for each H2O instance so that you can create models with the same starting conditions in alternative configurations. \\n\\n- **rho**: (Applicable only if **adaptive\\\\_rate** is enabled) Specify the adaptive learning rate time decay factor.   \\n\\n- **epsilon**:(Applicable only if **adaptive\\\\_rate** is enabled) Specify the adaptive learning rate time smoothing factor to avoid dividing by zero. \\n\\n- **max_w2**: Specify the constraint for the squared sum of the incoming weights per unit (e.g., for Rectifier).  \\n\\n- **initial\\\\_weight\\\\_distribution**: Select the initial weight distribution (Uniform Adaptive, Uniform, or Normal).   \\n\\n- **regression_stop**: (Regression models only) Specify the stopping criterion for regression error (MSE) on the training data. To disable this option, enter -1.  \\n\\n- **diagnostics**: Check this checkbox to compute the variable importances for input features (using the Gedeon method). For large networks, selecting this option can reduce speed. This option is selected by default. \\n\\n- **fast_mode**: Check this checkbox to enable fast mode, a minor approximation in back-propagation. This option is selected by default. \\n\\n- **force\\\\_load\\\\_balance**: Check this checkbox to force extra load balancing to increase training speed for small datasets and use all cores. This option is selected by default. \\n\\n- **single\\\\_node\\\\_mode**: Check this checkbox to force H2O to run on a single node for fine-tuning of model parameters. This option is not selected by default. \\n\\n- **shuffle\\\\_training\\\\_data**: Check this checkbox to shuffle the training data. This option is recommended if the training data is replicated and the value of **train\\\\_samples\\\\_per\\\\_iteration** is close to the number of nodes times the number of rows. This option is not selected by default. \\n\\n- **missing\\\\_values\\\\_handling**: Specify how to handle missing values (Skip or MeanImputation). This defaults to MeanImputation.   \\n\\n- **quiet_mode**: Check this checkbox to display less output in the standard output. This option is not selected by default. \\n\\n- **sparse**: Check this  checkbox to enable sparse data handling, which is more efficient for data with many zero values.  \\n\\n- **col_major**: Check this checkbox to use a column major weight matrix for the input layer. This option can speed up forward propagation but may reduce the speed of backpropagation. This option is not selected by default. \\n\\n- **average_activation**: Specify the average activation for the sparse autoencoder.  \\n\\t> - If **Rectifier** is used, the **average\\\\_activation** value must be positive. \\n\\n- **sparsity_beta**: (Applicable only if **autoencoder** is enabled) Specify the sparsity-based regularization optimization. For more information, refer to the following [link](http://www.mit.edu/~9.520/spring09/Classes/class11_sparsity.pdf).  \\n  \\n- **max\\\\_categorical\\\\_features**: Specify the maximum number of categorical features enforced via hashing. The value must be at least one. \\n\\n- **reproducible**: To force reproducibility on small data, check this checkbox. If this option is enabled, the model takes more time to generate, since it uses only one thread. \\n\\n- **export\\\\_weights\\\\_and\\\\_biases**: To export the neural network weights and biases as H2O frames, check this checkbox.\\n\\n- **elastic\\\\_averaging**: To enable elastic averaging between computing nodes, which can improve distributed model convergence, check this checkbox (experimental).\\n\\n\\n- **rate**: (Applicable only if **adaptive\\\\_rate** is disabled) Specify the learning rate. Higher values result in a less stable model, while lower values lead to slower convergence. \\n\\n- **rate\\\\_annealing**: (Applicable only if **adaptive\\\\_rate** is disabled) Specify the rate annealing value. The rate annealing is calculated as **rate**\\\\(1 + **rate\\\\_annealing** * samples). \\n\\n- **rate\\\\_decay**: (Applicable only if **adaptive\\\\_rate** is disabled) Specify the rate decay factor between layers. The rate decay is calculated as (N-th layer: **rate** * alpha^(N-1)). \\n\\n- **momentum\\\\_start**: (Applicable only if **adaptive\\\\_rate** is disabled) Specify the initial momentum at the beginning of training; we suggest 0.5. \\n\\n- **momentum\\\\_ramp**: (Applicable only if **adaptive\\\\_rate** is disabled) Specify the number of training samples for which the momentum increases. \\n\\n- **momentum\\\\_stable**: (Applicable only if **adaptive\\\\_rate** is disabled) Specify the final momentum after the ramp is over; we suggest 0.99. \\n\\n- **nesterov\\\\_accelerated\\\\_gradient**: (Applicable only if **adaptive\\\\_rate** is disabled) Enables the [Nesterov Accelerated Gradient](http://premolab.ru/pub_files/pub88/qhkDNEyp8.pdf). \\n\\n\\n- **initial\\\\_weight\\\\_scale**: (Applicable only if **initial\\\\_weight\\\\_distribution** is **Uniform** or **Normal**) Specify the scale of the distribution function. For **Uniform**, the values are drawn uniformly. For **Normal**, the values are drawn from a Normal distribution with a standard deviation.  \\n\\n\\n\\n### Interpreting a Deep Learning Model\\n\\nTo view the results, click the View button. The output for the Deep Learning model includes the following information for both the training and testing sets: \\n\\n- Model parameters (hidden)\\n- A chart of the variable importances\\n- A graph of the scoring history (training MSE and validation MSE vs epochs)\\n- Output (model category, weights, biases)\\n- Status of neuron layers (layer number, units, type, dropout, L1, L2, mean rate, rate RMS, momentum, mean weight, weight RMS, mean bias, bias RMS)\\n- Scoring history in tabular format\\n- Training metrics (model name, model checksum name, frame name, frame checksum name, description, model category, duration in ms, scoring time, predictions, MSE, R2, logloss)\\n- Top-K Hit Ratios (for multi-class classification)\\n- Confusion matrix (for classification)\\n\\n\\n\\n### FAQ\\n\\n- **How does the algorithm handle missing values during training?**\\n\\n  Depending on the selected missing value handling policy, they are either imputed mean or the whole row is skipped.  \\n  The default behavior is mean imputation. Note that categorical variables are imputed by adding an extra \"missing\" level.   \\n  Optionally, Deep Learning can skip all rows with any missing values. \\n\\n- **How does the algorithm handle missing values during testing?**\\n\\n  Missing values in the test set will be mean-imputed during scoring.\\n\\n- **What happens if the response has missing values?**\\n\\n  No errors will occur, but nothing will be learned from rows containing missing the response.\\n\\n- **Does it matter if the data is sorted?** \\n\\n  Yes, since the training set is processed in order. Depending whether `train_samples_per_iteration` is enabled, some rows will be skipped. If `shuffle_training_data` is enabled, then each thread that is processing a small subset of rows will process rows randomly, but it is not a global shuffle.\\n\\n- **Should data be shuffled before training?**\\n\\n  Yes, the data should be shuffled before training, especially if the dataset is sorted. \\n\\n- **How does the algorithm handle highly imbalanced data in a response column?**\\n\\n  Specify `balance_classes`, `class_sampling_factors` and `max_after_balance_size` to control over/under-sampling.\\n\\n- **What if there are a large number of columns?**\\n\\n  The input neuron layer\\'s size is scaled to the number of input features, so as the number of columns increases, the model complexity increases as well. \\n  \\n- **What if there are a large number of categorical factor levels?**\\n\\n  This is something to look out for. Say you have three columns: zip code (70k levels), height, and income. The resulting number of internally one-hot encoded features will be 70,002 and only 3 of them will be activated (non-zero). If the first hidden layer has 200 neurons, then the resulting weight matrix will be of size 70,002 x 200, which can take a long time to train and converge. In this case, we recommend either reducing the number of categorical factor levels upfront (e.g., using `h2o.interaction()` from R), or specifying `max_categorical_features` to use feature hashing to reduce the dimensionality.\\n\\n- **How does your Deep Learning Autoencoder work? Is it deep or shallow?**\\n\\n  H2Os DL autoencoder is based on the standard deep (multi-layer) neural net architecture, where the entire network is learned together, instead of being stacked layer-by-layer.  The only difference is that no response is required in the input and that the output layer has as many neurons as the input layer. If you dont achieve convergence, then try using the *Tanh* activation and fewer layers.  We have some example test scripts [here](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/), and even some that show [how stacked auto-encoders can be implemented in R](https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_stacked_autoencoder_large.R). \\n\\n- **When building the model, does Deep Learning use all features or a selection of the best features?**\\n\\n  For Deep Learning, all features are used, unless you manually specify that columns should be ignored. Adding an L1 penalty can make the model sparse, but it is still the full size. \\n\\n- **What is the relationship between iterations, epochs, and the `train_samples_per_iteration` parameter?**\\n\\n  Epochs measures the amount of training. An iteration is one MapReduce (MR) step - essentially, one pass over the data. The `train_samples_per_iteration` parameter is the amount of data to use for training for each MR step, which can be more or less than the number of rows. \\n\\n\\n- **When do `reduce()` calls occur, after each iteration or each epoch?**\\n\\n  Neither; `reduce()` calls occur after every two `map()` calls, between threads and ultimately between nodes. There are many `reduce()` calls, much more than one per MapReduce step (also known as an \"iteration\"). Epochs are not related to MR iterations, unless you specify `train_samples_per_iteration` as `0` or `-1` (or to number of rows/nodes). Otherwise, one MR iteration can train with an arbitrary number of training samples (as specified by `train_samples_per_iteration`). \\n\\n- **Does each Mapper task work on a separate neural-net model that is combined during reduction, or is each Mapper manipulating a shared object that\\'s persistent across nodes?**\\n\\n   Neither; there\\'s one model per compute node, so multiple Mappers/threads share one model, which is why H2O is not reproducible unless a small dataset is used and `force_load_balance=F` or `reproducible=T`, which effectively rebalances to a single chunk and leads to only one thread to launch a `map()`. The current behavior is simple model averaging; between-node model averaging via \"Elastic Averaging\" is currently [in progress](https://github.com/h2oai/h2o-3/issues/13535). \\n\\n- **Is the loss function and backpropagation performed after each individual training sample, each iteration, or at the epoch level?**\\n\\n  Loss function and backpropagation are performed after each training sample (mini-batch size 1 == online stochastic gradient descent). \\n\\n- **When using Hinton\\'s dropout and specifying an input dropout ratio of ~20% and `train_samples_per_iteration` is set to 50, will each of the 50 samples have a different set of the 20% input neurons suppressed?** \\n\\n  Yes - suppression is not done at the iteration level across as samples in that iteration. The dropout mask is different for each training sample. \\n\\n- **When using dropout parameters such as `input_dropout_ratio`, what happens if you use only `Rectifier` instead of `RectifierWithDropout` in the activation parameter?**\\n\\n  The amount of dropout on the input layer can be specified for all activation functions, but hidden layer dropout is only supported is set to `WithDropout`. The default hidden dropout is 50%, so you don\\'t need to specify anything but the activation type to get good results, but you can set the hidden dropout values for each layer separately. \\n\\n- **When using the `score_validation_sampling` and `score_training_samples` parameters, is scoring done at the end of the Deep Learning run?** \\n\\n  The majority of scoring takes place after each MR iteration. After the iteration is complete, it may or may not be scored, depending on two criteria: the time since the last scoring and the time needed for scoring. \\n\\n  The maximum time between scoring (`score_interval`, default = 5 seconds) and the maximum fraction of time spent scoring (`score_duty_cycle`) independently of loss function, backpropagation, etc. \\n\\n  Of course, using more training or validation samples will increase the time for scoring, as well as scoring more frequently. For more information about how this affects runtime, refer to the [Deep Learning Performance Guide](http://h2o.ai/blog/2015/02/deep-learning-performance/).\\n\\n- **How does the validation frame affect the built neuron network?**\\n\\n  The validation frame is only used for scoring and does not directly affect the model. However, the validation frame can be used stopping the model early if `overwrite_with_best_model = T`, which is the default. If this parameter is enabled, the model with the lowest validation error is displayed at the end of the training. \\n\\n  By default, the validation frame is used to tune the model parameters (such as number of epochs) and will return the best model as measured by the validation metrics, depending on how often the validation metrics are computed (`score_duty_cycle`) and whether the validation frame itself was sampled. \\n\\n  Model-internal sampling of the validation frame (`score_validation_samples` and `score_validation_sampling` for optional stratification) will affect early stopping quality. If you specify a validation frame but set `score_validation_samples` to more than the number of rows in the validation frame (instead of 0, which represents the entire frame), the validation metrics received at the end of training will not be reproducible, since the model does internal sampling. \\n\\n- **Are there any best practices for building a model using checkpointing?**\\n\\n In general, to get the best possible model, we recommend building a model with `train\\\\_samples\\\\_per\\\\_iteration = -2` (which is the default value for auto-tuning) and saving it. \\n\\n To improve the initial model, start from the previous model and add iterations by building another model, setting the checkpoint to the previous model, and changing `train\\\\_samples\\\\_per\\\\_iteration`, `target\\\\_ratio\\\\_comm\\\\_to\\\\_comp`, or other parameters. \\n\\n If you don\\'t know your model ID because it was generated by R, look it up using `h2o.ls()`. By default, Deep Learning model names start with `deeplearning_` To view the model, use `m <- h2o.getModel(\"my\\\\_model\\\\_id\")` or `summary(m)`. \\n\\n There are a few ways to manage checkpoint restarts: \\n\\n *Option 1*: (Multi-node only) Leave `train\\\\_samples\\\\_per\\\\_iteration = -2`, increase `target\\\\_comm\\\\_to\\\\_comp` from 0.05 to 0.25 or 0.5, which provides more communication. This should result in a better model when using multiple nodes. **Note:** This does not affect single-node performance. \\n\\n *Option 2*: (Single or multi-node) Set `train\\\\_samples\\\\_per\\\\_iteration` to \\\\(N\\\\), where \\\\(N\\\\) is the number of training samples used for training by the entire cluster for one iteration. Each of the nodes then trains on \\\\(N\\\\) randomly-chosen rows for every iteration. The number defined as \\\\(N\\\\) depends on the dataset size and the model complexity. \\n\\n *Option 3*: (Single or multi-node) Change regularization parameters such as `l1, l2, max\\\\_w2, input\\\\_droput\\\\_ratio` or `hidden\\\\_dropout\\\\_ratios`. We recommend build the first mode using `RectifierWithDropout`, `input\\\\_dropout\\\\_ratio = 0` (if there is suspected noise in the input), and `hidden\\\\_dropout\\\\_ratios=c(0,0,0)` (for the ability to enable dropout regularization later). \\n\\n- **How does class balancing work?**\\n\\n The `max\\\\_after\\\\_balance\\\\_size` parameter defines the maximum size of the over-sampled dataset. For example, if `max\\\\_after\\\\_balance\\\\_size = 3`, the over-sampled dataset will not be greater than three times the size of the original dataset. \\n\\n For example, if you have five classes with priors of 90%, 2.5%, 2.5%, and 2.5% (out of a total of one million rows) and you oversample to obtain a class balance using `balance\\\\_classes = T`, the result is all four minor classes are oversampled by forty times and the total dataset will be 4.5 times as large as the original dataset (900,000 rows of each class). If `max\\\\_after\\\\_balance\\\\_size = 3`, all five balance classes are reduced by 3/5 resulting in 600,000 rows each (three million total). \\n\\n To specify the per-class over- or under-sampling factors, use `class\\\\_sampling\\\\_factors`. In the previous example, the default behavior with `balance\\\\_classes` is equivalent to `c(1,40,40,40,40)`, while when `max\\\\_after\\\\_balance\\\\_size = 3`, the results would be `c(3/5,40*3/5,40*3/5,40*3/5)`. \\n\\n In all cases, the probabilities are adjusted to the pre-sampled space, so the minority classes will have lower average final probabilities than the majority class, even if they were sampled to reach class balance. \\n\\n- **How is variable importance calculated for Deep Learning?**\\n\\n For Deep Learning, variable importance is calculated using the Gedeon method. \\n\\n- **Why do my results include a negative R^2 value?**\\n\\n H2O computes the R^2 as `1 - MSE/variance`, where `MSE` is the mean squared error of the prediction, and `variance` is the (weighted) variance: `sum(w*Y*Y)/sum(w) - sum(w*Y)^2/sum(w)^2`, where `w` is the row weight (1 by default), and `Y` is the centered response.\\n\\n If the MSE is greater than the variance of the response, you will see a negative R^2 value. This indicates that the model got a really bad fit, and the results are not to be trusted. \\n\\n---\\n\\n### Deep Learning Algorithm \\n\\nTo compute deviance for a Deep Learning regression model, the following formula is used: \\n\\nLoss = Quadratic -> MSE==Deviance\\nFor Absolute/Laplace or Huber -> MSE != Deviance\\n\\nFor more information about how the Deep Learning algorithm works, refer to the [Deep Learning booklet](http://h2o.ai/resources). \\n\\n### References\\n\\n [\"Deep Learning.\" *Wikipedia: The free encyclopedia*. Wikimedia Foundation, Inc. 1 May 2015. Web. 4 May 2015.](http://en.wikipedia.org/wiki/Deep_learning)\\n\\n [\"Artificial Neural Network.\" *Wikipedia: The free encyclopedia*. Wikimedia Foundation, Inc. 22 April 2015. Web. 4 May 2015.](http://en.wikipedia.org/wiki/Artificial_neural_network)\\n\\n [Zeiler, Matthew D. \\'ADADELTA: An Adaptive Learning Rate Method\\'. Arxiv.org. N.p., 2012. Web. 4 May 2015.](http://arxiv.org/abs/1212.5701)\\n\\n [Sutskever, Ilya et al. \"On the importance of initialization and momementum in deep learning.\" JMLR:W&CP vol. 28. (2013).](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\\n\\n [Hinton, G.E. et. al. \"Improving neural networks by preventing co-adaptation of feature detectors.\" University of Toronto. (2012).](http://arxiv.org/pdf/1207.0580.pdf)\\n\\n [Wager, Stefan et. al. \"Dropout Training as Adaptive Regularization.\" Advances in Neural Information Processing Systems. (2013).](http://arxiv.org/abs/1307.1493)\\n\\n [Gedeon, TD. \"Data mining of inputs: analysing magnitude and functional measures.\" University of New South Wales. (1997).](http://www.ncbi.nlm.nih.gov/pubmed/9327276)\\n    \\n [Candel, Arno and Parmar, Viraj. \"Deep Learning with H2O.\" H2O.ai, Inc. (2015).](https://leanpub.com/deeplearning)\\n    \\n  [Deep Learning Training](http://learn.h2o.ai/content/hands-on_training/deep_learning.html)\\n    \\n  [Slideshare slide decks](http://www.slideshare.net/0xdata/presentations?order=latest)\\n    \\n  [Youtube channel](https://www.youtube.com/user/0xdata)\\n    \\n  [Candel, Arno. \"The Definitive Performance Tuning Guide for H2O Deep Learning.\" H2O.ai, Inc. (2015).](http://h2o.ai/blog/2015/02/deep-learning-performance/)\\n\\n  [Niu, Feng, et al. \"Hogwild!: A lock-free approach to parallelizing stochastic gradient descent.\" Advances in Neural Information Processing Systems 24 (2011): 693-701. (algorithm implemented is on p.5)](https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf)\\n\\n  [Hawkins, Simon et al. \"Outlier Detection Using Replicator Neural Networks.\" CSIRO Mathematical and Information Sciences](http://neuro.bstu.by/ai/To-dom/My_research/Paper-0-again/For-research/D-mining/Anomaly-D/KDD-cup-99/NN/dawak02.pdf)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## Cross-Validation\\n\\nN-fold cross-validation is used to validate a model internally, i.e., estimate the model performance without having to sacrifice a validation split. Also, you avoid statistical issues with your validation split (it might be a lucky split, especially for imbalanced data). Good values for N are around 5 to 10. Comparing the N validation metrics is always a good idea, to check the stability of the estimation, before trusting the main model.\\n\\nYou have to make sure, however, that the holdout sets for each of the N models are good. For i.i.d. data, the random splitting of the data into N pieces (default behavior) or modulo-based splitting is fine. For temporal or otherwise structured data with distinct events, you have to make sure to split the folds based on the events. For example, if you have observations (e.g., user transactions) from N cities and you want to build models on users from only N-1 cities and validate them on the remaining city (if you want to study the generalization to new cities, for example), you will need to specify the parameter fold_column\" to be the city column. Otherwise, you will have rows (users) from all N cities randomly blended into the N folds, and all N cv models will see all N cities, making the validation less useful (or totally wrong, depending on the distribution of the data).  This is known as data leakage: https://youtu.be/NHw_aKO5KUM?t=889\\n\\n### How Cross-Validation is Calculated\\n\\nIn general, for all algos that support the nfolds parameter, H2Os cross-validation works as follows:\\n\\nFor example, for nfolds=5, 6 models are built. The first 5 models (cross-validation models) are built on 80% of the training data, and a different 20% is held out for each of the 5 models. Then the main model is built on 100% of the training data. This main model is the model you get back from H2O in R, Python and Flow.\\n\\nThis main model contains training metrics and cross-validation metrics (and optionally, validation metrics if a validation frame was provided). The main model also contains pointers to the 5 cross-validation models for further inspection.\\n\\nAll 5 cross-validation models contain training metrics (from the 80% training data) and validation metrics (from their 20% holdout/validation data). To compute their individual validation metrics, each of the 5 cross-validation models had to make predictions on their 20% of of rows of the original training frame, and score against the true labels of the 20% holdout.\\n\\nFor the main model, this is how the cross-validation metrics are computed: The 5 holdout predictions are combined into one prediction for the full training dataset (i.e., predictions for every row of the training data, but the model making the prediction for a particular row has not seen that row during training). This holdout prediction\" is then scored against the true labels, and the overall cross-validation metrics are computed.\\n\\nThis approach has some implications. Scoring the holdout predictions freshly can result in different metrics than taking the average of the 5 validation metrics of the cross-validation models. For example, if the sizes of the holdout folds differ a lot (e.g., when a user-given fold_column is used), then the average should probably be replaced with a weighted average. Also, if the cross-validation models map to slightly different probability spaces, which can happen for small DL models that converge to different local minima, then the confused rank ordering of the combined predictions would lead to a significantly different AUC than the average.\\n\\n### Example in R\\n\\nTo gain more insights into the variance of the holdout metrics (e.g., AUCs), you can look up the cross-validation models, and inspect their validation metrics. Heres an R code example showing the two approaches:\\n\\n```\\nlibrary(h2o)\\nh2o.init()\\ndf <- h2o.importFile(\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv.zip\")\\ndf$CAPSULE <- as.factor(df$CAPSULE)\\nmodel_fit <- h2o.gbm(3:8,2,df,nfolds=5,seed=1234)\\n\\n# Default: AUC of holdout predictions\\nh2o.auc(model_fit,xval=TRUE)\\n\\n# Optional: Average the holdout AUCs\\ncvAUCs <- sapply(sapply(model_fit@model$cross_validation_models, `[[`, \"name\"), function(x) { h2o.auc(h2o.getModel(x), valid=TRUE) })\\nprint(cvAUCs)\\nmean(cvAUCs)\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': \"## Using Cross-Validated Predictions\\n\\nWith cross-validated model building, H2O builds N+1 models: N cross-validated model and 1 overarching model over all of the training data.\\n\\nEach cv-model produces a prediction frame pertaining to its fold. It can be saved and probed from the various clients if `keep_cross_validation_predictions` parameter is set in the model constructor.\\n\\nThese holdout predictions have some interesting properties. First they have names like:\\n\\n```\\n  prediction_GBM_model_1452035702801_1_cv_1\\n```\\nand they contain, unsurprisingly, predictions for the data held out in the fold. They also have the same number of rows as the entire input training frame with `0`s filled in for all rows that are not in the hold out. \\n\\nLet's look at an example. \\n\\nHere is a snippet of a three-class classification dataset (last column is the response column), with a 3-fold identification column appended to the end:\\n\\n\\n| sepal_len | sepal_wid | petal_len | petal_wid | class   | foldId |\\n|-----------|-----------|-----------|-----------|---------|--------|\\n| 5.1       | 3.5       | 1.4       | 0.2       | setosa  | 0      |\\n| 4.9       | 3.0       | 1.4       | 0.2       | setosa  | 0      |\\n| 4.7       | 3.2       | 1.3       | 0.2       | setosa  | 2      |\\n| 4.6       | 3.1       | 1.5       | 0.2       | setosa  | 1      |\\n| 5.0       | 3.6       | 1.4       | 0.2       | setosa  | 2      |\\n| 5.4       | 3.9       | 1.7       | 0.4       | setosa  | 1      |\\n| 4.6       | 3.4       | 1.4       | 0.3       | setosa  | 1      |\\n| 5.0       | 3.4       | 1.5       | 0.2       | setosa  | 0      |\\n| 4.4       | 2.9       | 1.4       | 0.4       | setosa  | 1      |\\n\\n\\nEach cross-validated model produces a prediction frame\\n\\n```\\n  prediction_GBM_model_1452035702801_1_cv_1\\n  prediction_GBM_model_1452035702801_1_cv_2 \\n  prediction_GBM_model_1452035702801_1_cv_3\\n```\\n\\nand each one has the following shape (for example the first one):\\n\\n```\\n  prediction_GBM_model_1452035702801_1_cv_1\\n``` \\n\\n| prediction | setosa | versicolor | virginica |\\n|------------|--------|------------|-----------|\\n| 1          | 0.0232 | 0.7321     | 0.2447    |\\n| 2          | 0.0543 | 0.2343     | 0.7114    |\\n| 0          | 0      | 0          | 0         |\\n| 0          | 0      | 0          | 0         |\\n| 0          | 0      | 0          | 0         |\\n| 0          | 0      | 0          | 0         |\\n| 0          | 0      | 0          | 0         |\\n| 0          | 0.8921 | 0.0321     | 0.0758    |\\n| 0          | 0      | 0          | 0         |\\n\\nThe training rows receive a prediction of `0` (more on this below) as well as `0` for all class probabilities. Each of these holdout predictions has the same number of rows as the input frame.\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/datascience/DataScienceH2O-Dev.md', 'section': '## Combining holdout predictions\\n\\nThe frame of cross-validated predictions is simply the superposition of the individual predictions. [Here\\'s an example from R](https://github.com/h2oai/h2o-3/issues/15145):\\n\\n``` \\nlibrary(h2o)\\nh2o.init()\\n\\n# H2O Cross-validated K-means example \\nprosPath <- system.file(\"extdata\", \"prostate.csv\", package=\"h2o\")\\nprostate.hex <- h2o.uploadFile(path = prosPath)\\nfit <- h2o.kmeans(training_frame = prostate.hex, \\n                  k = 10, \\n                  x = c(\"AGE\", \"RACE\", \"VOL\", \"GLEASON\"), \\n                  nfolds = 5,  #If you want to specify folds directly, then use \"fold_column\" arg\\n                  keep_cross_validation_predictions = TRUE)\\n\\n# This is where cv preds are stored:\\nfit@model$cross_validation_predictions$name\\n\\n\\n# Compress the CV preds into a single H2O Frame:\\n# Each fold\\'s preds are stored in a N x 1 col, where the row values for non-active folds are set to zero\\n# So we will compress this into a single 1-col H2O Frame (easier to digest)\\n\\nnfolds <- fit@params$actual$nfolds\\npredlist <- sapply(1:nfolds, function(v) h2o.getFrame(fit@model$cross_validation_predictions[[v]]$name)$predict, simplify = FALSE)\\ncvpred_sparse <- h2o.cbind(predlist)  # N x V Hdf with rows that are all zeros, except corresponding to the v^th fold if that rows is associated with v\\npred <- apply(cvpred_sparse, 1, sum)  # These are the cross-validated predicted cluster IDs for each of the 1:N observations\\n```\\n\\nThis can be extended to other family types as well (multinomial, binomial, regression):\\n\\n```\\n# helper function\\n.compress_to_cvpreds <- function(h2omodel, family) {\\n  # return the frame_id of the resulting 1-col Hdf of cvpreds for learner l\\n  V <- h2omodel@params$actual$nfolds\\n  if (family %in% c(\"bernoulli\", \"binomial\")) {\\n    predlist <- sapply(1:V, function(v) h2o.getFrame(h2omodel@model$cross_validation_predictions[[v]]$name)[,3], simplify = FALSE)\\n  } else {\\n    predlist <- sapply(1:V, function(v) h2o.getFrame(h2omodel@model$cross_validation_predictions[[v]]$name)$predict, simplify = FALSE)\\n  }\\n  cvpred_sparse <- h2o.cbind(predlist)  # N x V Hdf with rows that are all zeros, except corresponding to the v^th fold if that rows is associated with v\\n  cvpred_col <- apply(cvpred_sparse, 1, sum)\\n  return(cvpred_col)\\n}\\n\\n\\n# Extract cross-validated predicted values (in order of original rows)\\nh2o.cvpreds <- function(object) {\\n\\n  # Need to extract family from model object\\n  if (class(object) == \"H2OBinomialModel\") family <- \"binomial\"\\n  if (class(object) == \"H2OMulticlassModel\") family <- \"multinomial\"\\n  if (class(object) == \"H2ORegressionModel\") family <- \"gaussian\"\\n    \\n  cvpreds <- .compress_to_cvpreds(h2omodel = object, family = family)\\n  return(cvpreds)\\n}\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/dl/dl.md', 'section': '## Building a Model\\n\\n1. Once data are parsed, click the **View** button, then click the **Build Model** button. \\n2. Select `Deep Learning` from the drop-down **Select an algorithm** menu, then click the **Build model** button. \\n3. If the parsed training data is not already listed in the **Training_frame** drop-down list, select it. \\n\\n  >**Note**: If the **Ignore\\\\_const\\\\_col** checkbox is checked, a list of the excluded columns displays below the **Training_frame** drop-down list. \\n\\n4. From the drop-down **Validation_frame** list, select the parsed testing (validation) data. \\n5. From the **Ignored_columns** section, select the columns to ignore in the *Available* area to move them to the *Selected* area. For this example, do not select any columns. \\n6. From the drop-down **Response** list, select the last column (`C785`). \\n7. From the drop-down **Activation** list, select the activation function (for this example, select `Tanh`). \\n8. In the **Hidden** field, specify the hidden layer sizes (for this example, enter `50,50`). \\n9. In the **Epochs** field, enter the number of times to iterate the dataset (for this example, enter `0.1`). \\n10. Click the **Build Model** button.\\n\\n  ![Building Models](../images/DL_BuildModel.png)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/dl/dl.md', 'section': '## Results\\n\\nTo view the results, click the **View** button. The output for the Deep Learning model includes the following information for both the training and testing sets:\\n \\n- Model parameters (hidden)\\n- A chart of the variable importances\\n- A graph of the scoring history (training MSE and validation MSE vs epochs)\\n- Training and validation metrics confusion matrix\\n- Output (model category, weights, biases)\\n- Status of neuron layers (layer number, units, type, dropout, L1, L2, mean rate, rate RMS, momentum, mean weight, weight RMS, mean bias, bias RMS)\\n- Scoring history in tabular format\\n- Training and validation metrics (model name, model checksum name, frame name, frame checksum name, description, model category, duration in ms, scoring time, predictions, MSE, R2, logloss)\\n- Top-10 Hit Ratios for training and validation\\n- Preview POJO\\n  \\n   ![Viewing Model Results](../images/DL_Results.png)\\n   \\n For more details, click the **Inspect** button. \\n \\n  ![Inspecting Results](../images/DL_Inspect.png)\\n  \\n \\nSelect the appropriate link to view details for: \\n \\n- Parameters\\n- Output\\n- Neuron layer status\\n- Scoring history\\n- Training metrics\\n- Training metrics - Top-10 Hit Ratios\\n- Training metrics confusion matrix\\n- Validation metrics\\n- Validation metrics confusion matrix\\n- Variable importances\\n\\nThe scoring history graph, training metrics confusion matrix, and validation metrics confusion matrix are shown below. \\n\\n   ![Training Metrics Confusion Matrix](../images/DL_Inspect_Conf.png)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/gbm/gbm.md', 'section': '## Getting Started\\n\\nThis tutorial uses a publicly available data set that can be found at:\\n<ahref=\"http://archive.ics.uci.edu/ml/datasets/Arrhythmia\" target=\"_blank\">http://archive.ics.uci.edu/ml/datasets/Arrhythmia</a>.\\n\\nThe original data are the Arrhythmia data set made available by UCI\\nMachine Learning repository. They are composed of 452 observations and 279 attributes.\\n\\nIf you don\\'t have any data of your own to work with, you can find some example datasets here: <a href=\"http://data.h2o.ai\" target=\"_blank\">http://data.h2o.ai</a>\\n\\n\\n### Importing Data\\nBefore creating a model, import data into H2O:\\n\\n1. Click the **Assist Me!** button (the last button in the row of buttons below the menus). \\n\\n ![Assist Me button](../images/Flow_AssistMeButton.png)\\n2. Click the **importFiles** link and enter the file path to the dataset in the **Search** entry field. \\n3. Click the **Add all** link to add the file to the import queue, then click the **Import** button. \\n  ![Importing Files](../images/GBM_ImportFile.png)\\n\\n\\n\\n### Parsing Data\\nNow, parse the imported data: \\n\\n1. Click the **Parse these files...** button. \\n\\n >**Note**: The default options typically do not need to be changed unless the data does not parse correctly. \\n\\n2. From the drop-down **Parser** list, select the file type of the data set (Auto, XLS, CSV, or SVMLight). \\n3. If the data uses a separator, select it from the drop-down **Separator** list. \\n4. If the data uses a column header as the first row, select the **First row contains column names** radio button. If the first row contains data, select the **First row contains data** radio button. You can also select the **Auto** radio button to have H2O automatically determine if the first row of the dataset contains the column names or data. \\n5. If the data uses apostrophes ( `\\'` - also known as single quotes), check the **Enable single quotes as a field quotation character** checkbox. \\n6. Review the data in the **Data Preview** section, then click the **Parse** button.  \\n\\n  ![Parsing Data](../images/GBM_Parse.png)\\n\\n\\n  **NOTE**: Make sure the parse is complete by confirming progress is 100% before continuing to the next step, model building. For small datasets, this should only take a few seconds, but larger datasets take longer to parse.\\n\\n\\n\\n### Building a Model\\n\\n1. Once data are parsed, click the **View** button, then click the **Build Model** button. \\n2. Select `Gradient Boosting Machine` from the drop-down **Select an algorithm** menu, then click the **Build model** button. \\n3. If the parsed arrhythmia.hex file is not already listed in the **Training_frame** drop-down list, select it. Otherwise, continue to the next step. \\n4. From the **Ignored_columns** section, select the columns to ignore in the *Available* area to move them to the *Selected* area. For this example, do not select any columns. \\n5. From the drop-down **Response** list, select column 1 (`C1`).  \\n6. In the **Ntrees** field, specify the number of trees to build  (for this example, `20`). \\n7. In the **Max_depth** field, specify the maximum number of edges between the top node and the furthest node as a stopping criteria (for this example, use the default value of `5`). \\n8. In the **Min_rows** field, specify the minimum number of observations (rows) to include in any terminal node as a stopping criteria (for this example, `25`). \\n9. In the **Nbins** field, specify the number of bins to use for data splitting (for this example, use the default value of `20`). The split points are evaluated at the boundaries at each of these bins. As the value of **Nbins** increases, the algorithm approximates more closely the evaluation of each individual observation as a split point. The cost of this refinement is an increase in computational time.  \\n10. In the **Learn_rate** field, specify the tuning parameter (also known as shrinkage) to slow the convergence of the algorithm to a solution, which helps prevent overfitting. For this example, enter `0.3`. \\n11. Click the **Build Model** button. \\n\\n  ![Building Models](../images/GBM_BuildModel.png)\\n\\n\\n\\n### Viewing GBM Results\\n\\nThe output for GBM includes the following: \\n\\n- Model parameters (hidden)\\n- A graph of the scoring history (training MSE vs number of trees)\\n- A graph of the variable importances\\n- Output (model category, validation metrics, initf)\\n- Model summary (number of trees, min. depth, max. depth, mean depth, min. leaves, max. leaves, mean leaves)\\n- Scoring history in tabular format\\n- Training metrics (model name, model checksum name, frame name, description, model category, duration in ms, scoring time, predictions, MSE, R2)\\n- Variable importances in tabular format\\n- POJO Preview\\n\\n  ![GBM Model Results](../images/GBM_ModelResults.png)\\n\\n\\nFor classification models, the MSE is based on the classification error within the tree. For regression models, MSE is calculated from the squared deviances, as it is in standard regressions.\\n\\n\\n### Viewing Predictions\\n\\nTo view predictions, click the **Predict** button. From the drop-down **Frame** list, select the arrhythmia.hex file and click the **Predict** button. \\n\\nTo view more prediction data, click the **View Prediction Frame** button.\\n\\n  ![GBM: Viewing Prediction Frame](../images/GBM_ViewPredictFrame.png)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': '## Overview\\n\\nH2O now has random [hyperparameter search](https://en.wikipedia.org/wiki/Hyperparameter_optimization) with time- and metric-based early stopping.  Bergstra and Bengio[^1] write on p. 281:\\n\\n> **Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time.**\\n\\nEven smarter means of searching the hyperparameter space are in the pipeline, but for most use cases random search does as well.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': '## What Are Hyperparameters?\\n\\nNearly all model algorithms used in machine learning have a set of tuning \"knobs\" which affect how the learning algorithm fits the model to the data.  Examples are the regularization settings *alpha* and *lambda* for [Generalized Linear Modeling](http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/3/docs-website/h2o-docs/booklets/GLM_Vignette.pdf) or *ntrees* and *max_depth* for [Gradient Boosted Models](http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/3/docs-website/h2o-docs/booklets/GBM_Vignette.pdf). These knobs are called *hyperparameters* to distinguish them from internal model parameters, such as GLM\\'s beta coefficients or Deep Learning\\'s weights, which get learned from the data during the model training process.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': \"## What Is Hyperparameter Optimization?\\n\\nThe set of all combinations of values for these knobs is called the *hyperparameter space*.  We'd like to find a set of hyperparameter values which gives us the best model for our data in a reasonable amount of time.  This process is called [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\\n\\nH2O contains good default values for many datasets, but to get the best performance for your data you will want to tune at least some of these hyperparameters to maximize the predictive performance of your models. You should start with the most important hyperparameters for your algorithm of choice, for example *ntrees* and *max_depth* for the tree models or the *hidden* layers for Deep Learning.  \\n\\nH2O provides some guidance by grouping the hyperparameters by their importance in the Flow UI.  You should look carefully at the values of the ones marked *critical*, while the *secondary* or *expert* ones are generally used for special cases or fine tuning.\\n\\nNote that some hyperparameters, such as *learning_rate*, have a very wide dynamic range.  You should choose  values that reflect this for your search (e.g., powers of 10 or of 2) to ensure that you cover the most relevant parts of the hyperparameter space. (Bergstra and Bengio p. 290)\\n\\n### Measuring Model Quality\\n\\nThere are many different ways to measure model quality.  If you don't know which to use, H2O will choose a good general-purpose metric for you based on the category of your model ([binomial](https://en.wikipedia.org/wiki/Binary_classification) or [multinomial classification](https://en.wikipedia.org/wiki/Multiclass_classification), [regression](https://en.wikipedia.org/wiki/Regression_analysis), [clustering](https://en.wikipedia.org/wiki/Cluster_analysis), ...).  However, you may want to choose a metric to compare your models based on your specific goals (e.g., [maximizing AUC](https://www.kaggle.com/wiki/AreaUnderCurve), [minimizing log loss](https://www.kaggle.com/wiki/LogarithmicLoss), [minimizing false negatives](https://en.wikipedia.org/wiki/Precision_and_recall), [minimizing mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error), ...).\\n\\n### Overfitting\\n\\n[Overfitting](https://en.m.wikipedia.org/wiki/Overfitting) is the phenomenon of fitting a model so thoroughly to your training data that it begins to memorize the fine details of that specific data, rather than finding general characteristics of that data which will also apply to future data on which you want to make predictions.\\n\\nOverfitting not only applies to the model training process, but also to the *model selection* process.  During the process of tuning the hyperparameters and selecting the best model you should avoid overfitting them to your training data.  Otherwise, the hyperparameter values that you choose will be too highly tuned to your selection data, and will not generalize as well as they could to new data.  Note that this is the same principle as, but subtly different from, overfitting during model training. Ideally you should use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) or a [validation set](https://en.wikipedia.org/wiki/Test_set#Validation_set) during training and then a final holdout *test* ([validation](https://en.wikipedia.org/wiki/Test_set#Validation_set)) dataset for model selection.  As Bergstra and Bengio write on p. 290, \\n\\n> The standard practice for evaluating a model found by cross-validation is to report [test set error] for the [hyperparameter vector] that minimizes [validation error].\\n\\nYou can read much more on this topic in Chapter 7 of [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) from H2O advisors and Stanford professors Trevor Hastie and Rob Tibshirani with Jerome Friedman [^2].\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': '## Selecting Hyperparameters Manually and With Cartesian Grid\\n\\nThe traditional method of selecting the values for your hyperparameters has been to individually train a number of models with different combinations of values, and then to compare the model performance to choose the best model.  For example, for a tree-based model you might choose *ntrees* of (50, 100 and 200) and *max_depth* of (5, 10, 15 and 20) for a total of 3 x 4 = 12 models.  This process of trying out hyperparameter sets by hand is called *manual search*.  By looking at the models\\' predictive performance, as measured by test-set, cross-validation or validation metrics, you select the best hyperparameter settings for your data and needs.  \\n\\nAs the number of hyperparameters and the lists of desired values increase this obviously becomes quite tedious and difficult to manage.\\n\\n### A Little Help?\\n\\nFor several years H2O has included *grid search*, also known as *Cartesian Hyperparameter Search* or *exhaustive search*.  Grid search builds models for every combination of hyperparameter values that you specify.  \\n\\nBergstra and Bengio write on p. 283:\\n\\n> Grid search ... typically finds a better [set of hyperparameters] than purely manual sequential optimization (in the same amount of time)\\n\\nH2O keeps track of all the models resulting from the search, and allows you to sort the list based on any supported model metric (e.g., AUC or log loss).  For the example above, H2O would build your 12 models and return the list sorted with the best first, either using the metric of your choice or automatically using one that\\'s generally appropriate for your model\\'s category.\\n\\nH2O allows you to run multiple hyperparameter searches and to collect all the models for comparison in a single sortable result set: just name your grid and run multiple searches.  You can even add models from manual searches to the result set by specifying a grid search with a single value for each interesting hyperparameter:\\n\\n```Python\\n# Begin with a random search of a space of 6 * 5 = 30 possible models:\\nhyper_parameters = { \\'alpha\\': [0.01,0.1,0.3,0.5,0.7,0.9], \\n                     \\'lambda\\': [1e-4,1e-5,1e-6,1e-7,1e-8] }\\n\\nsearch_criteria = { \\'strategy\\': \"RandomDiscrete\", \\'seed\\': 42,\\n                    \\'stopping_metric\\': \"AUTO\", \\n                    \\'stopping_tolerance\\': 0.001,\\n                    \\'stopping_rounds\\': 2 }\\n\\t\\t    \\nrandom_plus_manual = \\n    H2OGridSearch(H2OGeneralizedLinearEstimator(family=\\'binomial\\', nfolds=5),\\n      hyper_parameters, \\n      grid_id=\"random_plus_manual\", \\n      search_criteria=search_criteria)\\n    \\nrandom_plus_manual.train(x=x,y=y, training_frame=training_data)\\n\\n# Now add a manual search to the results:\\nmanual_hyper_parameters = {\\'alpha\\': [0.05], \\'lambda\\': [1e-4]}\\nrandom_plus_manual = \\n    H2OGridSearch(H2OGeneralizedLinearEstimator(family=\\'binomial\\', nfolds=5),\\n      manual_hyper_parameters, \\n      grid_id=\"random_plus_manual\")\\n\\nrandom_plus_manual.train(x=x,y=y, training_frame=training_data)\\n\\nrandom_plus_manual.show()\\nprint(random_plus_manual.sort_by(\\'F1\\', False))\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': '## Searching Large Hyperparameter Spaces\\n\\nAs the number of hyperparameters being tuned increases, and the values that you would like to explore for those hyperparameters increases, you obviously get a combinatorial explosion in the number of models required for an exhaustive search.  Since we always have time constraints on the model tuning process the obvious thing to do is to narrow down our choices by doing a coarser search of the space. Given a fixed amount of time, making random choices of hyperparameter values generally gives results that are better than the best results of an Cartesian (exhaustive) search.  \\n\\nBergstra and Bengio write on p. 281:\\n\\n> **Compared with neural networks configured by a pure grid search,\\nwe find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time.** Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration\\nspace.\\n\\n> ...\\n\\n> **[F]or most data sets only a few of the hyper-parameters really matter,\\nbut ... different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets.**\\n\\n> ...\\n\\n> We propose random search as a substitute and baseline that is both reasonably efficient (roughly equivalent to or better than combinining manual search and grid search, in our experiments) and keeping the advantages of implementation simplicity and reproducibility of pure grid search.\\n\\n> ...\\n\\n> [R]andom search ... trades a small reduction in efficiency in low-dimensional spaces for a large improvement in efficiency in high-dimensional search spaces.\\n\\nAfter doing a random search, if desired you can then iterate by \"zooming in\" on the regions of the hyperparameter space which look promising.  You can do this by running additional, more targeted, random or Cartesian hyperparameter searches or manual searches.  For example, if you started with *alpha* values of [0.0, 0.25, 0.5, 0.75, 1.0] and the middle values look promising, you can follow up with a finer grid of [0.3, 0.4, 0.5, 0.6, 0.7].'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': '## Random Hyperparameter Search in H2O\\n\\nH2O has supported random hyperparameter search since version 3.8.1.1.  To use it, specify a grid search as you would with a Cartesian search, but add *search criteria* parameters to control the type and extent of the search.  You can specify a max runtime for the grid, a max number of models to build, or metric-based automatic early stopping.  If you choose several of these then H2O will stop when the first of the criteria are met.  As an example, you might specify \"stop when MSE has improved over the moving average of the best 5 models by less than 0.0001, but take no more than 12 hours\".\\n\\nH2O will choose a random set of hyperparameter values from the ones that you specify, without repeats, and build the models sequentially.  You can look at the incremental results while the models are being built by fetching the grid with the `h2o.getGrid` (R) or `h2o.get_grid` (Python) functions.  There\\'s also a `getGrids` command in Flow that will allow you to click on any of the grids you\\'ve built.  H2O\\'s Flow UI will soon plot the error metric as the grid is being built to make the progress easy to visualize, something like this:\\n\\n![Error for Random Search](images/error_for_random_search.png \"Error for Random Search\")\\n\\n### Choosing Search Criteria\\n\\nIn general, metric-based early stopping optionally combined with max runtime is the best choice.  The number of models it will take to converge toward a global best can vary a lot (see below), and metric-based early stopping accounts for this automatically by stopping the search process when the error curve ([learning curve](http://youtu.be/g4XluwGYPaA)[^3]) flattens out.\\n\\nThe number of models required for convergence depends on a number of things, but mostly on the \"shape\" of the error function in the hyperparameter space [Bergstra and Bengio p. 291].  While most algorithms perform well in a fairly large region of the hyperparameter space on most datasets, some combinations of dataset and algorithm are very sensitive: they have a very \"peaked\" error functions.  In tuning neural networks with a large numbers of hyperparameters and various datasets Bergstra and Bengio find convergence within 2-64 trials (models built), depending largely on which hyperparameters they choose to tune.  In some classes of search they reach convergence in 4-8 trials, even with a very large search space:\\n\\n> Random experiment efficiency curves of a single-layer neural network for eight of the data sets used in Larochelle et al. (2007) ... (7 hyper-parameters to optimize). ... Random searches of 8 trials match or outperform grid searches of (on average) 100 trials.\\n\\nSimpler algorithms such as GBM and GLM should require few trials to get close to a global minimum.\\n\\n### Examples: R\\n\\nThis example is clipped from [GridSearch.md](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/GridSearch.md):\\n\\n```r\\n\\n# Construct a large Cartesian hyper-parameter space\\nntrees_opts = c(10000)       # early stopping will stop earlier\\nmax_depth_opts = seq(1,20)\\nmin_rows_opts = c(1,5,10,20,50,100)\\nlearn_rate_opts = seq(0.001,0.01,0.001)\\nsample_rate_opts = seq(0.3,1,0.05)\\ncol_sample_rate_opts = seq(0.3,1,0.05)\\ncol_sample_rate_per_tree_opts = seq(0.3,1,0.05)\\n#nbins_cats_opts = seq(100,10000,100) # no categorical features\\n                                      # in this dataset\\n\\nhyper_params = list( ntrees = ntrees_opts, \\n                     max_depth = max_depth_opts, \\n                     min_rows = min_rows_opts, \\n                     learn_rate = learn_rate_opts,\\n                     sample_rate = sample_rate_opts,\\n                     col_sample_rate = col_sample_rate_opts,\\n                     col_sample_rate_per_tree = col_sample_rate_per_tree_opts\\n                     #,nbins_cats = nbins_cats_opts\\n)\\n\\n\\n# Search a random subset of these hyper-parmameters. Max runtime \\n# and max models are enforced, and the search will stop after we \\n# don\\'t improve much over the best 5 random models.\\nsearch_criteria = list(strategy = \"RandomDiscrete\", \\n                       max_runtime_secs = 600, \\n                       max_models = 100, \\n                       stopping_metric = \"AUTO\", \\n                       stopping_tolerance = 0.00001, \\n                       stopping_rounds = 5, \\n                       seed = 123456)\\n\\ngbm_grid <- h2o.grid(\"gbm\", \\n                     grid_id = \"mygrid\",\\n                     x = predictors, \\n                     y = response, \\n\\n                     # faster to use a 80/20 split\\n                     training_frame = trainSplit,\\n                     validation_frame = validSplit,\\n                     nfolds = 0,\\n\\n                     # alternatively, use N-fold cross-validation:\\n                     # training_frame = train,\\n                     # nfolds = 5,\\n\\n                     # Gaussian is best for MSE loss, but can try \\n                     # other distributions (\"laplace\", \"quantile\"):\\n                     distribution=\"gaussian\",\\n\\n                     # stop as soon as mse doesn\\'t improve by \\n                     # more than 0.1% on the validation set, \\n                     # for 2 consecutive scoring events:\\n                     stopping_rounds = 2,\\n                     stopping_tolerance = 1e-3,\\n                     stopping_metric = \"MSE\",\\n\\n                     # how often to score (affects early stopping):\\n                     score_tree_interval = 100, \\n                     \\n                     ## seed to control the sampling of the \\n                     ## Cartesian hyper-parameter space:\\n                     seed = 123456,\\n                     hyper_params = hyper_params,\\n                     search_criteria = search_criteria)\\n\\ngbm_sorted_grid <- h2o.getGrid(grid_id = \"mygrid\", sort_by = \"mse\")\\nprint(gbm_sorted_grid)\\n\\nbest_model <- h2o.getModel(gbm_sorted_grid@model_ids[[1]])\\nsummary(best_model)\\n```\\n\\nYou can find [another example here](https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/grid-search-model-selection.R).\\n\\n### Examples: Python\\n\\nThis example is clipped from  [pyunit\\\\_benign\\\\_glm\\\\_grid.py](https://github.com/h2oai/h2o-3/blob/master/h2o-py/tests/testdir_algos/glm/pyunit_benign_glm_grid.py):\\n\\n```python\\nhyper_parameters = {\\'alpha\\': [0.01,0.3,0.5], \\'lambda\\': [1e-5,1e-6,1e-7,1e-8]}\\n\\n# test search_criteria plumbing and max_models\\nsearch_criteria = { \\'strategy\\': \"RandomDiscrete\", \\'max_models\\': 3 }\\nmax_models_g = H2OGridSearch(H2OGeneralizedLinearEstimator(family=\\'binomial\\'),\\n                             hyper_parameters, search_criteria=search_criteria)\\nmax_models_g.train(x=x,y=y, training_frame=training_data)\\n\\nmax_models_g.show()\\nprint(max_models_g.grid_id)\\nprint(max_models_g.sort_by(\\'F1\\', False))\\n\\nassert len(max_models_g.models) == 3, \"expected 3 models, got: {}\".format(len(max_models_g.models))\\nprint(max_models_g.sorted_metric_table())\\nprint(max_models_g.get_grid(\"r2\"))\\n\\n# test search_criteria plumbing and asymptotic stopping\\nsearch_criteria = { \\'strategy\\': \"RandomDiscrete\", \\'seed\\': 42,\\n\\t\\t    \\'stopping_metric\\': \"AUTO\", \\'stopping_tolerance\\': 0.1,\\n\\t\\t    \\'stopping_rounds\\': 2 }\\nasymp_g = H2OGridSearch(H2OGeneralizedLinearEstimator(family=\\'binomial\\', nfolds=5),\\n\\t                hyper_parameters, search_criteria=search_criteria)\\nasymp_g.train(x=x,y=y, training_frame=training_data)\\n\\nasymp_g.show()\\nprint(asymp_g.grid_id)\\nprint(asymp_g.sort_by(\\'F1\\', False))\\n\\nassert len(asymp_g.models) == 5, \"expected 5 models, got: {}\".format(len(asymp_g.models))\\n```\\n\\n\\n### Examples: Flow\\n\\nFlow includes an example called GBM_GridSearch.flow which does both Cartesian and random searches:\\n\\n![search criteria](images/flow_random_grid_settings.png \"Random Search Settings\")\\n\\n![search criteria](images/flow_random_grid_criteria.png \"Random Search Criteria\")'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/random hyperparmeter search and roadmap.md', 'section': '## What\\'s That Up In The Road? A Head? (Roadmap)\\n\\nThis section covers possible improvements for hyperparameter search in H2O and lays out a roadmap.\\n\\n### Ease of Use\\n\\nWith the addition of random hyperparameter search it becomes more practical for non-experts to get good, albeit not expert, results with the ML model training process:\\n\\n> Algorithmic approaches to hyper-parameter optimization make machine learning results easier to disseminate, reproduce, and transfer to other domains.[^4] p. 8\\n\\nWe are looking into adding either fixed or heuristically-driven hyperparameter spaces for use with random search, essentially an \"I\\'m Feeling Lucky\" button for model building.\\n\\n\\n### Covering the Space Better\\n\\nOne possibility for improving random search is choosing sets of hyperparameters which cover the space more efficiently than randomly choosing each value independently.  Bergstra and Bengio cover this on pages 295-297, and find a potential improvement of only a few percentage points and only when doing searches of 100-500 models.  This is because, as they state earlier, the number of hyperparameters which are important for a given dataset is quite small (1-4), and the random search process covers this low number of dimensions quite well.  See the illustration of the projection of high hyperparameter space dimensions onto low on Bergstra and Bengio p. 284 and the plots of hyperparameter importance by dataset on p. 294.  On p. 295 they show that the speed of convergence of the search is directly related to the number of hyperparameters which are important for the given dataset.\\n\\nThere is ongoing research on trying to predetermine the \"variable importance\" of hyperparameters for a given dataset.  If this bears fruit we will be able to narrow the search so that we converge to a globally-good model more quickly.\\n\\n\\n### Learning the Hyperparameter Space\\n\\nBergstra and Bengio and Bergstra, Bengio, Bardenet and Kegl note that random hyperparameter search works almost as well as more sophisticated methods for the types of algorithms available in H2O.   For very complex algorithms like Deep Belief Networks (not available in H2O) they can be insufficient:\\n\\n> Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs.\\n\\n> ...\\n\\n> 1) Random search is competitive with the manual optimization of DBNs ... and 2) Automatic sequential optimization outperforms both manual and random search.\\n\\n*Automatic sequential optimization* refers here to techniques which build a model of the hyperparameter space and use it to guide the search process.  The most well-known of these is the use of Gaussian Process (GP) models.  Bergstra, Bengio, Bardenet and Kegl compare  random search against both Gaussian Process  and Tree-structured Parzen Estimator (TPE) learning techniques.  They train Deep Belief Networks of 10 hyperparameters on a very tiny dataset of 506 rows and 13 columns. [Bergstra, Bengio, Bardenet and Kegl p. 5], initializing the GP and TPE models with the results of a 30-model random search.\\n\\nThey find that for this test case the TPE method outperforms GP and GP outperforms random search beyond the initial 30 models.  However, they can\\'t explain whether TPE does better because it narrows in on good hyperparameters more quickly or conversely because it searches more randomly than GP. [Bergstra, Bengio, Bardenet and Kegl p. 7]  Also note that the size of the dataset is very, very small compared with the number of internal model parameters and model tuning hyperparameters.  It is a bit hard to believe that these results apply to datasets of typical sizes for users of H2O (hundreds of millions or billions of rows, and hundreds or thousands of columns).\\n\\nExperimentation and prototyping is clearly needed here to see which of these techniques, if any, are worth adding to H2O.\\n\\n\\n[^1]: Bergstra and Bengio. [*Random Search for Hyper-Parameter Optimization*, 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\\n\\n[^2]: Trevor Hastie, Rob Tibshirani and Jerome Friedman. [*The Elements of Statistical Learning*, 2008](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\\n\\n[^3]: Andrew Ng. [*Machine Learning*, 2016](https://class.coursera.org/ml-005/)\\n\\n[^4]: Bergstra, Bengio, Bardenet and Kegl. [*Algorithms for Hyper-parameter Optimization*, 2011](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/tutorials/rf/rf.md', 'section': '## Distributed Random Forest Tutorial\\n\\nThis tutorial describes how to create a Distributed Random Forest (DRF) model using H2O Flow.\\n\\nThose who have never used H2O before should refer to <a href=\"https://github.com/h2oai/h2o-dev/blob/master/h2o-docs/src/product/flow/README.md\" target=\"_blank\">Getting Started</a> for additional instructions on how to run H2O Flow.\\n\\n\\n### Getting Started\\n\\nThis tutorial uses a publicly available data set that can be found at <a href =\"http://archive.ics.uci.edu/ml/machine-learning-databases/internet_ads/\" target=\"_blank\">http://archive.ics.uci.edu/ml/machine-learning-databases/internet_ads/</a>\\n\\nThe data are composed of 3279 observations, 1557 attributes, and an a priori grouping assignment. The objective is to build a prediction tool that predicts whether an object is an internet ad or not.\\n\\nIf you don\\'t have any data of your own to work with, you can find some example datasets at <a href=\"http://data.h2o.ai\" target=\"_blank\">http://data.h2o.ai</a>.\\n\\n\\n#### Importing Data\\nBefore creating a model, import data into H2O:\\n\\n1. Click the **Assist Me!** button (the last button in the row of buttons below the menus). \\n\\n ![Assist Me button](../images/Flow_AssistMeButton.png)\\n\\n2. Click the **importFiles** link and enter the file path to the dataset in the **Search** entry field.  \\n3. Click the **Add all** link to add the file to the import queue, then click the **Import** button. \\n\\n  ![Importing Files](../images/RF_ImportFile.png)\\n\\n\\n#### Parsing Data\\n\\nNow, parse the imported data: \\n\\n1. Click the **Parse these files...** button. \\n\\n  **Note**: The default options typically do not need to be changed unless the data does not parse correctly. \\n\\n2. From the drop-down **Parser** list, select the file type of the data set (Auto, XLS, CSV, or SVMLight). \\n3. If the data uses a separator, select it from the drop-down **Separator** list. \\n4. If the data uses a column header as the first row, select the **First row contains column names** radio button. If the first row contains data, select the **First row contains data** radio button. To have H2O automatically determine if the first row of the dataset contains column names or data, select the **Auto** radio button. \\n5. If the data uses apostrophes ( `\\'` - also known as single quotes), check the **Enable single quotes as a field quotation character** checkbox. \\n6. To delete the imported dataset after parsing, check the **Delete on done** checkbox. \\n\\n  **NOTE**: In general, we recommend enabling this option. Retaining data requires memory resources, but does not aid in modeling because unparsed data cannot be used by H2O.\\n\\n7. Review the data in the **Edit Column Names and Types** section.\\n8. Click the **Next page** button until you reach the last page. \\n\\n   ![Page buttons](../images/Flow_PageButtons.png)\\n\\n9. For column 1559, select `Enum` from the drop-down column type menu. \\n10. Click the **Parse** button.  \\n\\n  ![Parsing Data](../images/RF_Parse.png)\\n\\n  **NOTE**: Make sure the parse is complete by confirming progress is 100% before continuing to the next step, model building. For small datasets, this should only take a few seconds, but larger datasets take longer to parse.\\n\\n\\n\\n### Building a Model\\n\\n1. Once data are parsed, click the **View** button, then click the **Build Model** button. \\n2. Select `Distributed RF` from the drop-down **Select an algorithm** menu, then click the **Build model** button. \\n3. If the parsed ad.hex file is not already listed in the **Training_frame** drop-down list, select it. Otherwise, continue to the next step. \\n4. From the **Response column** drop-down list, select `C1`. \\n5. In the **Ntrees** field, specify the number of trees for the model to build. For this example, enter `150`. \\n6. In the **Max_depth** field, specify the maximum distance from the root to the terminal node. For this example, use the default value of `20`. \\n7. In the **Mtries** field, specify the number of features on which the trees will be split. For this example, enter `1000`. \\n8. Click the **Build Model** button. \\n\\n   ![Random Forest Model Builder](../images/RF_BuildModel.png)\\n\\n\\n### DRF Output\\n\\nThe DRF model output includes the following: \\n\\n- Model parameters (hidden)\\n- Scoring history graph (number for each tree and MSE)\\n- ROC curve, training metrics, AUC (with drop-down menus to select thresholds and criterion) \\n- Variable importances (variable name, relative importance, scaled importance, percentage)\\n- Output (model category, validation metrics, initf)\\n- Model summary (number of trees, min. depth, max. depth, mean depth, min. leaves, max. leaves, mean leaves)\\n- Scoring history (in tabular format)\\n- Training metrics (model name, model checksum, frame name, frame checksum, description if applicable, model category, duration in ms, scoring time, predictions, MSE, R2, Logloss, AUC, Gini)\\n- Domain \\n- Training metrics (thresholds, F1, F2, F0Points, Accuracy, Precision, Recall, Specificity, Absolute MCC, min. per-class accuracy, TNS, FNS, FPS, TPS, IDX)\\n- Maximum metrics (metric, threshold, value, IDX)\\n- Variable importances\\n- Preview POJO\\n\\n\\n  ![Random Forest Model Results](../images/RF_Model_Results.png)\\n\\n### DRF Predict\\n\\nTo generate a prediction, click the **Predict** button in the model results and select the `ad.hex` file from the drop-down **Frame** list, then click the **Predict** button. \\n\\n  ![Random Forest Prediction](../images/RF_Predict.png)\\n\\nYou can also click the **Inspect** button to access more information (for example, columns or data). \\n\\n  ![Random Forest Prediction Details](../images/RF_Predict2.png)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2OBenefits.md', 'section': \"## What H2O Provides\\n\\n### Better Predictions\\n\\n- Powerful, ready-to-use algorithms that derive insights from all your data\\n\\n### Speed\\n\\n- In-memory parallel processing for real-time responsiveness, increasing efficiency, and running models without sampling\\n\\n### Ease of Use\\n\\n- Flow, an intuitive web UI that is designed to simplify a data scientist's workflow, allows you to modify, save, export, and share your workflow with others\\n\\n### Extensibility\\n\\n- Seamless Hadoop integration with distributed data ingestion from HDFS and S3\\n- Models are built using Java and can be exported as Plain Old Java Objects (POJO) for integration in your custom application\\n\\n### Scalability\\n\\n- Easy to iterate, develop, and train models on large data without extra modeling time\\n\\n### Real-time Scoring\\n\\n- Predict and score more accurately and 10x faster than the next best technology on the market\\n\\n\\n### Modeling with State of the Art Machine Learning Algorithms\\nModel | Description\\n--------------|------------\\nGeneralized Linear Models (GLM) | A flexible generalization of ordinary linear regression for response variables that have error distribution models other than a normal distribution. GLM unifies various other statistical models, including linear, logistic, Poisson, and more.\\nDecision Trees | A decision support tool that uses a tree-like graph or model of decisions and their possible consequences.\\nGradient Boosting (GBM) | A method to produce a prediction model in the form of an ensemble of weak prediction models. It builds the model in a stage-wise fashion and is generalized by allowing an arbitrary differentiable loss function. It is one of the most powerful methods available today.\\nK-Means | A method to uncover groups or clusters of data points often used for segmentation. It clusters observations into k certain points with the nearest mean.\\nAnomaly Detection | Identify the outliers in your data by invoking a powerful pattern recognition model.\\nDeep Learning | Model high-level abstractions in data by using non-linear transformations in a layer-by-layer method. Deep learning is an example of unsupervised learning and can make use of unlabeled data that other algorithms cannot.\\nNave Bayes | A probabilistic classifier that assumes the value of a particular feature is unrelated to the presence or absence of any other feature, given the class variable. It is often used in text categorization.\\nStacked Ensembles | A supervised ensemble machine learning algorithm that finds the optimal combination of a collection of prediction algorithms using a process called stacking.\\nXGBoost | An optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. This algorithm provides parallel tree boosting (also known as GBDT, GBM) that solves many data science problems in a fast and accurate way.\\nWord2vec | An algorithm that takes a text corpus as an input and produces the word vectors as output. The algorithm first creates a vocabulary from the training text data and then learns vector representations of the words.\\n\\n### Scoring Models with Confidence\\nScore Tool | Description\\t\\n-----|------------\\nPredict | Generate outcomes of a data set with any model. Predict with GLM, GBM, Decision Trees or Deep Learning models.\\nConfusion Matrix | Visualize the performance of an algorithm in a table to understand how a model performs.\\nAUC | A graphical plot to visualize the performance of a model by its sensitivity, true positive, false positive to select the best model.\\n*HitRatio | A classification matrix to visualize the ratio of the number of correctly classified and incorrectly classified cases.\\n*Multi-Model Scoring | Compare and contrast multiple models on a data set to find the best performer to deploy into production.\\n\\n--- \\n\\n### Use Cases\\n\\n- Fraud detection \\n- Churn identification to prevent turnover\\n- Predictive modeling for better marketing\\n- Profiling and behavior analysis\\n- Ad placement optimization to identify key metrics\\n- One-to-one marketing for improved campaign analysis\\n- Evaluation of ad campaign effectiveness\\n- Customer classification to predict purchase behavior or renewal rates\\n\\n### Customer Examples\\n\\n- Cisco saw a 15x increase in speed after implementing H2O into their Propensity to Buy (P2B) modeling factory.\\n- Paypal uses H2O's Deep Learning algorithm for fraud detection and prevention.\\n- ShareThis uses H2O for AdTech ROI maximization to optimize their advertising campaign placement. \\n- MarketShare uses H2O for marketing optimization to improve efficiency in cross-channel attribution and forecasting.\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2OBenefits.md', 'section': '## Required Resources\\n\\n### Hardware and Software\\n\\n- Java is required to run H2O. GNU compiler for Java and Open JDK are not supported. \\n- The amount of memory required depends on the size of your data. We recommend having four times as much memory as your largest dataset. \\n- To view a one-page document that outlines the system configurations we recommend, click [here](http://h2o.ai/product/recommended-systems-for-h2o/). \\n\\n### Data\\n\\nH2O works with tabular data, which can be imported as a single file or as a directory of files. The following formats are supported: \\n \\nCSV (delimited) files\\nORC\\nSVMLight\\nARFF\\nXLS\\nXLSX\\nAvro (without multifile parsing or column type modification)\\nParquet\\n\\n>Note that ORC is available only if H2O is running as a Hadoop job.\\n \\n The data does not need to be perfect, as some munging can be performed within H2O (such as excluding columns with a specified percentage of missing values). However, the more precise your dataset is, the more accurate your models will be. \\n \\n### Support Team\\n\\nH2O is designed to be easy to both set up and use, but we recommend assembling a team that includes: \\n\\n- A data scientist to create the models\\n- An IT specialist to help with deployment, especially for a large multi-node environment\\n- A UX developer, if you want to create a front-end interface for your application that uses H2O\\n\\nOur H2O team will work with you to help you get started with H2O.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md', 'section': '## Changes from H2O 2.8 to H2O 3.0\\n\\n### `h2o.exec`\\nThe `h2o.exec` command is no longer supported. Any workflows using `h2o.exec` must be revised to remove this command.  If the H2O 3.0 workflow contains any parameters or commands from H2O Classic, errors will result and the workflow will fail. \\n\\nThe purpose of `h2o.exec` was to wrap expressions so that they could be evaluated in a single `\\\\Exec2` call. For example, \\n `h2o.exec(fr[,1] + 2/fr[,3])`\\n and \\n `fr[,1] + 2/fr[,3]`\\nproduced the same results in H2O. However, the first example makes a single REST call and uses a single temp object, while the second makes several REST calls and uses several temp objects. \\n\\nDue to the improved architecture in H2O 3.0, the need to use `h2o.exec` has been eliminated, as the expression can be processed by R as an \"unwrapped\" typical R expression. \\n\\nCurrently, the only known exception is when `factor` is used in conjunction with `h2o.exec`. For example, `h2o.exec(fr$myIntCol <- factor(fr$myIntCol))` would become `fr$myIntCol <- as.factor(fr$myIntCol)`\\n\\nNote also that an array is not inside a string:\\n\\nAn int array is [1, 2, 3], *not* \"[1, 2, 3]\".\\n\\nA String array is [\"f00\", \"b4r\"], *not* \"[\\\\\"f00\\\\\", \\\\\"b4r\\\\\"]\"\\n\\nOnly string values are enclosed in double quotation marks (`\"`).  \\n\\n<a name=\"h2operf\"></a>\\n### `h2o.performance`\\n\\nTo access any exclusively binomial output, use `h2o.performance`, optionally with the corresponding accessor. The accessor can only use the model metrics object created by `h2o.performance`. Each accessor is named for its corresponding field (for example, `h2o.AUC`, `h2o.gini`, `h2o.F1`). `h2o.performance` supports all current algorithms except for K-Means. \\n\\nIf you specify a data frame as a second parameter, H2O will use the specified data frame for scoring. If you do not specify a second parameter, the training metrics for the model metrics object are used. \\n\\n### `xval` and `validation` slots\\n\\nThe `xval` slot has been removed, as `nfolds` is not currently supported. \\n\\nThe `validation` slot has been merged with the `model` slot. \\n\\n### Principal Components Regression (PCR)\\n\\nPrincipal Components Regression (PCR) has also been deprecated. To obtain PCR values, create a Principal Components Analysis (PCA) model, then create a GLM model from the scored data from the PCA model. \\n\\n### Saving and Loading Models\\n\\nSaving and loading a model from R is supported in version 3.0.0.18 and later. H2O 3.0 uses the same binary serialization method as previous versions of H2O, but saves the model and its dependencies into a directory, with each object as a separate file. The `save_CV` option for  available in previous versions of H2O has been deprecated, as `h2o.saveAll` and `h2o.loadAll` are not currently supported. The following commands are now supported: \\n\\n- `h2o.saveModel`\\n- `h2o.loadModel`\\n\\n\\n\\n**Table of Contents**\\n\\n- [GBM](#GBM)\\n- [GLM](#GLM)\\n- [K-Means](#Kmeans)\\n- [Deep Learning](#DL)\\n- [Distributed Random Forest](#DRF)\\n\\n\\n\\n<a name=\"GBM\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md', 'section': '## GBM\\n\\nN-fold cross-validation and grid search will be supported in a future version of H2O 3.0. \\n\\n### Renamed GBM Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`n.trees` | `ntrees`\\n`interaction.depth` | `max_depth`\\n`n.minobsinnode` | `min_rows`\\n`shrinkage` | `learn_rate`\\n`n.bins` | `nbins`\\n`validation` | `validation_frame`\\n`balance.classes` | `balance_classes`\\n`max.after.balance.size` | `max_after_balance_size`\\n\\n\\n### Deprecated GBM Parameters\\n\\nThe following parameters have been removed: \\n\\n- `group_split`: Bit-set group splitting of categorical variables is now the default. \\n- `importance`: Variable importances are now computed automatically and displayed in the model output. \\n- `holdout.fraction`: The fraction of the training data to hold out for validation is no longer supported. \\n- `grid.parallelism`: Specifying the number of parallel threads to run during a grid search is no longer supported. Grid search will be supported in a future version of H2O 3.0. \\n\\n### New GBM Parameters\\n\\nThe following parameters have been added: \\n\\n- `seed`: A random number to control sampling and initialization when `balance_classes` is enabled. \\n- `score_each_iteration`: Display error rate information after each tree in the requested set is built. \\n- `build_tree_one_node`: Run on a single node to use fewer CPUs. \\n\\n### GBM Algorithm Comparison\\n\\nH2O Classic  | H2O 3.0 \\n------------- | -------------\\n`h2o.gbm <- function(` | `h2o.gbm <- function(` \\n`x,` |`x,`\\n`y,` |`y,` \\n`data,` | `training_frame,`\\n`key = \"\",` | `model_id,` \\n&nbsp; | `checkpoint`\\n`distribution = \\'multinomial\\',` | `distribution = c(\"AUTO\", \"gaussian\", \"bernoulli\", \"multinomial\", \"poisson\", \"gamma\", \"tweedie\"),` \\n&nbsp; | `tweedie_power = 1.5,`\\n`n.trees = 10,` | `ntrees = 50`\\n`interaction.depth = 5,` | `max_depth = 5,` \\n`n.minobsinnode = 10,` | `min_rows = 10,` \\n`shrinkage = 0.1,` | `learn_rate = 0.1,`\\n&nbsp; | `sample_rate = 1`\\n&nbsp; | `col_sample_rate = 1` \\n`n.bins = 20,`| `nbins = 20,` \\n&nbsp; | `nbins_top_level,`\\n&nbsp; | `nbins_cats = 1024,`\\n`validation,` | `validation_frame = NULL,` \\n`balance.classes = FALSE` | `balance_classes = FALSE,` \\n`max.after.balance.size = 5,` | `max_after_balance_size = 1,` \\n &nbsp; | `seed,` \\n &nbsp; | `build_tree_one_node = FALSE,`\\n &nbsp; | `nfolds = 0,`\\n &nbsp; | `fold_column = NULL,`\\n &nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),`\\n &nbsp; | `keep_cross_validation_predictions = FALSE,`\\n &nbsp; | `score_each_iteration = FALSE,`\\n &nbsp; | `stopping_rounds = 0,`\\n &nbsp; | `stopping_metric = c(\"AUTO\", \"deviance\", \"logloss\", \"MSE\", \"AUC\", \"r2\", \"misclassification\"),`\\n &nbsp; | `stopping_tolerance = 0.001,`\\n &nbsp; | `offset_column = NULL,`\\n &nbsp; | `weights_column = NULL,`\\n`group_split = TRUE,` | \\n`importance = FALSE,` | \\n`holdout.fraction = 0,` | \\n`class.sampling.factors = NULL,` | \\n`grid.parallelism = 1)` | \\n\\n### Output\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | -------------\\n`@model$priorDistribution`| &nbsp;  | `all`\\n`@model$params` | `@allparameters` | `all`\\n`@model$err` | `@model$scoring_history` | `all`\\n`@model$classification` | &nbsp;  | `all`\\n`@model$varimp` | `@model$variable_importances` | `all`\\n`@model$confusion` | `@model$training_metrics@metrics$cm$table`  | `binomial` and `multinomial`\\n`@model$auc` | `@model$training_metrics@metrics$AUC`  | `binomial`\\n`@model$gini` | `@model$training_metrics@metrics$Gini`  | `binomial`\\n`@model$best_cutoff` | &nbsp;  | `binomial`\\n`@model$F1` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f1`  | `binomial`\\n`@model$F2` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f2`  | `binomial`\\n`@model$accuracy` | `@model$training_metrics@metrics$thresholds_and_metric_scores$accuracy`  | `binomial`\\n`@model$error` | &nbsp;  | `binomial`\\n`@model$precision` | `@model$training_metrics@metrics$thresholds_and_metric_scores$precision`  | `binomial`\\n`@model$recall` | `@model$training_metrics@metrics$thresholds_and_metric_scores$recall`  | `binomial`\\n`@model$mcc` | `@model$training_metrics@metrics$thresholds_and_metric_scores$absolute_MCC`  | `binomial`\\n`@model$max_per_class_err` | currently replaced by `@model$training_metrics@metrics$thresholds_and_metric_scores$min_per_class_correct`  | `binomial`\\n\\n\\n\\n\\n\\n---\\n\\n<a name=\"GLM\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md', 'section': '## GLM\\n\\nN-fold cross-validation and grid search will be supported in a future version of H2O 3.0. \\n\\n### Renamed GLM Parameters\\n\\nThe following parameters have been renamed, but retain the same functions:\\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`nlambda` | `nlambdas`\\n`lambda.min.ratio` | `lambda_min_ratio`\\n `iter.max` | `max_iterations`\\n `epsilon` | `beta_epsilon`\\n\\n### Deprecated GLM Parameters\\n \\nThe following parameters have been removed: \\n \\n - `return_all_lambda`: A logical value indicating whether to return every model built during the lambda search. (may be re-added)\\n - `higher_accuracy`: For improved accuracy, adjust the `beta_epsilon` value. \\n - `strong_rules`: Discards predictors likely to have 0 coefficients prior to model building. (may be re-added as enabled by default)\\n - `non_negative`: Specify a non-negative response. (may be re-added)\\n - `variable_importances`: Variable importances are now computed automatically and displayed in the model output. They have been renamed to *Normalized Coefficient Magnitudes*. \\n - `disable_line_search`: This parameter has been deprecated, as it was mainly used for testing purposes. \\n - `offset`: Specify a column as an offset. (may be re-added)\\n - `max_predictors`: Stops training the algorithm if the number of predictors exceeds the specified value. (may be re-added)\\n\\n### New GLM Parameters\\n \\n The following parameters have been added: \\n \\n - `validation_frame`: Specify the validation dataset. \\n - `solver`: Select IRLSM or LBFGS. \\n\\n### GLM Algorithm Comparison\\n\\n\\nH2O Classic | H2O 3.0 \\n------------- | -------------\\n`h2o.glm <- function(` | `h2o.startGLMJob <- function(`\\n`x,` | `x,`\\n`y,` | `y,` \\n`data,` |`training_frame,` \\n`key = \"\",` | `model_id,` \\n &nbsp; | `validation_frame`\\n`iter.max = 100,` |  `max_iterations = 50,` \\n`epsilon = 1e-4` | `beta_epsilon = 0` \\n`strong_rules = TRUE,` | \\n`return_all_lambda = FALSE,` | \\n`intercept = TRUE,` | `intercept = TRUE`\\n`non_negative = FALSE,` | \\n&nbsp; | `solver = c(\"IRLSM\", \"L_BFGS\"),`\\n`standardize = TRUE,` | `standardize = TRUE,` \\n`family,` | `family = c(\"gaussian\", \"binomial\", \"poisson\", \"gamma\", \"tweedie\"),` \\n`link,` | `link = c(\"family_default\", \"identity\", \"logit\", \"log\", \"inverse\", \"tweedie\"),`\\n`tweedie.p = ifelse(family == \"tweedie\",1.5, NA_real_)` |  `tweedie_variance_power = NaN,`\\n&nbsp; | `tweedie_link_power = NaN,`\\n`alpha = 0.5,` | `alpha = 0.5,` \\n`prior = NULL` | `prior = 0.0,` \\n`lambda = 1e-5,` | `lambda = 1e-05,` \\n`lambda_search = FALSE,` | `lambda_search = FALSE,` \\n`nlambda = -1,` | `nlambdas = -1,` \\n`lambda.min.ratio = -1,` | `lambda_min_ratio = 1.0,` \\n`use_all_factor_levels = FALSE` | `use_all_factor_levels = FALSE,` \\n`nfolds = 0,` |  `nfolds = 0,`\\n`beta_constraints = NULL,` | `beta_constraint = NULL)` \\n`higher_accuracy = FALSE,` |  \\n`variable_importances = FALSE,` | \\n`disable_line_search = FALSE,` | \\n`offset = NULL,` | \\n`max_predictors = -1)` |\\n\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | -------------\\n`@model$params` | `@allparameters` | `all`\\n`@model$coefficients` | `@model$coefficients` | `all`\\n`@model$nomalized_coefficients` | `@model$coefficients_table$norm_coefficients` | `all`\\n`@model$rank` | `@model$rank` | `all`\\n`@model$iter` |`@model$iter` | `all`\\n`@model$lambda` | &nbsp;  | `all`\\n`@model$deviance` | `@model$residual_deviance` | `all`\\n`@model$null.deviance` | `@model$null_deviance` | `all`\\n`@model$df.residual` | `@model$residual_degrees_of_freedom` | `all`\\n`@model$df.null` | `@model$null_degrees_of_freedom` | `all`\\n`@model$aic` | `@model$AIC`| `all`\\n`@model$train.err` |  &nbsp; | `binomial`\\n`@model$prior` | &nbsp;  | `binomial`\\n`@model$thresholds` | `@model$threshold` | `binomial`\\n`@model$best_threshold` | &nbsp;  | `binomial`\\n`@model$auc` | `@model$AUC` | `binomial`\\n`@model$confusion` | &nbsp;  | `binomial`\\n\\n<a name=\"Kmeans\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md', 'section': '## K-Means\\n\\n### Renamed K-Means Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`centers` | `k`\\n`cols` | `x`\\n`iter.max` | `max_iterations`\\n`normalize` | `standardize`\\n\\n**Note** In H2O, the `normalize` parameter was disabled by default. The `standardize` parameter is enabled by default in H2O 3.0 to provide more accurate results for datasets containing columns with large values. \\n\\n### New K-Means Parameters\\n\\nThe following parameters have been added:\\n\\n- `user` has been added as an additional option for the `init` parameter. Using this parameter forces the K-Means algorithm to start at the user-specified points. \\n- `user_points`: Specify starting points for the K-Means algorithm. \\n\\n### K-Means Algorithm Comparison\\n\\nH2O Classic | H2O 3.0\\n------------- | -------------\\n`h2o.kmeans <- function(` | `h2o.kmeans <- function(`\\n`data,` | `training_frame,` \\n`cols = \\'\\',` | `x,`\\n`centers,` | `k,`\\n`key = \"\",` | `model_id,`\\n`iter.max = 10,` | `max_iterations = 1000,`\\n`normalize = FALSE,`  | `standardize = TRUE,`\\n`init = \"none\",` | `init = c(\"Furthest\",\"Random\", \"PlusPlus\"),`\\n`seed = 0,` | `seed)`\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O and the corresponding component name in H2O 3.0 (if supported).\\n\\nH2O Classic | H2O 3.0\\n------------- | -------------\\n`@model$params` | `@allparameters`\\n`@model$centers` | `@model$centers`\\n`@model$tot.withinss` | `@model$tot_withinss`\\n`@model$size` | `@model$size`\\n`@model$iter` | `@model$iterations`\\n&nbsp; | `@model$_scoring_history`\\n&nbsp; | `@model$_model_summary`\\n\\n---\\n\\n<a name=\"DL\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md', 'section': '## Deep Learning\\n\\nN-fold cross-validation and grid search will be supported in a future version of H2O 3.0. \\n\\n**Note**: If the results in the confusion matrix are incorrect, verify that `score_training_samples` is equal to 0. By default, only the first 10,000 rows are included. \\n\\n### Renamed Deep Learning Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`validation` | `validation_frame`\\n`class.sampling.factors` | `class_sampling_factors`\\n`override_with_best_model` | `overwrite_with_best_model`\\n`dlmodel@model$valid_class_error` | `@model$validation_metrics@$MSE`\\n\\n\\n### Deprecated DL Parameters\\n\\nThe following parameters have been removed:\\n\\n- `classification`: Classification is now inferred from the data type.\\n- `holdout_fraction`: Fraction of the training data to hold out for validation.\\n- `dlmodel@model$best_cutoff`: This output parameter has been removed. \\n- `max_hit_ratio_k` : This parameter has been removed. \\n\\n### New DL Parameters\\n\\nThe following parameters have been added: \\n\\n- `export_weights_and_biases`: An additional option allowing users to export the raw weights and biases as H2O frames. \\n\\nThe following options for the `loss` parameter have been added:\\n\\n- `absolute`: Provides strong penalties for mispredictions \\n- `huber`: Can improve results for regression \\n\\n### DL Algorithm Comparison\\n\\nH2O Classic  | H2O 3.0 \\n------------- | -------------\\n`h2o.deeplearning <- function(x,` | `h2o.deeplearning (x, `\\n`y,` | `y,`\\n`data,` | `training_frame,` \\n`key = \"\",` | `model_id = \"\",`\\n`override_with_best_model,` | `overwrite_with_best_model = true,` \\n`classification = TRUE,` | \\n`nfolds = 0,` |  `nfolds = 0`\\n`validation,` | `validation_frame,` \\n`holdout_fraction = 0,` |  \\n`checkpoint = \" \"` | `checkpoint,` \\n`autoencoder,` | `autoencoder = false,` \\n`use_all_factor_levels,` | `use_all_factor_levels = true`\\n`activation,` | `_activation = c(\"Rectifier\", \"Tanh\", \"TanhWithDropout\", \"RectifierWithDropout\", \"Maxout\", \"MaxoutWithDropout\"),`\\n`hidden,` | `hidden= c(200, 200),`\\n`epochs,` | `epochs = 10.0,`\\n`train_samples_per_iteration,` |`train_samples_per_iteration = -2,`\\n&nbsp; | `target_ratio_comm_to_comp = 0.05`\\n`seed,` | `_seed,` \\n`adaptive_rate,` | `adaptive_rate = true,` \\n`rho,` | `rho = 0.99,` \\n`epsilon,` | `epsilon = 1e-08,` \\n`rate,` | `rate = .005,` \\n`rate_annealing,` | `rate_annealing = 1e-06,` \\n`rate_decay,` | `rate_decay = 1.0,` \\n`momentum_start,` | `momentum_start = 0,`\\n`momentum_ramp,` | `momentum_ramp = 1e+06,`\\n`momentum_stable,` | `momentum_stable = 0,` \\n`nesterov_accelerated_gradient,` | `nesterov_accelerated_gradient = true,`\\n`input_dropout_ratio,` | `input_dropout_ratio = 0.0,` \\n`hidden_dropout_ratios,` | `hidden_dropout_ratios,` \\n`l1,` | `l1 = 0.0,` \\n`l2,` | `l2 = 0.0,` \\n`max_w2,` | `max_w2 = Inf,`\\n`initial_weight_distribution,` | `initial_weight_distribution = c(\"UniformAdaptive\",\"Uniform\", \"Normal\"),`\\n`initial_weight_scale,` | `initial_weight_scale = 1.0,`\\n`loss,` | `loss = \"Automatic\", \"CrossEntropy\", \"Quadratic\", \"Absolute\", \"Huber\"),`\\n&nbsp; | `distribution = c(\"AUTO\", \"gaussian\", \"bernoulli\", \"multinomial\", \"poisson\", \"gamma\", \"tweedie\", \"laplace\", \"huber\"),`\\n&nbsp; | `tweedie_power = 1.5,`\\n`score_interval,` | `score_interval = 5,` \\n`score_training_samples,` | `score_training_samples = 10000l,` \\n`score_validation_samples,` | `score_validation_samples = 0l,`\\n`score_duty_cycle,` | `score_duty_cycle = 0.1,` \\n`classification_stop,` | `classification_stop = 0`\\n`regression_stop,` | `regression_stop = 1e-6,`\\n&nbsp; | `stopping_rounds = 5,`\\n&nbsp; | `stopping_metric = c(\"AUTO\", \"deviance\", \"logloss\", \"MSE\", \"AUC\", \"r2\", \"misclassification\"),`\\n&nbsp; | `stopping_tolerance = 0,`\\n`quiet_mode,` | `quiet_mode = false,`\\n`max_confusion_matrix_size,` | `max_confusion_matrix_size,`\\n`max_hit_ratio_k,` | \\n`balance_classes,` | `balance_classes = false,`\\n`class_sampling_factors,` | `class_sampling_factors,`\\n`max_after_balance_size,` | `max_after_balance_size,` \\n`score_validation_sampling,` | `score_validation_sampling,`\\n`diagnostics,` | `diagnostics = true,` \\n`variable_importances,` | `variable_importances = false,`\\n`fast_mode,` | `fast_mode = true,` \\n`ignore_const_cols,` | `ignore_const_cols = true,`\\n`force_load_balance,` | `force_load_balance = true,`\\n`replicate_training_data,` | `replicate_training_data = true,`\\n`single_node_mode,` | `single_node_mode = false,` \\n`shuffle_training_data,` | `shuffle_training_data = false,`\\n`sparse,` | `sparse = false,` \\n`col_major,` | `col_major = false,`\\n`max_categorical_features,` | `max_categorical_features,`\\n`reproducible)` | `reproducible=FALSE,` \\n`average_activation` | `average_activation = 0,`\\n &nbsp; | `sparsity_beta = 0`\\n &nbsp; | `export_weights_and_biases=FALSE,`\\n &nbsp; | `offset_column = NULL,` \\n &nbsp; | `weights_column = NULL,`\\n &nbsp; | `nfolds = 0,`\\n &nbsp; | `fold_column = NULL,`\\n &nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),`\\n &nbsp; | `keep_cross_validation_predictions = FALSE)`\\n \\n \\n### Output\\n\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | ------------- \\n`@model$priorDistribution`| &nbsp;  | `all`\\n`@model$params` | `@allparameters` | `all`\\n`@model$train_class_error` | `@model$training_metrics@metrics@$MSE`  | `all`\\n`@model$valid_class_error` | `@model$validation_metrics@$MSE` | `all`\\n`@model$varimp` | `@model$_variable_importances` | `all`\\n`@model$confusion` | `@model$training_metrics@metrics$cm$table`  | `binomial` and `multinomial`\\n`@model$train_auc` | `@model$train_AUC`  | `binomial`\\n&nbsp; | `@model$_validation_metrics` | `all`\\n&nbsp; | `@model$_model_summary` | `all`\\n&nbsp; | `@model$_scoring_history` | `all`\\n\\n \\n ---\\n\\n<a name=\"DRF\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md', 'section': '## Distributed Random Forest\\n\\n### Changes to DRF in H2O 3.0 \\n\\nDistributed Random Forest (DRF) was represented as `h2o.randomForest(type=\"BigData\", ...)` in H2O Classic. In H2O Classic, SpeeDRF (`type=\"fast\"`) was not as accurate, especially for complex data with categoricals, and did not address regression problems. DRF (`type=\"BigData\"`) was at least as accurate as SpeeDRF (`type=\"fast\"`) and was the only algorithm that scaled to big data (data too large to fit on a single node). \\nIn H2O 3.0, our plan is to improve the performance of DRF so that the data fits on a single node (optimally, for all cases), which will make SpeeDRF obsolete. Ultimately, the goal is provide a single algorithm that provides the \"best of both worlds\" for all datasets and use cases. \\nPlease note that H2O does not currently support the ability to specify the number of trees when using `h2o.predict` for a DRF model. \\n\\n**Note**: H2O 3.0 only supports DRF. SpeeDRF is no longer supported. The functionality of DRF in H2O 3.0 is similar to DRF functionality in H2O. \\n\\n### Renamed DRF Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`validation` | `validation_frame`\\n`sample.rate` | `sample_rate`\\n`ntree` | `ntrees` \\n`depth` | `max_depth`\\n`balance.classes` | `balance_classes`\\n`score.each.iteration` | `score_each_iteration`\\n`class.sampling.factors` | `class_sampling_factors`\\n`nodesize` | `min_rows`\\n\\n\\n### Deprecated DRF Parameters\\n\\nThe following parameters have been removed: \\n\\n- `classification`: This is now automatically inferred from the response type. To achieve classification with a 0/1 response column, explicitly convert the response to a factor (`as.factor()`). \\n- `importance`: Variable importances are now computed automatically and displayed in the model output. \\n- `holdout.fraction`: Specifying the fraction of the training data to hold out for validation is no longer supported. \\n- `doGrpSplit`: The bit-set group splitting of categorical variables is now the default. \\n- `verbose`: Infonrmation about tree splits and extra statistics is now included automatically in the stdout. \\n- `oobee`: The out-of-bag error estimate is now computed automatically (if no validation set is specified).\\n- `stat.type`: This parameter was used for SpeeDRF, which is no longer supported.\\n- `type`: This parameter was used for SpeeDRF, which is no longer supported. \\n\\n\\n\\n### New DRF Parameters\\n\\nThe following parameter has been added: \\n\\n- `build_tree_one_node`: Run on a single node to use fewer CPUs. \\n\\n### DRF Algorithm Comparison\\n\\nH2O Classic | H2O 3.0\\n------------- | -------------\\n`h2o.randomForest <- function(x,` | `h2o.randomForest <- function(`\\n`x,` | `x,` \\n`y,` | `y,` \\n`data,` | `training_frame,` \\n`key=\"\",` | `model_id,` \\n`validation,` | `validation_frame,` \\n`mtries = -1,` | `mtries = -1,` \\n`sample.rate=2/3,` | `sample_rate = 0.632,` \\n &nbsp; | `build_tree_one_node = FALSE,` \\n`ntree=50` | `ntrees=50,` \\n`depth=20,` | `max_depth = 20,` \\n &nbsp; | `min_rows = 1,`\\n`nbins=20,` | `nbins = 20,` \\n`balance.classes = FALSE,` | `balance_classes = FALSE,` \\n`score.each.iteration = FALSE,` | `score_each_iteration = FALSE,` \\n`seed = -1,` | `seed` \\n`nodesize = 1,` |  \\n`classification=TRUE,` | \\n`importance=FALSE,` | \\n`nfolds=0,` | \\n`holdout.fraction = 0,` | \\n`max.after.balance.size = 5,` | `max_after_balance_size)` \\n`class.sampling.factors = NULL,` | &nbsp; \\n`doGrpSplit = TRUE,` | \\n`verbose = FALSE,` |\\n`oobee = TRUE,` | \\n`stat.type = \"ENTROPY\",` |\\n`type = \"fast\")` | \\n\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | -------------\\n`@model$priorDistribution`| &nbsp;  | `all`\\n`@model$params` | `@allparameters` | `all`\\n`@model$mse` | `@model$scoring_history` | `all`\\n`@model$forest` | `@model$model_summary`  | `all`\\n`@model$classification` | &nbsp;  | `all`\\n`@model$varimp` | `@model$variable_importances` | `all`\\n`@model$confusion` | `@model$training_metrics@metrics$cm$table`  | `binomial` and `multinomial`\\n`@model$auc` | `@model$training_metrics@metrics$AUC`  | `binomial`\\n`@model$gini` | `@model$training_metrics@metrics$Gini`  | `binomial`\\n`@model$best_cutoff` | &nbsp;  | `binomial`\\n`@model$F1` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f1`  | `binomial`\\n`@model$F2` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f2`  | `binomial`\\n`@model$accuracy` | `@model$training_metrics@metrics$thresholds_and_metric_scores$accuracy`  | `binomial`\\n`@model$Error` | `@model$Error`  | `binomial`\\n`@model$precision` | `@model$training_metrics@metrics$thresholds_and_metric_scores$precision`  | `binomial`\\n`@model$recall` | `@model$training_metrics@metrics$thresholds_and_metric_scores$recall`  | `binomial`\\n`@model$mcc` | `@model$training_metrics@metrics$thresholds_and_metric_scores$absolute_MCC`  | `binomial`\\n`@model$max_per_class_err` | currently replaced by `@model$training_metrics@metrics$thresholds_and_metric_scores$min_per_class_correct`  | `binomial`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/JavaChanges.md', 'section': '## Unify distribution parameter cross algorithms\\n\\nThe representation of the distribution family has been unified across the H2O code base. The `GBMParameters#_distribution` type has been changed from `GBMModel.GBMParameters.Family` to `hex.genmodel.utils.DistributionFamily`. The enum `GBMModel.GBMParameters.Family` has been deprecated. Use the enum `hex.genmodel.utils.DistributionFamily` instead.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/JavaChanges.md', 'section': '## `ValueString#equals` semantics changed\\n\\nThis change affects all comparisons using the form `new ValueString(\"test\") == \"test\"`. In previous versions of H2O, the method `water.parser.BufferedString#equals` was used for comparing Java strings. This method has been deprecated; instead, use the `toString` method to convert the ValueString to a Java string, then compare the results using the `String#equals` method.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/JavaChanges.md', 'section': '## Start of H2O client app changed\\n\\nThe method `water.H2OClientApp#start` has been deprecated. Use the `main` method instead.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/JavaChanges.md', 'section': '## Use of type parameter for `water.Key` unified\\n\\nAll methods accepting or returning `water.Key` have been changed to always accept or return a generic form of `Key<T>`. For example, a signature of the method `Key#make` has been changed to `public static <P extends Keyed> Key<P> make()`. Clients should always use `Key` with a specific target type (e.g., `Key<Frame>`, `Key<Model>`).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Shadowing\\n\\nSince release `3.30.1.1`, Jetty 9 is bundled even for Hadoop 2.x based distributions. To avoid classpath collisions, as Hadoop 2.x distributions typically\\nbundle some version of Jetty 8, the Jetty 9 inside H2O is **shadowed**. \\n\\n- The original package names starting with `org.eclipse.jetty.*` have been shadowed by using `ai.h2o` prefix, resulting in\\n`ai.h2o.org.eclipse.jetty.*` pattern.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': \"## LDAP login module\\n\\nH2O's official [documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/security.html) advises to use `ai.h2o.org.eclipse.jetty.plus.jaas.spi.LdapLoginModule` class for LDAP configuration.\\nSuch configuration is still the **valid** one after this change.\\nHowever, due to Jetty not being shadow in previous versions, some users still used the variant without `ai.h2o` package prefix: `org.eclipse.jetty.plus.jaas.spi.LdapLoginModule`.\\nOn Hadoop 3 based distributions, where Jetty 9 had already been present before this complete migration to Jetty 9, some users might have been able to use `org.eclipse.jetty.jaas.spi.LdapLoginModule` \\n(missing `plus` package). This is no longer possible and users should always turn to the official [documentation](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/security.html)\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': \"## Embedded H2O implications\\n\\nThe module `h2o-jetty-9` builds a `jar` containing eclipse packages in the relocated form to the `ai.h2o.org.eclipse.*` package.\\nThe same applies for `h2o-jetty-8` module, which is now considered deprecated. Any systems embedding H2O are advised to use `h2o-jetty-9` module instead.\\nAPI of Jetty 8 and 9 varies, therefore in the unlikely case of H2O being embedded in a system which uses Jetty APIs directly, changes related to the API differences must be made.\\n\\n# Migrating to H2O 3.0\\n\\nWe're excited about the upcoming release of the latest and greatest version of H2O, and we hope you are too! H2O 3.0 has lots of improvements, including: \\n\\n- Powerful Python APIs\\n- Flow, a brand-new intuitive web UI\\n- The ability to share, annotate, and modify workflows\\n- Versioned REST APIs with full metadata\\n- Spark integration using Sparkling Water\\n- Improved algorithm accuracy and speed\\n\\nand much more! Overall, H2O has been retooled for better accuracy and performance and to provide additional functionality. If you're a current user of H2O, we strongly encourage you to upgrade to the latest version to take advantage of the latest features and capabilities. \\n\\nPlease be aware that H2O 3.0 will supersede all previous versions of H2O as the primary version as of May 15th, 2015. Support for previous versions will be offered for a limited time, but there will no longer be any significant updates to the previous version of H2O. \\n\\nThe following information and links will inform you about what's new and different and help you prepare to upgrade to H2O 3.0. \\n\\nOverall, H2O 3.0 is more stable, elegant, and simplified, with additional capabilities not available in previous versions of H2O. \\n\\n---\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Algorithm Changes\\n\\nMost of the algorithms available in previous versions of H2O have been improved in terms of speed and accuracy. Currently available model types include:\\n\\n### Supervised \\n\\n- **Generalized Linear Model (GLM)**: Binomial classification, multinomial classification, regression (including logistic regression)\\n- **Distributed Random Forest (DRF)**: Binomial classification, multinomial classification, regression\\n- **Gradient Boosting Machine (GBM)**: Binomial classification, multinomial classification, regression\\n- **Deep Learning (DL)**: Binomial classification, multinomial classification, regression\\n- Naive Bayes\\n- Stacked Ensembles\\n- XGBoost\\n\\n### Unsupervised\\n\\n- K-means\\n- Principal Component Analysis\\n- Autoencoder\\n- Generalized Low Rank Models\\n\\n### Miscellaneous\\n\\n- **Word2vec**\\n\\nCheck back for updates, as these algorithms will be re-introduced in an improved form in a future version of H2O. \\n\\n**Note**: The SpeeDRF model has been removed, as it was originally intended as an optimization for small data only. This optimization will be added to the Distributed Random Forest model automatically for small data in a future version of H2O. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Parsing Changes\\n\\nIn H2O Classic, the parser reads all the data and tries to guess the column type. In H2O 3.0, the parser reads a subset and makes a type guess for each column. In Flow, you can view the preliminary parse results in the **Edit Column Names and Types** area. To change the column type, select an option from the drop-down menu to the right  of the column. H2O 3.0 can also automatically identify mixed-type columns; in H2O Classic, if one column is mixed integers or real numbers using a string, the output is blank. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Web UI Changes\\n\\nOur web UI has been completely overhauled with a much more intuitive interface that is similar to IPython Notebook. Each point-and-click action is translated immediately into an individual workflow script that can be saved for later interactive and offline use.  As a result, you can now revise and rerun your workflows easily, and can even add comments and rich media. \\n\\nFor more information, refer to our [Getting Started with Flow](https://github.com/h2oai/h2o-dev/blob/master/h2o-docs/src/product/flow/README.md) guide, which comprehensively documents how to use Flow. You can also view this brief [video](https://www.youtube.com/watch?v=wzeuFfbW7WE), which provides an overview of Flow in action. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': \"## API Users\\n\\nH2O's new Python API allows Pythonistas to use H2O in their favorite environment. Using the Python command line or an integrated development environment like IPython Notebook, H2O users can control clusters and manage massive datasets quickly. \\n\\nH2O's REST API is the basis for the web UI (Flow), as well as the R and Python APIs, and is versioned for stability. It is also easier to understand and use, with full metadata available dynamically from the server, allowing for easier integration by developers. \\n\\n---\"}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Java Users\\n\\nGenerated Java REST classes ease REST API use by external programs running in a Java Virtual Machine (JVM).\\n\\nAs in previous versions of H2O, users can export trained models as Java objects for easy integration into JVM applications. H2O is currently the only ML tool that provides this capability, making it the data science tool of choice for enterprise developers. \\n\\n---'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## R Users\\n\\nIf you use H2O primarily in R, be aware that as a result of the improvements to the R package for H2O scripts created using previous versions (Nunes 2.8.6.2 or prior) will require minor revisions to work with H2O 3.0. \\n\\nTo assist our R users in upgrading to H2O 3.0, a \"shim\" tool has been developed. The [shim](https://github.com/h2oai/h2o-dev/blob/9795c401b7be339be56b1b366ffe816133cccb9d/h2o-r/h2o-package/R/shim.R) reviews your script, identifies deprecated or revised parameters and arguments, and suggests replacements. \\n\\n  >**Note**: As of Slater v.3.2.0.10, this shim will no longer be available. \\n\\nThere is also an [R Porting Guide](#PortingGuide) that provides a side-by-side comparison of the algorithms in the previous version of H2O with H2O 3.0. It outlines the new, revised, and deprecated parameters for each algorithm, as well as the changes to the output. \\n\\n---\\n\\n<a name=\"PortingGuide\"></a>\\n\\n# Porting R Scripts\\n\\nThis document outlines how to port R scripts written in previous versions of H2O (Nunes 2.8.6.2 or prior, also known as \"H2O Classic\") for compatibility with the new H2O 3.0 API. When upgrading from H2O to H2O 3.0, most functions are the same. However, there are some differences that will need to be resolved when porting any scripts that were originally created using H2O to H2O 3.0. \\n\\nThe original R script for H2O is listed first, followed by the updated script for H2O 3.0. \\n\\nSome of the parameters have been renamed for consistency. For each algorithm, a table that describes the differences is provided. \\n\\nFor additional assistance within R, enter a question mark before the command (for example, `?h2o.glm`). \\n\\nThere is also a \"shim\" available that will review R scripts created with previous versions of H2O, identify deprecated or renamed parameters, and suggest replacements. For more information, refer to the repo [here](https://github.com/h2oai/h2o-dev/blob/d9693a97da939a2b77c24507c8b40a5992192489/h2o-r/h2o-package/R/shim.R).'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Changes from H2O 2.8 to H2O 3.0\\n\\n### `h2o.exec`\\nThe `h2o.exec` command is no longer supported. Any workflows using `h2o.exec` must be revised to remove this command.  If the H2O 3.0 workflow contains any parameters or commands from H2O Classic, errors will result and the workflow will fail. \\n\\nThe purpose of `h2o.exec` was to wrap expressions so that they could be evaluated in a single `\\\\Exec2` call. For example, \\n `h2o.exec(fr[,1] + 2/fr[,3])`\\n and \\n `fr[,1] + 2/fr[,3]`\\nproduced the same results in H2O. However, the first example makes a single REST call and uses a single temp object, while the second makes several REST calls and uses several temp objects. \\n\\nDue to the improved architecture in H2O 3.0, the need to use `h2o.exec` has been eliminated, as the expression can be processed by R as an \"unwrapped\" typical R expression. \\n\\nCurrently, the only known exception is when `factor` is used in conjunction with `h2o.exec`. For example, `h2o.exec(fr$myIntCol <- factor(fr$myIntCol))` would become `fr$myIntCol <- as.factor(fr$myIntCol)`\\n\\nNote also that an array is not inside a string:\\n\\nAn int array is [1, 2, 3], *not* \"[1, 2, 3]\".\\n\\nA String array is [\"f00\", \"b4r\"], *not* \"[\\\\\"f00\\\\\", \\\\\"b4r\\\\\"]\"\\n\\nOnly string values are enclosed in double quotation marks (`\"`).  \\n\\n<a name=\"h2operf\"></a>\\n### `h2o.performance`\\n\\nTo access any exclusively binomial output, use `h2o.performance`, optionally with the corresponding accessor. The accessor can only use the model metrics object created by `h2o.performance`. Each accessor is named for its corresponding field (for example, `h2o.AUC`, `h2o.gini`, `h2o.F1`). `h2o.performance` supports all current algorithms except for K-Means. \\n\\nIf you specify a data frame as a second parameter, H2O will use the specified data frame for scoring. If you do not specify a second parameter, the training metrics for the model metrics object are used. \\n\\n### `xval` and `validation` slots\\n\\nThe `xval` slot has been removed, as `nfolds` is not currently supported. \\n\\nThe `validation` slot has been merged with the `model` slot. \\n\\n### Principal Components Regression (PCR)\\n\\nPrincipal Components Regression (PCR) has also been deprecated. To obtain PCR values, create a Principal Components Analysis (PCA) model, then create a GLM model from the scored data from the PCA model. \\n\\n### Saving and Loading Models\\n\\nSaving and loading a model from R is supported in version 3.0.0.18 and later. H2O 3.0 uses the same binary serialization method as previous versions of H2O, but saves the model and its dependencies into a directory, with each object as a separate file. The `save_CV` option for  available in previous versions of H2O has been deprecated, as `h2o.saveAll` and `h2o.loadAll` are not currently supported. The following commands are now supported: \\n\\n- `h2o.saveModel`\\n- `h2o.loadModel`\\n\\n\\n\\n**Table of Contents**\\n\\n- [GBM](#GBM)\\n- [GLM](#GLM)\\n- [K-Means](#Kmeans)\\n- [Deep Learning](#DL)\\n- [Distributed Random Forest](#DRF)\\n\\n\\n\\n<a name=\"GBM\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## GBM\\n\\nN-fold cross-validation and grid search are currently supported in H2O 3.0. \\n\\n### Renamed GBM Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`n.trees` | `ntrees`\\n`interaction.depth` | `max_depth`\\n`n.minobsinnode` | `min_rows`\\n`shrinkage` | `learn_rate`\\n`n.bins` | `nbins`\\n`validation` | `validation_frame`\\n`balance.classes` | `balance_classes`\\n`max.after.balance.size` | `max_after_balance_size`\\n\\n\\n### Deprecated GBM Parameters\\n\\nThe following parameters have been removed: \\n\\n- `group_split`: Bit-set group splitting of categorical variables is now the default. \\n- `importance`: Variable importances are now computed automatically and displayed in the model output. \\n- `holdout.fraction`: The fraction of the training data to hold out for validation is no longer supported. \\n- `grid.parallelism`: Specifying the number of parallel threads to run during a grid search is no longer supported. \\n\\n### New GBM Parameters\\n\\nThe following parameters have been added: \\n\\n- `seed`: A random number to control sampling and initialization when `balance_classes` is enabled. \\n- `score_each_iteration`: Display error rate information after each tree in the requested set is built. \\n- `build_tree_one_node`: Run on a single node to use fewer CPUs. \\n\\n### GBM Algorithm Comparison\\n\\nH2O Classic  | H2O 3.0 \\n------------- | -------------\\n`h2o.gbm <- function(` | `h2o.gbm <- function(` \\n`x,` |`x,`\\n`y,` |`y,` \\n`data,` | `training_frame,`\\n`key = \"\",` | `model_id,` \\n&nbsp; | `checkpoint`\\n`distribution = \\'multinomial\\',` | `distribution = c(\"AUTO\", \"gaussian\", \"bernoulli\", \"multinomial\", \"poisson\", \"gamma\", \"tweedie\"),` \\n&nbsp; | `tweedie_power = 1.5,`\\n`n.trees = 10,` | `ntrees = 50`\\n`interaction.depth = 5,` | `max_depth = 5,` \\n`n.minobsinnode = 10,` | `min_rows = 10,` \\n`shrinkage = 0.1,` | `learn_rate = 0.1,`\\n&nbsp; | `sample_rate = 1`\\n&nbsp; | `col_sample_rate = 1` \\n`n.bins = 20,`| `nbins = 20,` \\n&nbsp; | `nbins_top_level,`\\n&nbsp; | `nbins_cats = 1024,`\\n`validation,` | `validation_frame = NULL,` \\n`balance.classes = FALSE` | `balance_classes = FALSE,` \\n`max.after.balance.size = 5,` | `max_after_balance_size = 1,` \\n &nbsp; | `seed,` \\n &nbsp; | `build_tree_one_node = FALSE,`\\n &nbsp; | `nfolds = 0,`\\n &nbsp; | `fold_column = NULL,`\\n &nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),`\\n &nbsp; | `keep_cross_validation_predictions = FALSE,`\\n &nbsp; | `score_each_iteration = FALSE,`\\n &nbsp; | `stopping_rounds = 0,`\\n &nbsp; | `stopping_metric = c(\"AUTO\", \"deviance\", \"logloss\", \"MSE\", \"AUC\", \"r2\", \"misclassification,\" \"mean_per_class_error\"),`\\n &nbsp; | `stopping_tolerance = 0.001,`\\n &nbsp; | `offset_column = NULL,`\\n &nbsp; | `weights_column = NULL,`\\n`group_split = TRUE,` | \\n`importance = FALSE,` | \\n`holdout.fraction = 0,` | \\n`class.sampling.factors = NULL,` | \\n`grid.parallelism = 1)` | \\n\\n\\n### Output\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | -------------\\n`@model$priorDistribution`| &nbsp;  | `all`\\n`@model$params` | `@allparameters` | `all`\\n`@model$err` | `@model$scoring_history` | `all`\\n`@model$classification` | &nbsp;  | `all`\\n`@model$varimp` | `@model$variable_importances` | `all`\\n`@model$confusion` | `@model$training_metrics@metrics$cm$table`  | `binomial` and `multinomial`\\n`@model$auc` | `@model$training_metrics@metrics$AUC`  | `binomial`\\n`@model$gini` | `@model$training_metrics@metrics$Gini`  | `binomial`\\n`@model$best_cutoff` | &nbsp;  | `binomial`\\n`@model$F1` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f1`  | `binomial`\\n`@model$F2` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f2`  | `binomial`\\n`@model$accuracy` | `@model$training_metrics@metrics$thresholds_and_metric_scores$accuracy`  | `binomial`\\n`@model$error` | &nbsp;  | `binomial`\\n`@model$precision` | `@model$training_metrics@metrics$thresholds_and_metric_scores$precision`  | `binomial`\\n`@model$recall` | `@model$training_metrics@metrics$thresholds_and_metric_scores$recall`  | `binomial`\\n`@model$mcc` | `@model$training_metrics@metrics$thresholds_and_metric_scores$absolute_MCC`  | `binomial`\\n`@model$max_per_class_err` | currently replaced by `@model$training_metrics@metrics$thresholds_and_metric_scores$min_per_class_correct`  | `binomial`\\n\\n\\n\\n\\n\\n---\\n\\n<a name=\"GLM\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## GLM\\n\\n### Renamed GLM Parameters\\n\\nThe following parameters have been renamed, but retain the same functions:\\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`nlambda` | `nlambdas`\\n`lambda.min.ratio` | `lambda_min_ratio`\\n `iter.max` | `max_iterations`\\n `epsilon` | `beta_epsilon`\\n\\n### Deprecated GLM Parameters\\n \\nThe following parameters have been removed: \\n \\n - `return_all_lambda`: A logical value indicating whether to return every model built during the lambda search. (may be re-added)\\n - `higher_accuracy`: For improved accuracy, adjust the `beta_epsilon` value. \\n - `strong_rules`: Discards predictors likely to have 0 coefficients prior to model building. (may be re-added as enabled by default)\\n - `non_negative`: Specify a non-negative response. (may be re-added)\\n - `variable_importances`: Variable importances are now computed automatically and displayed in the model output. They have been renamed to *Normalized Coefficient Magnitudes*. \\n - `disable_line_search`: This parameter has been deprecated, as it was mainly used for testing purposes. \\n - `max_predictors`: Stops training the algorithm if the number of predictors exceeds the specified value. (may be re-added)\\n\\n### New GLM Parameters\\n \\n The following parameters have been added: \\n \\n - `validation_frame`: Specify the validation dataset. \\n - `solver`: Select IRLSM or LBFGS. \\n\\n### GLM Algorithm Comparison\\n\\n\\nH2O Classic | H2O 3.0 \\n------------- | -------------\\n`h2o.glm <- function(` | `h2o.glm(`\\n`x,` | `x,`\\n`y,` | `y,` \\n`data,` |`training_frame,` \\n`key = \"\",` | `model_id,` \\n &nbsp; | `validation_frame = NULL`\\n`iter.max = 100,` |  `max_iterations = 50,` \\n`epsilon = 1e-4` | `beta_epsilon = 0` \\n`strong_rules = TRUE,` | \\n`return_all_lambda = FALSE,` | \\n`intercept = TRUE,` | `intercept = TRUE`\\n`non_negative = FALSE,` | \\n&nbsp; | `solver = c(\"IRLSM\", \"L_BFGS\"),`\\n`standardize = TRUE,` | `standardize = TRUE,` \\n`family,` | `family = c(\"gaussian\", \"binomial\", \"multinomial\", \"poisson\", \"gamma\", \"tweedie\"),` \\n`link,` | `link = c(\"family_default\", \"identity\", \"logit\", \"log\", \"inverse\", \"tweedie\"),`\\n`tweedie.p = ifelse(family == \"tweedie\",1.5, NA_real_)` |  `tweedie_variance_power = NaN,`\\n&nbsp; | `tweedie_link_power = NaN,`\\n`alpha = 0.5,` | `alpha = 0.5,` \\n`prior = NULL` | `prior = 0.0,` \\n`lambda = 1e-5,` | `lambda = 1e-05,` \\n`lambda_search = FALSE,` | `lambda_search = FALSE,` \\n`nlambda = -1,` | `nlambdas = -1,` \\n`lambda.min.ratio = -1,` | `lambda_min_ratio = 1.0,` \\n`use_all_factor_levels = FALSE` | `use_all_factor_levels = FALSE,` \\n`nfolds = 0,` |  `nfolds = 0,`\\n&nbsp; | `fold_column = NULL,`\\n&nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),`\\n&nbsp; | `keep_cross_validation_predictions = FALSE,`\\n`beta_constraints = NULL,` | `beta_constraints = NULL)` \\n`higher_accuracy = FALSE,` |  \\n`variable_importances = FALSE,` | \\n`disable_line_search = FALSE,` | \\n`offset = NULL,` | `offset_column = NULL,`\\n&nbsp; | `weights_column = NULL,`\\n&nbsp; | `intercept = TRUE,`\\n`max_predictors = -1)` | `max_active_predictors = -1)`\\n\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | -------------\\n`@model$params` | `@allparameters` | `all`\\n`@model$coefficients` | `@model$coefficients` | `all`\\n`@model$nomalized_coefficients` | `@model$coefficients_table$norm_coefficients` | `all`\\n`@model$rank` | `@model$rank` | `all`\\n`@model$iter` |`@model$iter` | `all`\\n`@model$lambda` | &nbsp;  | `all`\\n`@model$deviance` | `@model$residual_deviance` | `all`\\n`@model$null.deviance` | `@model$null_deviance` | `all`\\n`@model$df.residual` | `@model$residual_degrees_of_freedom` | `all`\\n`@model$df.null` | `@model$null_degrees_of_freedom` | `all`\\n`@model$aic` | `@model$AIC`| `all`\\n`@model$train.err` |  &nbsp; | `binomial`\\n`@model$prior` | &nbsp;  | `binomial`\\n`@model$thresholds` | `@model$threshold` | `binomial`\\n`@model$best_threshold` | &nbsp;  | `binomial`\\n`@model$auc` | `@model$AUC` | `binomial`\\n`@model$confusion` | &nbsp;  | `binomial`\\n\\n<a name=\"Kmeans\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## K-Means\\n\\n### Renamed K-Means Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`centers` | `k`\\n`cols` | `x`\\n`iter.max` | `max_iterations`\\n`normalize` | `standardize`\\n\\n**Note** In H2O, the `normalize` parameter was disabled by default. The `standardize` parameter is enabled by default in H2O 3.0 to provide more accurate results for datasets containing columns with large values. \\n\\n### New K-Means Parameters\\n\\nThe following parameters have been added:\\n\\n- `user` has been added as an additional option for the `init` parameter. Using this parameter forces the K-Means algorithm to start at the user-specified points. \\n- `user_points`: Specify starting points for the K-Means algorithm. \\n\\n### K-Means Algorithm Comparison\\n\\nH2O Classic | H2O 3.0\\n------------- | -------------\\n`h2o.kmeans <- function(` | `h2o.kmeans(`\\n`data,` | `training_frame,` \\n`cols = \\'\\',` | `x,`\\n`centers,` | `k,`\\n`key = \"\",` | `model_id,`\\n`iter.max = 10,` | `max_iterations = 1000,`\\n`normalize = FALSE,`  | `standardize = TRUE,`\\n`init = \"none\",` | `init = c(\"Furthest\",\"Random\", \"PlusPlus\"),`\\n`seed = 0,` | `seed,`\\n&nbsp; | `nfolds = 0,`\\n&nbsp; | `fold_column = NULL,` \\n&nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),`\\n&nbsp; | `keep_cross_validation_predictions = FALSE)`\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O and the corresponding component name in H2O 3.0 (if supported).\\n\\nH2O Classic | H2O 3.0\\n------------- | -------------\\n`@model$params` | `@allparameters`\\n`@model$centers` | `@model$centers`\\n`@model$tot.withinss` | `@model$tot_withinss`\\n`@model$size` | `@model$size`\\n`@model$iter` | `@model$iterations`\\n&nbsp; | `@model$_scoring_history`\\n&nbsp; | `@model$_model_summary`\\n\\n---\\n\\n<a name=\"DL\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Deep Learning\\n\\n**Note**: If the results in the confusion matrix are incorrect, verify that `score_training_samples` is equal to 0. By default, only the first 10,000 rows are included. \\n\\n### Renamed Deep Learning Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`validation` | `validation_frame`\\n`class.sampling.factors` | `class_sampling_factors`\\n`override_with_best_model` | `overwrite_with_best_model`\\n`dlmodel@model$valid_class_error` | `@model$validation_metrics@$MSE`\\n\\n\\n### Deprecated DL Parameters\\n\\nThe following parameters have been removed:\\n\\n- `classification`: Classification is now inferred from the data type.\\n- `holdout_fraction`: Fraction of the training data to hold out for validation.\\n- `dlmodel@model$best_cutoff`: This output parameter has been removed. \\n- `max_hit_ratio_k`: This parameter has been removed.\\n\\n### New DL Parameters\\n\\nThe following parameters have been added: \\n\\n- `export_weights_and_biases`: An additional option allowing users to export the raw weights and biases as H2O frames. \\n\\nThe following options for the `loss` parameter have been added:\\n\\n- `absolute`: Provides strong penalties for mispredictions \\n- `huber`: Can improve results for regression \\n\\n### DL Algorithm Comparison\\n\\nH2O Classic  | H2O 3.0 \\n------------- | -------------\\n`h2o.deeplearning <- function(x,` | `h2o.deeplearning (x, `\\n`y,` | `y,`\\n`data,` | `training_frame,` \\n`key = \"\",` | `model_id = \"\",`\\n`override_with_best_model,` | `overwrite_with_best_model = true,` \\n`classification = TRUE,` | \\n`nfolds = 0,` |  `nfolds = 0`\\n`validation,` | `validation_frame,` \\n`holdout_fraction = 0,` |  \\n`checkpoint = \" \"` | `checkpoint,` \\n`autoencoder,` | `autoencoder = false,` \\n`use_all_factor_levels,` | `use_all_factor_levels = true`\\n`activation,` | `_activation = c(\"Rectifier\", \"Tanh\", \"TanhWithDropout\", \"RectifierWithDropout\", \"Maxout\", \"MaxoutWithDropout\"),`\\n`hidden,` | `hidden= c(200, 200),`\\n`epochs,` | `epochs = 10.0,`\\n`train_samples_per_iteration,` |`train_samples_per_iteration = -2,`\\n&nbsp; | `target_ratio_comm_to_comp = 0.05`\\n`seed,` | `_seed,` \\n`adaptive_rate,` | `adaptive_rate = true,` \\n`rho,` | `rho = 0.99,` \\n`epsilon,` | `epsilon = 1e-08,` \\n`rate,` | `rate = .005,` \\n`rate_annealing,` | `rate_annealing = 1e-06,` \\n`rate_decay,` | `rate_decay = 1.0,` \\n`momentum_start,` | `momentum_start = 0,`\\n`momentum_ramp,` | `momentum_ramp = 1e+06,`\\n`momentum_stable,` | `momentum_stable = 0,` \\n`nesterov_accelerated_gradient,` | `nesterov_accelerated_gradient = true,`\\n`input_dropout_ratio,` | `input_dropout_ratio = 0.0,` \\n`hidden_dropout_ratios,` | `hidden_dropout_ratios,` \\n`l1,` | `l1 = 0.0,` \\n`l2,` | `l2 = 0.0,` \\n`max_w2,` | `max_w2 = Inf,`\\n`initial_weight_distribution,` | `initial_weight_distribution = c(\"UniformAdaptive\",\"Uniform\", \"Normal\"),`\\n`initial_weight_scale,` | `initial_weight_scale = 1.0,`\\n`loss,` | `loss = \"Automatic\", \"CrossEntropy\", \"Quadratic\", \"Absolute\", \"Huber\"),`\\n&nbsp; | `distribution = c(\"AUTO\", \"gaussian\", \"bernoulli\", \"multinomial\", \"poisson\", \"gamma\", \"tweedie\", \"laplace\", \"huber\"),`\\n&nbsp; | `tweedie_power = 1.5,`\\n`score_interval,` | `score_interval = 5,` \\n`score_training_samples,` | `score_training_samples = 10000l,` \\n`score_validation_samples,` | `score_validation_samples = 0l,`\\n`score_duty_cycle,` | `score_duty_cycle = 0.1,` \\n`classification_stop,` | `classification_stop = 0`\\n`regression_stop,` | `regression_stop = 1e-6,`\\n&nbsp; | `stopping_rounds = 5,`\\n&nbsp; | `stopping_metric = c(\"AUTO\", \"deviance\", \"logloss\", \"MSE\", \"AUC\", \"r2\", \"misclassification\", \"mean_per_class_error\"),`\\n&nbsp; | `stopping_tolerance = 0,`\\n`quiet_mode,` | `quiet_mode = false,`\\n`max_confusion_matrix_size,` | `max_confusion_matrix_size,`\\n`max_hit_ratio_k,` | \\n`balance_classes,` | `balance_classes = false,`\\n`class_sampling_factors,` | `class_sampling_factors,`\\n`max_after_balance_size,` | `max_after_balance_size,` \\n`score_validation_sampling,` | `score_validation_sampling,`\\n`diagnostics,` | `diagnostics = true,` \\n`variable_importances,` | `variable_importances = false,`\\n`fast_mode,` | `fast_mode = true,` \\n`ignore_const_cols,` | `ignore_const_cols = true,`\\n`force_load_balance,` | `force_load_balance = true,`\\n`replicate_training_data,` | `replicate_training_data = true,`\\n`single_node_mode,` | `single_node_mode = false,` \\n`shuffle_training_data,` | `shuffle_training_data = false,`\\n`sparse,` | `sparse = false,` \\n`col_major,` | `col_major = false,`\\n`max_categorical_features,` | `max_categorical_features,`\\n`reproducible)` | `reproducible=FALSE,` \\n`average_activation` | `average_activation = 0,`\\n &nbsp; | `sparsity_beta = 0`\\n &nbsp; | `export_weights_and_biases=FALSE,`\\n &nbsp; | `offset_column = NULL,` \\n &nbsp; | `weights_column = NULL,`\\n &nbsp; | `nfolds = 0,`\\n &nbsp; | `fold_column = NULL,`\\n &nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),`\\n &nbsp; | `keep_cross_validation_predictions = FALSE)`\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | ------------- \\n`@model$priorDistribution`| &nbsp;  | `all`\\n`@model$params` | `@allparameters` | `all`\\n`@model$train_class_error` | `@model$training_metrics@metrics@$MSE`  | `all`\\n`@model$valid_class_error` | `@model$validation_metrics@$MSE` | `all`|\\n`@model$varimp` | `@model$_variable_importances` | `all`\\n`@model$confusion` | `@model$training_metrics@metrics$cm$table`  | `binomial` and `multinomial`\\n`@model$train_auc` | `@model$train_AUC`  | `binomial`\\n&nbsp; | `@model$_validation_metrics` | `all`\\n&nbsp; | `@model$_model_summary` | `all`\\n&nbsp; | `@model$_scoring_history` | `all`\\n\\n \\n ---\\n\\n<a name=\"DRF\"></a>'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Distributed Random Forest\\n\\n### Changes to DRF in H2O 3.0 \\n\\nDistributed Random Forest (DRF) was represented as `h2o.randomForest(type=\"BigData\", ...)` in H2O Classic. In H2O Classic, SpeeDRF (`type=\"fast\"`) was not as accurate, especially for complex data with categoricals, and did not address regression problems. DRF (`type=\"BigData\"`) was at least as accurate as SpeeDRF (`type=\"fast\"`) and was the only algorithm that scaled to big data (data too large to fit on a single node). \\nIn H2O 3.0, our plan is to improve the performance of DRF so that the data fits on a single node (optimally, for all cases), which will make SpeeDRF obsolete. Ultimately, the goal is provide a single algorithm that provides the \"best of both worlds\" for all datasets and use cases. \\nPlease note that H2O does not currently support the ability to specify the number of trees when using `h2o.predict` for a DRF model. \\n\\n\\n**Note**: H2O 3.0 only supports DRF. SpeeDRF is no longer supported. The functionality of DRF in H2O 3.0 is similar to DRF functionality in H2O. \\n\\n### Renamed DRF Parameters\\n\\nThe following parameters have been renamed, but retain the same functions: \\n\\nH2O Classic Parameter Name | H2O 3.0 Parameter Name\\n-------------------|-----------------------\\n`data` | `training_frame`\\n`key` | `model_id`\\n`validation` | `validation_frame`\\n`sample.rate` | `sample_rate`\\n`ntree` | `ntrees` \\n`depth` | `max_depth`\\n`balance.classes` | `balance_classes`\\n`score.each.iteration` | `score_each_iteration`\\n`class.sampling.factors` | `class_sampling_factors`\\n`nodesize` | `min_rows`\\n\\n\\n### Deprecated DRF Parameters\\n\\nThe following parameters have been removed: \\n\\n- `classification`: This is now automatically inferred from the response type. To achieve classification with a 0/1 response column, explicitly convert the response to a factor (`as.factor()`). \\n- `importance`: Variable importances are now computed automatically and displayed in the model output. \\n- `holdout.fraction`: Specifying the fraction of the training data to hold out for validation is no longer supported. \\n- `doGrpSplit`: The bit-set group splitting of categorical variables is now the default. \\n- `verbose`: Infonrmation about tree splits and extra statistics is now included automatically in the stdout. \\n- `oobee`: The out-of-bag error estimate is now computed automatically (if no validation set is specified).\\n- `stat.type`: This parameter was used for SpeeDRF, which is no longer supported.\\n- `type`: This parameter was used for SpeeDRF, which is no longer supported. \\n\\n### New DRF Parameters\\n\\nThe following parameter has been added: \\n\\n- `build_tree_one_node`: Run on a single node to use fewer CPUs. \\n\\n### DRF Algorithm Comparison\\n\\nH2O Classic | H2O 3.0\\n------------- | -------------\\n`h2o.randomForest <- function(x,` | `h2o.randomForest <- function(`\\n`x,` | `x,` \\n`y,` | `y,` \\n`data,` | `training_frame,` \\n`key=\"\",` | `model_id,` \\n`validation,` | `validation_frame,` \\n`mtries = -1,` | `mtries = -1,` \\n`sample.rate=2/3,` | `sample_rate = 0.632,` \\n &nbsp; | `build_tree_one_node = FALSE,` \\n`ntree=50` | `ntrees=50,` \\n`depth=20,` | `max_depth = 20,` \\n &nbsp; | `min_rows = 1,`\\n`nbins=20,` | `nbins = 20,` \\n&nbsp; | `nbins_top_level,`\\n&nbsp; | `nbins_cats =1024,`\\n&nbsp; | `binomial_double_trees = FALSE,`\\n`balance.classes = FALSE,` | `balance_classes = FALSE,` \\n`seed = -1,` | `seed` \\n`nodesize = 1,` |  \\n`classification=TRUE,` | \\n`importance=FALSE,` | \\n&nbsp; | `weights_column = NULL,`\\n`nfolds=0,` | `nfolds = 0,`\\n&nbsp; | `fold_column = NULL,`\\n&nbsp; | `fold_assignment = c(\"AUTO\", \"Random\", \"Modulo\"),` \\n&nbsp; | `keep_cross_validation_predictions = FALSE,`\\n&nbsp; | `score_each_iteration = FALSE,`\\n&nbsp; | `stopping_rounds = 0,`\\n&nbsp; | `stopping_metric = c(\"AUTO\", \"deviance\", \"logloss\", \"MSE\", \"AUC\", \"r2\", \"misclassification\", \"mean_per_class_error\"), `\\n&nbsp; | `stopping_tolerance = 0.001)`\\n`holdout.fraction = 0,` | \\n`max.after.balance.size = 5,` | `max_after_balance_size,` \\n`class.sampling.factors = NULL,` | &nbsp; \\n`doGrpSplit = TRUE,` | \\n`verbose = FALSE,` |\\n`oobee = TRUE,` | \\n`stat.type = \"ENTROPY\",` |\\n`type = \"fast\")` | \\n\\n\\n### Output\\n\\n\\nThe following table provides the component name in H2O, the corresponding component name in H2O 3.0 (if supported), and the model type (binomial, multinomial, or all). Many components are now included in `h2o.performance`; for more information, refer to [(`h2o.performance`)](#h2operf).\\n\\nH2O Classic | H2O 3.0  | Model Type\\n------------- | ------------- | -------------\\n`@model$priorDistribution`| &nbsp;  | `all`\\n`@model$params` | `@allparameters` | `all`\\n`@model$mse` | `@model$scoring_history` | `all`\\n`@model$forest` | `@model$model_summary`  | `all`\\n`@model$classification` | &nbsp;  | `all`\\n`@model$varimp` | `@model$variable_importances` | `all`\\n`@model$confusion` | `@model$training_metrics@metrics$cm$table`  | `binomial` and `multinomial`\\n`@model$auc` | `@model$training_metrics@metrics$AUC`  | `binomial`\\n`@model$gini` | `@model$training_metrics@metrics$Gini`  | `binomial`\\n`@model$best_cutoff` | &nbsp;  | `binomial`\\n`@model$F1` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f1`  | `binomial`\\n`@model$F2` | `@model$training_metrics@metrics$thresholds_and_metric_scores$f2`  | `binomial`\\n`@model$accuracy` | `@model$training_metrics@metrics$thresholds_and_metric_scores$accuracy`  | `binomial`\\n`@model$Error` | `@model$Error`  | `binomial`\\n`@model$precision` | `@model$training_metrics@metrics$thresholds_and_metric_scores$precision`  | `binomial`\\n`@model$recall` | `@model$training_metrics@metrics$thresholds_and_metric_scores$recall`  | `binomial`\\n`@model$mcc` | `@model$training_metrics@metrics$thresholds_and_metric_scores$absolute_MCC`  | `binomial`\\n`@model$max_per_class_err` | currently replaced by `@model$training_metrics@metrics$thresholds_and_metric_scores$min_per_class_correct`  | `binomial`'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Migration.md', 'section': '## Github Users\\n\\nAll users who pull directly from the H2O classic repo on Github should be aware that this repo will be renamed. To retain access to the original H2O (2.8.6.2 and prior) repository: \\n\\n**The simple way**\\n\\nThis is the easiest way to change your local repo and is recommended for most users. \\n\\n1. Enter `git remote -v` to view a list of your repositories. \\n2. Copy the address your H2O classic repo (refer to the text in brackets below - your address will vary depending on your connection method):\\n\\n  ```\\n  H2O_User-MBP:h2o H2O_User$ git remote -v\\n  origin\\thttps://{H2O_User@github.com}/h2oai/h2o.git (fetch)\\n  origin\\thttps://{H2O_User@github.com}/h2oai/h2o.git (push)\\n  ```\\n3. Enter `git remote set-url origin {H2O_User@github.com}:h2oai/h2o-2.git`, where `{H2O_User@github.com}` represents the address copied in the previous step. \\n\\n**The more complicated way**\\n\\nThis method involves editing the Github config file and should only be attempted by users who are confident enough with their knowledge of Github to do so. \\n\\n1. Enter `vim .git/config`. \\n2. Look for the `[remote \"origin\"]` section:\\n\\n   ```\\n   [remote \"origin\"]\\n        url = https://H2O_User@github.com/h2oai/h2o.git\\n        fetch = +refs/heads/*:refs/remotes/origin/*\\n    ```\\n3. In the `url =` line, change `h2o.git` to `h2o-2.git`. \\n4. Save the changes.  \\n\\nThe latest version of H2O is stored in the `h2o-3` repository. All previous links to this repo will still work, but if you would like to manually update your Github configuration, follow the instructions above, replacing `h2o-2` with `h2o-3`.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/RChanges.md', 'section': '## H2O Connection Object\\n\\nThe H2O connection object (`conn`) has been removed from nearly all calls.\\nThe `conn` object is still used in the `h2o.clusterIsUp` command. \\n\\nAny `conn` references for commands other than `h2o.clusterIsUp` must be removed from scripts to ensure compatibility.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/RChanges.md', 'section': '## Changes to `apply`\\n\\nThe data shape returned by `apply` is now identical to the default behavior in R. Any column-wide changes produce column-wide results. \\n\\nFor example, in previous versions, if `apply` on `MARGIN` was equal to `2`, then 200 rows would be returned in one column. Now, 200 columns are produced in one row. \\n\\nTo revert to the previous behavior, use the transpose function using the R command `t`.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/RChanges.md', 'section': '## Temp Management\\n\\nFor users who regularly remove the temporary data frames and keys manually, the temp management rules have been improved in the following ways:\\n\\n- For a data frame created in R: \\n\\n  - If no name is specified, a temporary name is assigned, which is deleted when the cluster is stopped or after a R GC cycle\\n\\n  - If a name is specified, that name is used until it is manually deleted\\n\\n- Parsed input data and models are given names and not automatically deleted when the cluster is stopped; a temporary column holds the parsed data until it is deleted during the R GC cycle \\n\\n- If your cluster is running low on memory, run an R GC cycle to delete temporary data frames and keys'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/RChanges.md', 'section': '## S4 to S3\\n\\nThe internal H2O object, which was previously an S4 object, is now an S3 object. You must use S3 operations to access objects (instead of S4). The risk of overloading depends on whether the package overloads the existing package type.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/RChanges.md', 'section': '## `frame_id` to `id`\\n\\nThe `frame_id` property has been renamed to `id`. This property is used in the `h2o.getFrame` command.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Rdoc.md', 'section': '## Munging operations in R:\\n\\n### Overview:\\n\\nOperating on an `H2OFrame` object triggers the rollup of the expression to be executed, but the expression itself is not evaluated. Instead, an AST is built from the R expression using R\\'s built-in parser, which handles operator precedence. In the case of assignment, the AST is stashed into the variable in the assignment. The AST is bound to an R variable as a promise to evaluate the expression on demand. When evaluation is forced, the AST is walked, converted to JSON, and shipped over to H2O. The result returned by H2O is a key pointing to the newly-created frame. Depending on the methods used, the results may not be an H2OFrame return type. Any extra preprocessing of data returned by H2O is discussed in each instance, as it varies from method to method.\\n\\n\\n### What\\'s implemented?\\nMany of R\\'s generic S3 methods can be combined with H2OFrame objects so that  the result is coerced to an object of the appropriate type (typically an H2OFrame object). To view a list of R\\'s generic methods, use `getGenerics()`. A call to `showMethods(classes=\"H2OFrame\")` displays a list of permissible operations with H2OFrame objects. S3 methods are divided into four groups: \\n\\n- Math\\n- Ops\\n- Complex\\n- Summary\\n\\nWith the exception of Complex, H2OFrame methods fall into these categories as well. Specifically, the group divisions follow the S4 divisions: \\n\\n- Ops\\n- Math\\n- Math2\\n- Summary\\n\\n\\n### List:\\n\\n#### Ops Group\\n\\nThis group includes:\\n\\n- **Arith**, for performing arithmetic on numeric or complex vectors\\n- **Compare**, for comparing values\\n- **Logic**, for logical operations\\n\\n| **Ops** |&nbsp; |&nbsp; |&nbsp; |\\n|-----|-----|----|-----|\\n|  **Arith**|&nbsp; |&nbsp; | &nbsp;|\\n|`+`  |`-` | `*`|`/`|\\n|`^`  | `%%`| `%/%` | &nbsp;|\\n|  **Compare**| &nbsp;|&nbsp; |&nbsp; |\\n| `==`| `!=` | `<`|&nbsp;|\\n|`<=` | `>=`| `>`|&nbsp; |\\n|  **Logic**|&nbsp; | &nbsp;| &nbsp;|\\n|`&`| ``|&nbsp;|&nbsp;|\\n\\n\\n#### Math Group\\n\\nThis group includes:\\n\\n- **Trigonometric**, for trigonometric functions\\n- **Hyperbolic**, for hyperbolic functions\\n- **Miscellaneous**, which contains the absolute value and square root functions\\n- **Sign**, which returns a vector with the signs of the corresponding elements of x (does not work for complex vectors)\\n- **Rounding**, which allows rounding of numbers\\n- **Logarithms/Exponentials**, which compute logarithmic and exponential functions\\n- **Special**, which contains gamma functions\\n- **Cumulative**, which returns the cumulative sums, products, minima, or maxima\\n\\n\\n| **Math** |&nbsp; |&nbsp; | \\n|-----|-----|-----|\\n|**Miscellaneous** |&nbsp;|\\n| `abs` | `sqrt`|&nbsp;|\\n|**Rounding**|&nbsp;|\\n| `floor`|`ceiling`| `trunc`|\\n|**Log/Exp**|\\n|`exp`|`expm1`|`log1p`|\\n| **Trigonometric** |\\n|`cos`|`sin`|`tan`|\\n|`acos`|`asin`|`atan`|\\n|**Hyperbolic**|\\n|`cosh`|`sinh`|`tanh`|\\n|`acosh`|`asinh`|`atanh`|\\n|**Sign**|\\n|`sign`|`round`|`signif`|\\n|**Special**|\\n|`lgamma`|`gamma`|`digamma`|`trigamma`|\\n|**Cumulative**|&nbsp;|\\n|`cumsum`|`cumprod`|`cummax`|`cummin`|\\n\\n\\n\\n#### Summary Group\\n\\nThis group includes:\\n\\n- **Maxima/Minima**, which returns the maxima and minima \\n- **Range**, which returns the minimum and maximum\\n- **Product**, which returns the product\\n- **Sum**, which returns the sum\\n- **All**, which tells the user if all values are true\\n- **Any**, which tells the user if any values are true\\n\\n| **Summary**| |\\n|-----|-----|\\n|`max`|`min`|\\n|`range`|`prod`|\\n|`sum`|`all`|\\n|`any`|\\n\\n#### Non-Group Generic\\n\\nThis group includes:\\n\\n- **Logic**, for logical operations\\n- **Matrix Multiplication**, for multiplying two matrices \\n- **Extract/Replace**, for extracting or replacing part of an object\\n- **Value Matching**, for returning matching vectors\\n- **Apply**, for returning the values resulting from a function\\n\\n\\n| **Non-Group Generic** | &nbsp;|&nbsp; |&nbsp; |\\n|-----|-----|-----|-----|\\n|`!`|\\n|**Extract/Replace**|||\\n|`[`|`[[`|`[[<-`|`[<-`\\n| `$<-`|\\n|**Matrix Multiplication**|&nbsp;|\\n|`%/%`| `%*%`|\\n|**Value Matching**|&nbsp;|\\n|`%in%`|\\n|**Apply**|&nbsp;|\\n|`apply`|\\n|`as.character`|`as.data.frame`|\\n|`as.environment`|`as.factor`|`as.h2o`|`as.matrix`|\\n|`as.numeric`|`colnames`|`colnames<-`|`cut`|\\n|`dim`|`head`|`h2o.anyfactor`|`h2o.cbind`|\\n|`h2o.ddply`|`h2o.levels`|`h2o.rbind`|`h2o.runif`|\\n|`h2o.setLevel`|`h2o.table`|`ifelse`|`is.factor`|\\n|`is.na`|`length`|`log`|`match`|\\n|`mean`|`median`|`names`|`names<-`|\\n|`ncol`|`nrow`|`pop`|`push`|\\n|`quantile`|`reset`|`sapply`|`scale`|\\n|`sd`|`show`|`subset`|`summary`|\\n|`t`|`tail`|`transform`|`trunc`|\\n|`var`|`within`|\\n\\n\\n\\n\\n\\n\\n\\n# Data Prep in R\\nstandard data prep\\n\\n\\n# Data Manipulation in R\\n\\nhow to move data back and forth between data in R \\nslicing\\ncreating new columns\\n\\n# Examples/Demos\\n\\n# Support \\n\\nUsers of the H2O package may submit general inquiries and bug reports using the \"h2o\" tag on [Stack Overflow](https://stackoverflow.com/questions/tagged/h2o). Alternatively, specific bugs or issues may be filed to the GitHub issue, [https://github.com/h2oai/h2o-3/issues](https://github.com/h2oai/h2o-3/issues).\\n\\n# References\\n\\n# Appendix\\n(commands)'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Upgrade.md', 'section': '## Why Upgrade?\\n\\nH2O 3.0 represents our latest iteration of H2O. It includes many improvements, such as a simplified architecture, faster and more accurate algorithms, and an interactive web UI. \\n\\nAs of May 15th, 2015, this version will supersede the previous version of H2O. Support for previous versions of H2O will be provided for a limited time, but there will no longer be significant updates to the previous version of H2O. \\n\\nFor a comparison of H2O and H2O 3.0, please refer to <a href=\"https://github.com/h2oai/h2o-dev/blob/jessica-dev-docs/h2o-docs/src/product/upgrade/H2OvsH2O-Dev.md\" target=\"_blank\">this document</a>. \\n\\n### Python Support\\n\\nPython is only supported on the latest version of H2O. For more information, refer to the <a href=\"https://github.com/h2oai/h2o-dev/blob/master/h2o-py/README.rst\" target=\"_blank\">Python installation instructions</a>.\\n\\n### Sparkling Water Support\\n\\nSparkling Water is only supported with H2O 3.0. For more information, refer to the <a href=\"https://github.com/h2oai/sparkling-water/blob/master/README.md\" target=\"_blank\">Sparkling Water repo</a>.'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Upgrade.md', 'section': '## Supported Algorithms\\n\\nH2O 3.0 will soon provide feature parity with previous versions of H2O. Currently, the following algorithms are supported: \\n\\n### Supervised \\n\\n- **Generalized Linear Model (GLM)**: Binomial classification, multinomial classification, regression (including logistic regression)\\n- **Distributed Random Forest (DRF)**: Binomial classification, multinomial classification, regression\\n- **Gradient Boosting Machine (GBM)**: Binomial classification, multinomial classification, regression\\n- **Deep Learning (DL)**: Binomial classification, multinomial classification, regression\\n- Naive Bayes\\n- Stacked Ensembles\\n- XGBoost\\n\\n### Unsupervised\\n\\n- K-means\\n- Principal Component Analysis\\n- Autoencoder\\n- Generalized Low Rank Models\\n\\n### Miscellaneous\\n\\n- **Word2vec**'}\n",
      "{'filename': 'h2o-3-master/h2o-docs/src/product/upgrade/Upgrade.md', 'section': '## How to Update R Scripts\\n\\nDue to the numerous enhancements to the H2O package for R to make it more consistent and simplified, some parameters have been renamed or deprecated. \\n\\nTo assist R users in updating their existing scripts for compatibility with H2O 3.0, a \"shim\" has been developed. When you run the shim on your script, any deprecated or renamed parameters are identified and a suggested replacement is provided. You can access the shim <a href=\"https://github.com/h2oai/h2o-dev/blob/9795c401b7be339be56b1b366ffe816133cccb9d/h2o-r/h2o-package/R/shim.R\" target=\"_blank\">here</a>.\\n\\n  >**Note**: As of Slater v.3.2.0.10, this shim will no longer be available. \\n\\nAdditionally, there is a <a href=\"https://github.com/h2oai/h2o-dev/blob/master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md\" target=\"_blank\">document</a> available that provides a side-by-side comparison of the differences between versions.'}\n",
      "{'filename': 'h2o-3-master/h2o-k8s/README.md', 'section': '## Running H2O in K8s - user\\'s guide\\n\\nH2O Pods deployed on Kubernetes cluster require a \\n[headless service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) \\nfor H2O Node discovery. The headless service, instead of load-balancing incoming requests to the underlying\\nH2O pods, returns a set of adresses of all the underlying pods. It is therefore the responsibility of the K8S\\ncluster administrator to set-up the service correctly to cover H2O nodes only.\\n\\n### Creating the headless service\\nFirst, a headless service must be created on Kubernetes.  \\n\\n```yaml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: h2o-service\\n  namespace: <namespace-name>\\nspec:\\n  type: ClusterIP\\n  clusterIP: None\\n  selector:\\n    app: h2o-k8s\\n  ports:\\n  - protocol: TCP\\n    port: 54321\\n```\\n\\nThe `clusterIP: None` defines the service as headless. The `port: 54321` is the default H2O port. Users and client libraries\\nuse this port to talk to the H2O cluster.\\n\\nThe `app: h2o-k8s` setting is of **great importance**, as this is the name of the application with H2O pods inside. \\nPlease make sure this setting corresponds to the name of H2O deployment name chosen.\\n\\n### Creating the H2O Stateful Set\\n\\nIt is **strongly recommended** to run H2O as a [Stateful set](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)\\non a Kubernetes cluster. Kubernetes assumes all the pods inside the cluster are stateful and does not attempt to restart\\nthe individual pods on failure. Once a job is triggered on an H2O cluster, the cluster is locked and no additional nodes\\ncan be added. Therefore, the cluster has to be restarted as a whole if required - which is a perfect fit for a StatefulSet.\\n\\nIn order to ensure reproducibility, all requests should be directed towards H2O Leader node. Leader node election is done\\nafter the set of nodes discovered is complete. Therefore, after the clustering is complete and the leader node is known,\\nonly the pod with H2O leader node should be made available. This also makes the service(s) on top of the deployment route\\nall requests only to the leader node. To achieve that, readiness probe residing on `/kubernetes/isLeaderNode` address is used.\\nOnce the clustering is done, all nodes but the leader node mark themselves as not ready, leaving only the leader node exposed.\\n\\n```yaml\\napiVersion: apps/v1\\nkind: StatefulSet\\nmetadata:\\n  name: h2o-stateful-set\\n  namespace: <namespace-name>\\nspec:\\n  serviceName: h2o-service\\n  podManagementPolicy: \"Parallel\"\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: h2o-k8s\\n  template:\\n    metadata:\\n      labels:\\n        app: h2o-k8s\\n    spec:\\n      containers:\\n        - name: h2o-k8s\\n          image: \\'h2oai/h2o-open-source-k8s:<tagname>\\'\\n          resources:\\n            requests:\\n              memory: \"4Gi\"\\n          ports:\\n            - containerPort: 54321\\n              protocol: TCP\\n          readinessProbe:\\n            httpGet:\\n              path: /kubernetes/isLeaderNode\\n              port: 8081\\n            initialDelaySeconds: 5\\n            periodSeconds: 5\\n            failureThreshold: 1\\n          env:\\n          - name: H2O_KUBERNETES_SERVICE_DNS\\n            value: h2o-service.<namespace-name>.svc.cluster.local\\n          - name: H2O_NODE_LOOKUP_TIMEOUT\\n            value: \\'180\\'\\n          - name: H2O_NODE_EXPECTED_COUNT\\n            value: \\'3\\'\\n          - name: H2O_KUBERNETES_API_PORT\\n            value: \\'8081\\'\\n```\\nBesides standardized Kubernetes settings, like `replicas: 3` defining the number of pods with H2O instantiated, there are\\nseveral settings to pay attention to.\\n\\nThe name of the application `app: h2o-k8s` must correspond to the name expected by the above-defined headless service in order\\nfor the H2O node discovery to work. H2O communicates on port 54321, therefore `containerPort: 54321`must be exposed to\\nmake it possible for the clients to connect.\\n\\nThe documentation of the official H2O Docker images is available at the official [H2O Docker Hub page](https://hub.docker.com/r/h2oai/h2o-open-source-k8s). Use the `nightly` tag to\\nget the bleeding-edge Docker image with H2O inside. \\n\\nEnvironment variables:\\n\\n1. `H2O_KUBERNETES_SERVICE_DNS` - **[MANDATORY]** Crucial for the clustering to work. The format usually follows the\\n `<service-name>.<project-name>.svc.cluster.local` pattern. This setting enables H2O node discovery via DNS.\\n  It must be modified to match the name of the headless service created. Also, pay attention to the rest of the address\\n  to match the specifics of your Kubernetes implementation.\\n1. `H2O_NODE_LOOKUP_TIMEOUT` - **[OPTIONAL]** Node lookup constraint. Time before the node lookup is ended. \\n1. `H2O_NODE_EXPECTED_COUNT` - **[OPTIONAL]** Node lookup constraint. Expected number of H2O pods to be discovered.\\n1. `H2O_KUBERNETES_API_PORT` - **[OPTIONAL]** Port for Kubernetes API checks to listen on. Defaults to 8080.\\n\\nIf none of the optional lookup constraints is specified, a sensible default node lookup timeout will be set - currently\\ndefaults to 3 minutes. If any of the lookup constraints are defined, the H2O node lookup is terminated on whichever \\ncondition is met first.\\n\\n### Exposing H2O cluster\\n\\nExposing the H2O cluster is a responsibility of the Kubernetes administrator. By default, an\\n [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) can be created. Different platforms offer\\n different capabilities, e.g. OpenShift offers [Routes](https://docs.openshift.com/container-platform/4.3/networking/routes/route-configuration.html).'}\n",
      "{'filename': 'h2o-3-master/h2o-k8s/tests/clustering/README.md', 'section': \"## DNS Test Scenario\\n\\nTests clustering by means of DNS records of a [Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/).\\nCurrently, this is the only scenario tests, as there are no other scenarios supported yet.\\n\\n![Test scenario](readme/h2o-k8s-clustering.png)\\n\\nExactly `n` H2O pods (for simplicity and speed `n = 2`) are deployed to a Kubernetes cluster using a Deployment.\\nA `StatefulSet` is a direct match for stateful applications like H2O, however, at the time this test suite was created,\\nthe `kubectl wait` command did not support `StatefulSet`. Therefore, `Deployment` has been used.\\n\\n### Test stage definition\\n\\nA new **nightly** stage named `Kubernetes` has been created in `{h2o-home}/scripts/groovy/defineTestsStages.groovy`.\\nEvery stages in H2O runes inside an H2O container. The `Kubernetes` stage has it's own container named `harbor.h2o.ai/opsh2oai/h2o-3-k8s`. Latest\\nrevision is always used. The image of that docker container is represented by the `Dockerfile` file in this very folder.\\nChanges can be done by building the container with `docker build . -t harbor.h2o.ai/opsh2oai/h2o-3-k8s` and pushing to `harbor.h2o.ai`.\\nThe build is defined in `{h2o-home}/docker/Jenkinsfile-build-k8s-test-docker`.\\n\\nIn that image, a `Docker` is installed together with [k3d](https://github.com/rancher/k3d) by Rancher.\\nK3D serves as a convenience tool to install [k3s](https://k3s.io/), a lightweight Kubernetes implementation.\\nAfter the cluster is started, H2O Deployment is applied, together with a headless service and an Ingress. The deployment of\\n`n` together with the headless service tests whether H2O is capable to form a cluster. The Ingress is set-up to make\\nthe H2O cluster size testable from outside of the K8S cluster, using `h2o-cluster-check.sh`. Before the cluster-size check is\\nstarted, `kubectl wait` is used to wait for the pods to be deployed. The pod with H2O consists of a single Docker container,\\nwith JDK and `h2o.jar` mounted from the build that is running. This docker container build is defined in\\n`{h2o-home}/docker/Jenkinsfile-build-k8s-test-h2o-docker`.\\n\\nAn automated build has been set-up in Jenkins to build both images: `Jenkins` -> `H2O-3` -> `docker-images` -> `h2o-3-k8s-test-docker-build`.\\n\\nThe deployment speed of H2O pods depends heavily on connection to `harbor.h2o.ai`, as there is a secondary Docker image to run H2O pods,\\nand this image is downloaded from `harbor.h2o.ai` every single time. As the whole Kubernetes docker runs inside a Docker and\\nis intended to be used only once, there is no cache. Usually, this stage is a matter of seconds. \\n\\nAs soon as H2O pods are deployed, the `h2o-cluster-check.sh` is started. This queries H2O for cluster info by `curl http://localhost:8080/3/Cloud`.\\nThe cloud size in the JSON returned must be equal to the expected value. If it is equal, then the test is considered to be a pass\\nand an exit value of `0` is returned, indicating a passed test to Jenkins. Otherwise, a value of `1` is returned, signaling a \\nfailed test to Jenkins. In both cases, before the script exists, a cleanup of the Kubernetes cluster is done using `k3d delete`\\nbefore the outer Docker is killed. This is an important step, as in case host Docker is used, the container with K3S Kubernetes\\ncluster could have lived on.\"}\n",
      "{'filename': 'h2o-3-master/h2o-k8s/tests/clustering/README.md', 'section': \"## Assisted clustering test scenario\\n\\nAssisted clustering doesn't require the presence of a headless service in order to perform clustering. This implies\\nthere are less resources allocated and process of clustering (often most importantly the speed o it) is dependent on\\nexternal assistance. In Kubernetes environment, such assistant is most often an [H2O Operator](https://github.com/h2oai/h2o-kubernetes).\\nAssisted clustering itself is not restricte to Kubernetes - it is a generic mechanism to be leveraged in any envirnment\\nsuitable. Details are to be found in the [h2o-clustering](../../../h2o-clustering/README.md) module.\\n\\nAs H2O Operator is tested in its own repository, the test suite uses a separate script `assisted-clustering.py` deployed\\ninside the Kubernetes cluster in a POD. Once the script is ran, it finds H2O pods by given deployment name within given\\nnamespace. It then collects all ClusterIP addresses (internal Kubernetes IP address) of all H2O pods from given deployment\\nand sends these in a form of H2O flatfile to each node. Then verifies each H2O node inside each pod reports the same status.\"}\n",
      "{'filename': 'h2o-3-master/h2o-py-cloud-extensions/README.md', 'section': '## Install\\n\\nInstall prerequisites to your Python environment (Jupyter kernel):\\n```\\npip install h2o\\npip install https://s3.amazonaws.com/artifacts.h2o.ai/releases/ai/h2o/mlops/rel-0.58.0/5/h2o_mlops_client-0.58.0%2B42d986c.rel0.58.0.5-py2.py3-none-any.whl\\n```\\n\\nInstall the library:\\n```\\n# TODO UPDATE url:\\npip install https://h2o-release.s3.amazonaws.com/h2o/cloud-extensions/h2o_cloud_extensions-3.39.0.99999-py2.py3-none-any.whl\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-py-cloud-extensions/README.md', 'section': '## Configure\\n\\nOpen a Python environment and import h2o with cloud extensions and create or connect to h2o cluster\\n```\\nimport h2o\\nimport h2o_cloud_extensions as hce\\nh2o.init()\\n```\\n\\nSet a H2O Cloud instance you will be connecting to:\\n```\\nhce.settings.connection.client_id = \\'hac-platform-public\\'\\nhce.settings.connection.token_endpoint_url = \\'https://auth.internal.dedicated.h2o.ai/auth/realms/hac/protocol/openid-connect/token\\'\\n```\\n\\nGet authenticated against H2O.ai cloud (https://internal.dedicated.h2o.ai/auth/get-platform-token) and set platform token:\\n```\\nhce.settings.connection.refresh_token = \"TOKEN_THAT_YOU_RECIEVED_AFTER_AUTHENTICATION\"\\n```\\n\\n### MLOps\\nSet MLOps instance and the project that you will utilize for publishing your models.\\n```\\nhce.settings.mlops.api_url = \\'https://mlops-api.internal.dedicated.h2o.ai\\'\\nhce.settings.mlops.project_name = \\'My-project-for-h2o-3-models\\'\\n```\\n\\nIf the project with a given name does not exist, it will be automatically created. In that case, it would be\\nconvenient to set also a project description.\\n```\\nhce.settings.mlops.project_description = \\'H2O-3 is so cool. This will be fun!\\'\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-py-cloud-extensions/README.md', 'section': '## MLOps Integration\\n\\nH2O-3 cloud extension library is able to publish and deploy MOJO models trained via a single H2O algorithm (estimator)\\nGrid Search and AutoML.\\n\\n### Model Trained via a Single Algorithm (Estimator)\\nTrain a model:\\n```\\nfrom h2o.estimators import H2OGradientBoostingEstimator\\n\\nprostate = h2o.import_file(\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv\")\\nprostate[\"CAPSULE\"] = prostate[\"CAPSULE\"].asfactor()\\npredictors = [\"ID\",\"AGE\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\",\"GLEASON\"]\\nresponse = \"CAPSULE\"\\n\\npros_gbm = H2OGradientBoostingEstimator(nfolds=5,\\n                                        seed=1111,\\n                                        keep_cross_validation_predictions = True)\\n                                        \\npros_gbm.train(x=predictors, y=response, training_frame=prostate)\\n```\\n\\n#### Model Publishing\\nTo manually publish a trained model, call:\\n```\\npros_gbm.publish()\\n```\\n\\nThe above method call could be performed automatically straight after the model is trained if the below is set:\\n```\\nhce.settings.mlops.estimator.automatic_publishing = True\\n```\\n\\nTo check whether the model was published or not, call:\\n```\\npros_gbm.is_published()\\n```\\n\\n#### Model Deployment\\nTo manually deploy a trained model, call:\\n```\\npros_gbm.deploy(environment = \"DEV\")\\n```\\n\\nThe environment does not have to be specified if the below is set:\\n```\\nhce.settings.mlops.deployment_environment = \"DEV\"\\n```\\n\\nA model could be automatically deployed to an environment specified with the above settings straight\\nafter publishing the model. To achieve that, set the below:\\n```\\nhce.settings.mlops.estimator.automatic_deployment = True\\n```\\n\\nTo check whether the model was deployed to a given environment, call:\\n```\\npros_gbm.is_deployed(environment = \"DEV\")\\n```\\n\\n### Model Trained via Grid Search\\nTrain models:\\n```\\nfrom h2o.estimators import H2OGradientBoostingEstimator\\nfrom h2o.grid.grid_search import H2OGridSearch\\n\\nprostate = h2o.import_file(\"http://s3.amazonaws.com/h2o-public-test-data/smalldata/prostate/prostate.csv\")\\nprostate[\"CAPSULE\"] = prostate[\"CAPSULE\"].asfactor()\\npredictors = [\"ID\",\"AGE\",\"RACE\",\"DPROS\",\"DCAPS\",\"PSA\",\"VOL\",\"GLEASON\"]\\nresponse = \"CAPSULE\"\\ngbm_params = {\\'learn_rate\\': [i * 0.01 for i in range(1, 11)],\\n                \\'max_depth\\': list(range(2, 11)),\\n                \\'sample_rate\\': [i * 0.1 for i in range(5, 11)],\\n                \\'col_sample_rate\\': [i * 0.1 for i in range(1, 11)]}\\n\\nsearch_criteria = {\\'strategy\\': \\'RandomDiscrete\\', \\'max_models\\': 5, \\'seed\\': 2}\\n\\ngbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator,\\n                          hyper_params=gbm_params,\\n                          search_criteria=search_criteria)\\n                          \\ngbm_grid.train(x=predictors, y=response, training_frame=prostate)\\n```\\n\\n#### Model Publishing\\nTo manually publish all trained models, call:\\n```\\ngbm_grid.publish()\\n```\\n\\nThe above method call could be performed automatically straight after all models are trained if the below is set:\\n```\\nhce.settings.mlops.grid_search.automatic_publishing = True\\n```\\n\\nTo check whether all models were published, call:\\n```\\ngbm_grid.is_published()\\n```\\n\\n#### Model Deployment\\nTo manually deploy all published models, call:\\n```\\ngbm_grid.deploy(environment = \"DEV\")\\n```\\n\\nThe environment does not have to be specified if you specify the below:\\n```\\nhce.settings.mlops.deployment_environment = \"DEV\"\\n```\\n\\nModels could be automatically deployed to an environment specified with the above settings straight\\nafter publishing all models. To achieve that, set the below:\\n```\\nhce.settings.mlops.grid_search.automatic_deployment = True\\n```\\n\\nTo check whether all models were deployed to a given environment, call:\\n```\\ngbm_grid.is_deployed(environment = \"DEV\")\\n```\\n\\n### Model Trained via AutoML\\nRun AutoML to train models:\\n```\\nfrom h2o.automl import H2OAutoML\\n\\ntrain = h2o.import_file(\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/higgs/higgs_train_10k.csv\")\\nx = train.columns\\ny = \"response\"\\nx.remove(y)\\ntrain[y] = train[y].asfactor()\\n\\naml = H2OAutoML(max_models=5, seed=1)\\n\\naml.train(x=x, y=y, training_frame=train)\\n```\\n\\n#### Model Publishing\\nTo manually publish all trained models, call:\\n```\\naml.publish(strategy=\"all\")\\n```\\n\\nThe parameter `stragegy` is set to `best` by default, which publishes only the best performing model from leaderboard.\\nThe other option is to set the parameter to `\"all\"`, which publishes all leaderboard models. The default parameter value\\ncould be changed by the below command:\\n```\\nhce.settings.mlops.automl.publishing_strategy = \"all\"\\n```\\n\\nThe call of `aml.publish()` could be performed automatically straight after model training is finished if the below is set:\\n```\\nhce.settings.mlops.automl.automatic_publishing = True\\n```\\n\\nTo check whether all models were published (Change to `stragegy=\"best\"` if you\\'re interested only in the best model),\\ncall:\\n```\\naml.is_published(strategy=\"all\")\\n```\\n\\n#### Model Deployment\\nTo manually deploy all published models, call:\\n```\\naml.deploy(environment = \"DEV\")\\n```\\n\\nThe environment does not have to be specified if you specify the below:\\n```\\nhce.settings.mlops.deployment_environment = \"DEV\"\\n```\\n\\nModels could be automatically deployed to an environment specified with the above settings straight\\nafter publishing all models. To achieve that, set the below:\\n```\\nhce.settings.mlops.grid_search.automatic_deployment = True\\n```\\n\\nTo check whether all models published models were deployed to a given environment, call:\\n```\\naml.is_deployed(environment = \"DEV\")\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-py/README.md', 'section': '## Prerequisites:\\n\\n- Python 3.6.x, 3.7.x, 3.8.x, 3.9.x or 3.10.x\\n  - Numpy 1.9.2 or greater\\n\\nThis module depends on **requests**, **tabulate**, and **scikit-learn** modules, all of which are available on pypi:\\n\\n    $ pip install requests\\n    $ pip install tabulate\\n    $ pip install scikit-learn'}\n",
      "{'filename': 'h2o-3-master/h2o-py/README.md', 'section': '## Downloading and Installing\\n\\nYou can always download the latest stable version of the **h2o** Python package from the following page: [http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html](http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html) \\n\\nReview the installation instructions on the download page. \\n\\nAlternatively, you can build the h2o Python package from source (see below).'}\n",
      "{'filename': 'h2o-3-master/h2o-py/README.md', 'section': '## Building it yourself\\n\\nThe Python package is built as part of the normal build process.\\n\\nClone the [h2o-3 repository](https://github.com/h2oai/h2o-3) on GitHub. \\n\\nIn the top-level h2o-3 directory, use `$ ./gradlew build`.\\n\\nTo build the Python component by itself, first type `$ cd h2o-py`, and then type `$ ../gradlew build`.'}\n",
      "{'filename': 'h2o-3-master/h2o-py/README.md', 'section': '## Documentation/References\\n\\n- [Python Module Documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html)\\n- [Python Booklet](<http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/PythonBooklet.pdf>)\\n- [Python FAQ](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/faq.html#python>)\\n- [YouTube video - Quick Start with Python](https://www.youtube.com/watch?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0&v=K8J3dPBEz1s>)'}\n",
      "{'filename': 'h2o-3-master/h2o-py/demos/README.md', 'section': '## Prerequisites:\\n\\n- Python 3.7\\n\\n---\\n\\nInstall iPython Notebook\\n-------------------------\\n\\n1. Download pip, a Python package manager (if it\\'s not already installed):\\n\\n    `$ sudo easy_install pip`\\n\\n2. Install iPython using pip install:\\n\\n    `$ sudo pip install \"ipython[notebook]\"`\\n\\n---\\n\\nInstall dependencies\\n--------------------\\n\\nThis module uses requests and tabulate modules, both of which are available on pypi, the Python package index.\\n\\n    $ sudo pip install requests\\n    $ sudo pip install tabulate\\n  \\n---\\n\\nInstall and Launch H2O\\n----------------------\\n\\nTo use H2O in Python, follow the instructions on the **Install in Python** tab after selecting the H2O version on the [H2O Downloads page](http://h2o.ai/download). \\n\\nLaunch H2O outside of the iPython notebook. You can do this in the top directory of your H2O build download. The version of H2O running must match the version of the H2O Python module for Python to connect to H2O. \\nTo access the H2O Web UI, go to [https://localhost:54321](https://localhost:54321) in your web browser.\\n\\n---\\n\\nOpen Demos Notebook\\n-------------------\\n\\nOpen the prostate_gbm.ipynb file. The notebook contains a demo that starts H2O, imports a prostate dataset into H2O, builds a GBM model, and predicts on the training set with the recently built model. Use Shift+Return to execute each cell and proceed to the next cell in the notebook .\\n\\n    $ ipython notebook prostate_gbm.ipynb\\n\\nAll demos are available here:\\n\\n * [iPython Demos](https://github.com/h2oai/h2o-3/tree/master/h2o-py/demos)\\n\\n---\\n\\n\\nRunning Python Examples\\n-----------------------\\n\\nTo set up your Python environment to run these examples, download and install H2O from Python using the instructions above. \\n\\n\\n### Available Demos\\n\\n- [Predict Airline Delays](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/airlines_demo_small.ipynb) - Uses historical airlines flight data to build multiple classification models to label any flight as either delayed or not delayed.\\n- [Chicago Crime Rate](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_chicago_crimes.ipynb) - Uses weather and city statistics to compare arrest rates with the total crimes for each category. \\n- [NYC Citibike Demand with Weather](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/citi_bike_large.ipynb) - Takes monthly bike ride data (~10 million rows) for the past two years to predict bike demand at each bike share station. Weather data is also incorporated to better predict bike usage.\\n- [NYC Citibike Demand with Weather - smaller dataset](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/citi_bike_small.ipynb) - Takes monthly bike ride data (~1 million rows) for the past two years to predict bike demand at each bike share station. Weather data is also incorporated to better predict bike usage.\\n- [Confusion Matrix & ROC](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/cm_roc.ipynb) - Creates a GBM and GLM model using the airlines dataset, including confusion matrices, ROCs, and scoring histories. \\n- [Imputation](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/imputation.ipynb) - Substitutes values for missing data (imputes) the airlines dataset. \\n- [Not Equal Factor](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/not_equal_factor.ipynb) - Try to slice the airlines dataset using != `factor_level`. \\n- [Airline Confusion Matrices](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/confusion_matrices_binomial.ipynb) - Uses the airlines dataset to generate confusion matrices for algorithm performance analysis.\\n- [Deep Learning for Prostate Cancer Analysis](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/deeplearning.ipynb) - Uses the prostate dataset to build a Deep Learning model. \\n- [Airlines Prep](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/prep_airlines.ipynb) - Condition the airline dataset by filtering out NAs if the departure delay in the input dataset is unknown. Anything longer than `minutesOfDelayWeTolerate` is treated as delayed. \\n- [GBM model using prostate dataset](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/prostate_gbm.ipynb) - Creates a GBM model using the prostate dataset.\\n- [Balance Classes](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/rf_balance_classes.ipynb) - Imports the airlines dataset, parses it, displays a summary, and runs GLM with a binomial link function. \\n- [Clustering with KMeans](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/kmeans_aic_bic_diagnostics.ipynb) - Demonstrates kmeans clusters and different diagnostics for selecting the number of clusters.  Link to data is provided in the notebook.\\n- [EEG Eye State](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_tutorial_eeg_eyestate.ipynb) - Uses EEG data collected from an Emotiv Neuroheadset and classifies eye state (open vs closed) with a GBM.  \\n- [Tree fetch demo](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/tree_demo.ipynb) - Trains a basic GBM model based on Airlines dataset & fetches the tree behind the model. Exploration of the tree fetched is explained.\\n\\n\\n\\n### Corresponding Datasets\\n\\n\\n#### Airlines Datasets \\n\\n- [AirlinesTest](https://github.com/h2oai/h2o-2/raw/master/smalldata/airlines/AirlinesTest.csv.zip) and [AirlinesTrain](https://github.com/h2oai/h2o-2/raw/master/smalldata/airlines/AirlinesTrain.csv.zip) - Used in [Confusion Matrix & ROC](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/cm_roc.ipynb), [Airline Confusion Matrices](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/confusion_matrices_binomial.ipynb), [Balance Classes](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/rf_balance_classes.ipynb) and [Balance Classes](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/rf_balance_classes.ipynb)\\n\\n- [Allyears2k_headers](https://github.com/h2oai/h2o-2/raw/master/smalldata/airlines/allyears2k_headers.zip) - Used in [Predict Airline Delays](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/airlines_demo_small.ipynb), [Imputation](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/imputation.ipynb), [Not Equal Factor](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/not_equal_factor.ipynb), and [Airlines Prep](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/prep_airlines.ipynb)\\n\\n#### Chicago Crime\\n\\n- [chicagoAllWeather](https://github.com/h2oai/sparkling-water/raw/master/examples/smalldata/chicagoAllWeather.csv), [chicagoCensus](https://github.com/h2oai/sparkling-water/raw/master/examples/smalldata/chicagoCensus.csv), and [chicagoCrimes10k](https://github.com/h2oai/sparkling-water/raw/master/examples/smalldata/chicagoCrimes10k.csv) - Used in [Chicago Crime Rate](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/H2O_chicago_crimes.ipynb)\\n\\n#### Citibike Data\\n - Used in [NYC Citibike Demand with Weather](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/citi_bike_large.ipynb) \\n  \\n  \\t* [2013-07 - 157MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-07.csv)\\n  \\t* [2013-08 - 186MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-08.csv)\\n  \\t* [2013-09 - 193MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-09.csv)\\n  \\t* [2013-10 - 193MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-10.csv)\\n  \\t* [2013-11 - 126MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-11.csv)\\n  \\t* [2013-12 - 83MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-12.csv)\\n  \\t* [2014-01 - 56MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-01.csv)\\n  \\t* [2014-02 - 42MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-02.csv)\\n  \\t* [2014-03 - 82MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-03.csv)\\n  \\t* [2014-04 - 125MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-04.csv)\\n  \\t* [2014-05 - 161MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-05.csv)\\n  \\t* [2014-06 - 175MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-06.csv)\\n  \\t* [2014-07 - 180MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-07.csv)\\n  \\t* [2014-08 - 180MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-08.csv)\\n  \\t\\n -  [2013-10 - 193MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-10.csv) - Used in [NYC Citibike Demand with Weather - smaller dataset](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/citi_bike_small.ipynb)\\n\\n-  **NYC Weather Data** - Used in [NYC Citibike Demand with Weather](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/citi_bike_large.ipynb) and [NYC Citibike Demand with Weather - smaller dataset](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/citi_bike_small.ipynb)\\n  \\n    * [NYC Hourly Weather - 2013](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/31081_New_York_City__Hourly_2013.csv)    \\n    * [NYC Hourly Weather - 2014](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/31081_New_York_City__Hourly_2014.csv)\\n\\n\\n#### Prostate Data\\n\\n- [Prostate Dataset](https://github.com/h2oai/sparkling-water/raw/master/examples/smalldata/prostate.csv) - Used in [Deep Learning for Prostate Cancer Analysis](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/deeplearning.ipynb) and [GBM model using prostate dataset](https://github.com/h2oai/h2o-3/blob/master/h2o-py/demos/prostate_gbm.ipynb)'}\n",
      "{'filename': 'h2o-3-master/h2o-r/README.md', 'section': '## Downloading\\n\\nYou can always download the latest stable version of the **h2o** R package from the following page: [http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html](http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html) \\n\\nAlternatively, you can build the h2o R package from source (see below), or install the package from [CRAN](https://cran.r-project.org/package=h2o).'}\n",
      "{'filename': 'h2o-3-master/h2o-r/README.md', 'section': '## Building it yourself\\n\\nThe R package is built as part of the normal build process. Please following [this instruction](https://github.com/h2oai/h2o-3#41-building-from-the-command-line-quick-start) to build H2O-3.\\n\\nIf you want to build the R component by itself, instead of executing `./gradlew build`, you can execute the following: `$ cd h2o-r; ../gradlew build`.\\n\\nThe output of the build is a CRAN-like layout in the R directory.\\n\\nThe output of the build is a CRAN-like layout in the R directory.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/README.md', 'section': '## Installing\\n\\n###  Installation from the command line after build\\n\\n1. Navigate to the top-level `h2o-3` directory: `cd ~/h2o-3`. \\n2. Install the H2O package for R: `R CMD INSTALL h2o-r/R/src/contrib/h2o_****.tar.gz`\\n\\n   **Note**: Do not copy and paste the command above. You must replace the asterisks (*) with the current H2O .tar version number. Look in the `h2o-3/h2o-r/R/src/contrib/` directory for the version number. \\n\\n###  Installation from within R\\n\\n1. Detach any currently loaded H2O package for R.  \\n\\n  ```\\n  if (\"package:h2o\" %in% search()) detach(\"package:h2o\", unload=TRUE)\\n  ```\\n\\n2. Remove any previously installed H2O package for R.  \\n\\n  ```\\n  if (\"h2o\" %in% rownames(installed.packages())) remove.packages(\"h2o\")\\n  ```\\n\\n3. Install H2O R package along with its dependencies.\\n  \\n  Install latest CRAN version:\\n\\n  ```\\n  install.packages(\"h2o\")\\n  ```\\n\\n  Install latest H2O repo version, 1-2 releases ahead:\\n\\n  ```\\n  repos <- c(\"https://h2o-release.s3.amazonaws.com/h2o/rel-turing/9/R\", getOption(\"repos\"))\\n  install.packages(\"h2o\", type=\"source\", repos=repos)\\n  ```\\n  \\n   **Note**: Do not copy and paste the command above. You may need to replace `rel-turchin/9` with the current H2O build number. Refer to the H2O download page at [h2o.ai/download](http://h2o.ai/download) for latest build number.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/README.md', 'section': '## Running\\n\\n###  Start H2O from the command line\\n\\nMake sure your current directory is the h2o-3 top directory.\\n`$ java -jar h2o-app/build/libs/h2o-app.jar`  \\n\\n```\\n10-08 12:33:32.410 172.16.2.32:54321     22468  main      INFO: ----- H2O started  -----\\n10-08 12:33:32.484 172.16.2.32:54321     22468  main      INFO: Build git branch: (unknown)\\n10-08 12:33:32.484 172.16.2.32:54321     22468  main      INFO: Build git hash: (unknown)\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Build git describe: (unknown)\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Build project version: (unknown)\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Built by: \\'(unknown)\\'\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Built on: \\'(unknown)\\'\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Java availableProcessors: 8\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Java heap totalMemory: 245.5 MB\\n10-08 12:33:32.485 172.16.2.32:54321     22468  main      INFO: Java heap maxMemory: 3.56 GB\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Java version: Java 1.7.0_51 (from Oracle Corporation)\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: OS   version: Mac OS X 10.9.4 (x86_64)\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Possible IP Address: en0 (en0), fe80:0:0:0:2acf:e9ff:fe1c:ccf%4\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Possible IP Address: en0 (en0), 172.16.2.32\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Possible IP Address: lo0 (lo0), fe80:0:0:0:0:0:0:1%1\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Possible IP Address: lo0 (lo0), 0:0:0:0:0:0:0:1\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Possible IP Address: lo0 (lo0), 127.0.0.1\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Internal communication uses port: 54322\\n10-08 12:33:32.486 172.16.2.32:54321     22468  main      INFO: Listening for HTTP and REST traffic on  http://172.16.2.32:54321/\\n10-08 12:33:32.487 172.16.2.32:54321     22468  main      INFO: H2O cloud name: \\'tomk\\' on /172.16.2.32:54321, discovery address /225.54.105.89:57654\\n10-08 12:33:32.487 172.16.2.32:54321     22468  main      INFO: If you have trouble connecting, try SSH tunneling from your local machine (e.g., via port 55555):\\n10-08 12:33:32.487 172.16.2.32:54321     22468  main      INFO:   1. Open a terminal and run \\'ssh -L 55555:localhost:54321 tomk@172.16.2.32\\'\\n10-08 12:33:32.487 172.16.2.32:54321     22468  main      INFO:   2. Point your browser to http://localhost:55555\\n10-08 12:33:32.583 172.16.2.32:54321     22468  main      INFO: Cloud of size 1 formed [/172.16.2.32:54321]\\n10-08 12:33:32.583 172.16.2.32:54321     22468  main      INFO: Log dir: \\'/tmp/h2o-tomk/h2ologs\\'\\n```\\n\\n\\n###  Connect to H2O from within R\\n\\nTo load the H2O package in R, use `library(h2o)`  \\n\\n```\\n\\n----------------------------------------------------------------------\\n\\nYour next step is to start H2O and get a connection object (named\\n\\'localH2O\\', for example):\\n    > localH2O = h2o.init()\\n\\nFor H2O package documentation, ask for help:\\n    > ??h2o\\n\\nAfter starting H2O, you can use the Web UI at http://localhost:54321\\nFor more information visit http://docs.h2o.ai\\n\\n----------------------------------------------------------------------\\n\\n```\\n\\n\\nTo launch H2O, use `localH2O = h2o.init(nthreads = - 1)`  \\n\\n**Note**: The `nthreads = -1` parameter launches H2O using all available CPUs and is only applicable if you launch H2O locally using R. If you start H2O locally outside of R or start H2O on Hadoop, the `nthreads = -1` parameter is not applicable. \\n\\n\\n```\\nH2O is not running yet, starting it now...\\n\\nNote:  In case of errors look at the following log files:\\n    /var/folders/yl/cq5nhky53hjcl9wrqxt39kz80000gn/T//RtmpKkZY3r/h2o_H2O_User_started_from_r.out\\n    /var/folders/yl/cq5nhky53hjcl9wrqxt39kz80000gn/T//RtmpKkZY3r/h2o_H2O_User_started_from_r.err\\n\\njava version \"1.8.0_25\"\\nJava(TM) SE Runtime Environment (build 1.8.0_25-b17)\\nJava HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)\\n\\n.Successfully connected to http://127.0.0.1:54321/ \\n\\nR is connected to H2O cluster:\\n    H2O cluster uptime:         1 seconds 405 milliseconds \\n    H2O cluster version:        3.1.0.3031 \\n    H2O cluster name:           H2O_started_from_R_H2O_User_nqf165 \\n    H2O cluster total nodes:    1 \\n    H2O cluster total memory:   3.56 GB \\n    H2O cluster total cores:    8 \\n    H2O cluster allowed cores:  2 \\n    H2O cluster healthy:        TRUE \\n\\nNote:  As started, H2O is limited to the CRAN default of 2 CPUs.\\n       Shut down and restart H2O as shown below to use all your CPUs.\\n           > h2o.shutdown(localH2O)\\n           > localH2O = h2o.init(nthreads = -1)\\n```\\n\\n# Documentation/References\\n\\n- [R Package Documentation](http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Rdoc.html)\\n- [Porting R Scripts Guide](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/upgrade/H2ODevPortingRScripts.md)\\n- [R FAQ](https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/howto/FAQ.md#r)\\n- [YouTube video - Quick Start with R](https://www.youtube.com/watch?list=PLNtMya54qvOHbBdA1x8FNRSpMBEHmhxr0&v=zzV1kTCnmR0)'}\n",
      "{'filename': 'h2o-3-master/h2o-r/demos/README.md', 'section': '## Setting up Environment\\n\\nTo set up your R environment to run these examples the user will need to download and install the `h2o` package in R. The instructions for doing so are on the [top level](../README.md).'}\n",
      "{'filename': 'h2o-3-master/h2o-r/demos/README.md', 'section': \"## R Examples\\n\\n### Available Demos\\n\\n  * [`Predict Airlines Delays`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.airlines.delay.large.R) - Uses historical airlines flight data to build multiple classification models to label any flight as either delayed or not delayed.\\n\\n  * [`Predict Airlines Delays with Weather`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.airlines.delay.weather.large.R) - Uses historical airlines flight and weather data and joins the two datasets into one large table, then builds multiple classification models to label any flight as either delayed or not delayed.\\n  \\n  * [`NYC Citibike Demand with Weather`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.citi.bike.small.R) - Uses monthly bike ride data for the past two years to predict bike demand at each bike share station. Weather data is also incorporated to better predict bike usage.\\n  \\n  * [`Predict Bad Lending Club Loans`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.lending.club.large.R) - Uses [Lending Club data](https://www.lendingclub.com/info/download-data.action) to classify bad loans or loans where the user has been charged off or defaulted.\\n\\n  \\n  \\n### Corresponding Datasets\\n\\n  *  **Airlines Data** for [`Predict Airlines Delays`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.airlines.delay.large.R) and [`Predict Airlines Delays with Weather`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.airlines.delay.weather.large.R) - Any of the following datasets will work for the demo; choose an appropriate dataset size based on speed and scale.\\n  \\n  \\t* [2 Thousand Rows - 4.3MB](https://s3.amazonaws.com/h2o-airlines-unpacked/allyears2k.csv)\\n  \\t* [5.8 Million Rows - 580MB](https://s3.amazonaws.com/h2o-airlines-unpacked/airlines_all.05p.csv)\\n  \\t* [152 Million Rows - 14.5GB](https://s3.amazonaws.com/h2o-airlines-unpacked/allyears.1987.2013.csv)\\n  \\t\\n  *  **Chicago Weather Data** for [`Predict Airlines Delays with Weather`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.airlines.delay.weather.large.R)\\n  \\n  \\t* [2005-2008 Weather Data Near Chicago Airport](https://s3.amazonaws.com/h2o-public-test-data/smalldata/chicago/Chicago_Ohare_International_Airport.csv)\\n  \\n  *  **Citibike Data** for [`NYC Citibike Demand with Weather`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.citi.bike.small.R) - Choose the amount of bike ride data you want for your analysis; the range is from a single month to all available months.\\n  \\n  \\t* [2013-07 - 157MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-07.csv)\\n  \\t* [2013-08 - 186MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-08.csv)\\n  \\t* [2013-09 - 193MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-09.csv)\\n  \\t* [2013-10 - 193MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-10.csv)\\n  \\t* [2013-11 - 126MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-11.csv)\\n  \\t* [2013-12 - 83MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2013-12.csv)\\n  \\t* [2014-01 - 56MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-01.csv)\\n  \\t* [2014-02 - 42MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-02.csv)\\n  \\t* [2014-03 - 82MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-03.csv)\\n  \\t* [2014-04 - 125MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-04.csv)\\n  \\t* [2014-05 - 161MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-05.csv)\\n  \\t* [2014-06 - 175MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-06.csv)\\n  \\t* [2014-07 - 180MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-07.csv)\\n  \\t* [2014-08 - 180MB](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/2014-08.csv)\\n  \\t\\n  \\n  *  **NYC Weather Data** for [`NYC Citibike Demand with Weather`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.citi.bike.small.R)\\n  \\n    * [NYC Hourly Weather - 2013](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/31081_New_York_City__Hourly_2013.csv)    \\n    * [NYC Hourly Weather - 2014](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/31081_New_York_City__Hourly_2014.csv)\\n    * [NYC Hourly Weather - 2015](https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/citibike-nyc/31081_New_York_City__Hourly_2015.csv)\\n    \\n  *  **Lending Club Data** for [`Predict Bad Lending Club Loans`](https://github.com/h2oai/h2o-3/blob/master/h2o-r/demos/rdemo.lending.club.large.R) - Access the data on the [Lending Club site](https://www.lendingclub.com/info/download-data.action) and to access the secure dataset used in this demo you'll need a Lending Club account.\\n\\n###  Running Examples from Command Line\\n\\n0. If necessary, edit the working directory for each script.\\n0. Download the appropriate dataset and edit the dataset path in the R script if it is not located in your working directory.\\n0. To run a R demo script, run `R -f` followed by the file. For example to run the airlines demo:\\n\\n```\\nR -f airlines_delay.R\\n```\\n\\n###  Running Examples from R\\n0. If necessary, edit the working directory for each script.\\n0. Download the appropriate dataset and edit the dataset path in the R script if it is not located in your working directory.\\n0. To run a R demo script, open up the R script and execute the notebook line by line using Control+Return/Enter.\"}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Tutorial\\n\\nThe [H2O Ensemble Tutorial](https://github.com/h2oai/h2o-tutorials/blob/master/tutorials/ensembles-stacking/README.md) from [H2O World 2015](http://h2oworld.h2o.ai/) and accompanying [slides](https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/ensembles-stacking/H2O_World_2015_Ensembles.pdf) are good places to learn about the algorithm and our implementation.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Installation\\n\\n### Prerequisites\\n\\nThe **h2oEnsemble** R package requires the **h2o** R package (and its dependencies) to run.  We always recommend the latest stable version, which you can find on the [H2O R Downloads page](http://www.h2o.ai/download/h2o/r).\\n\\n\\n### Install H2O Ensemble (Stable)\\nThe latest stable version, compatible with the latest stable version of h2o, be installed as follows:\\n\\n```r\\ninstall.packages(\"https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.2.1.tar.gz\", repos = NULL)\\n``` \\n\\nTo install **h2oEnsemble** for a previous stable release of H2O, you can replace the URL above with the URL in the table below that matches your H2O version.\\n\\n|H2O Stable Release| Recommended version| Other compatible versions |\\n|:---------|:----------|:----------|\\n|H2O 3.10.5.1 - [latest stable h2o](https://www.h2o.ai/download/) (>=Vajda)|[h2oEnsemble 0.2.1](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.2.1.tar.gz)| [h2oEnsemble 0.2.0](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.2.0.tar.gz), [h2oEnsemble 0.1.9](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.9.tar.gz)|\\n|H2O 3.10.1.1 - [3.10.4.8](http://h2o-release.s3.amazonaws.com/h2o/rel-ueno/8/index.html) (Turnbull - Ueno)|[h2oEnsemble 0.1.9](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.9.tar.gz)| |\\n|H2O 3.8.0.1 - [3.10.0.10](http://h2o-release.s3.amazonaws.com/h2o/rel-turing/10/index.html) (Tukey - Turing)|[h2oEnsemble 0.1.8](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.8.tar.gz)| [h2oEnsemble 0.1.7](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.7.tar.gz), [h2oEnsemble 0.1.6](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.6.tar.gz)|\\n|H2O 3.6.0.1 - [3.6.0.8](http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/8/index.html) (Tibshirani)|[h2oEnsemble 0.1.5](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.5.tar.gz)||\\n|H2O 3.2.0.1 - [3.2.0.9](http://h2o-release.s3.amazonaws.com/h2o/rel-slater/9/index.html) (Slater)|[h2oEnsemble 0.1.4](https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.4.tar.gz)||\\n\\n### Install Development Version\\nThe following are two ways that you can install the development version of the **h2oEnsemble** package. \\n\\n- Install directly from GitHub in R using `devtools::install_github()`:\\n\\n```r\\nlibrary(devtools)\\ninstall_github(\"h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package\")\\n```\\n\\n- Clone the main h2o-3 repository and install the package:\\n\\n```bash\\ngit clone https://github.com/h2oai/h2o-3.git\\nR CMD INSTALL h2o-3/h2o-r/ensemble/h2oEnsemble-package\\n```'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Create Ensembles\\n\\n- An example of how to train and test an ensemble is in the `h2o.ensemble()` function documentation in the **h2oEnsemble** package and also in the [H2O Ensemble Tutorial](http://learn.h2o.ai/content/tutorials/ensembles-stacking/index.html).\\n- The ensemble is defined by its set of base learning algorithms and the metalearning algorithm.  Algorithm wrapper functions are used to specify these algorithms.\\n- The ensemble fit is an object of class, \"h2o.ensemble\" (actually just a list containing model elements).\\n- Also available since version 0.1.8 is the `h2o.stack()` function, which can take existing H2O models as a list and fit the metalearning function.  This function also returns an object of class, \"h2o.ensemble\".'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Wrapper Functions\\n\\n- The ensemble works by using wrapper functions (located in the `wrappers.R` file in the package).  These wrapper functions are used to specify the base learner and metalearner algorithms for the ensemble.  Examples of how to create custom algorithm wrappers are available in the [H2O Ensemble Tutorial](http://learn.h2o.ai/content/tutorials/ensembles-stacking/index.html).\\n- This methodology of using wrapper functions is modeled after the [SuperLearner](http://cran.r-project.org/web/packages/SuperLearner/index.html) and [subsemble](http://cran.r-project.org/web/packages/subsemble/index.html) ensemble learning packages.  The use of wrapper functions makes the ensemble code cleaner by providing a unified interface.\\n- Often it is a good idea to include variants of one algorithm/function by specifying different tuning parameters for different base learners.  There is an examples of how to create new variants of the wrapper functions in the [create\\\\_h2o\\\\_wrappers.R](https://github.com/h2oai/h2o-3/blob/master/h2o-r/ensemble/create_h2o_wrappers.R) script, as well as in the `h2o.ensemble()` R function documentation.\\n- The wrapper functions must have unique names.\\n- The more diverse the base learner library, the better!'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Metalearning\\n\\n- Historically, methods such as GLM or [non-negative least squares (NNLS)](https://en.wikipedia.org/wiki/Non-negative_least_squares) have been used to find the optimal weighted combination of the base learners, however any supervised learning algorithm can be used as a metalearner.  To use a GLM with non-negative weights, you simply pass `non_negative = TRUE` to the generic, `h2o.glm.wrapper()` function as follows:\\n\\n```r\\nh2o.glm_nn <- function(..., non_negative = TRUE) {\\n  h2o.glm.wrapper(..., non_negative = non_negative)\\n}\\nmetalearner <- \"h2o.glm_nn\"\\n```\\n- We allow the user to specify any H2O-supported algorithm as a metalearner, however, since the features in the metalearning step are highly correlated, we recommend using a metalearner such as a regularized GLM or Deep Neural Net.  (e.g. `h2o.glm.wrapper()`, which is the default, or `h2o.deeplearning.wrapper()`).\\n- Since the metalearning step is relatively quick compared to the base learning tasks, we recommend using the `h2o.metalearn()` function to re-train the ensemble fit using different metalearning algorithms.\\n- At this time, we still support using SuperLearner-based functions for metalearners, although, for performance reasons, it is not recommended.  For example, you can use the `SL.glm()` (included in the [SuperLearner](http://cran.r-project.org/web/packages/SuperLearner/index.html) R package).  When using a SuperLearner-based function for a metalearner, an *N x L* matrix will be pulled into R memory from H2O (*N* is number of observations and *L* is the number of base learners).  This may cause the code to fail for training sets of greater than ~8M rows due to a memory allocation issue.  Support for SuperLearner-based metalearners will be deprecated in the future.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Saving Ensembles\\n\\nCheck out the `h2o.save_ensemble()` and `h2o.load_ensemble()` functions to save and load your ensemble models.  An \"ensemble model\" is the collection of the base learners, metalearners and an `.RData` file containing metadata about ensemble.\\n\\nIf you want to use ensembles in production, then you may want to use the H2O Stacked Ensemble implementation in the **h2o** package instead.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Known Issues\\n\\n- This package is incompatible with R 3.0.0-3.1.0 due to a [parser bug](https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=15753) in R.  Upgrade to R 3.1.1 or greater to resolve the issue.  It may work on earlier versions of R but has not been tested.\\n- When using a `h2o.deeplearning.wrapper()` model as a base learner, it is not possible to reproduce ensemble model results exactly (even when using the `seed` argument of `h2o.ensemble()`) if your H2O cluster uses multiple cores.  This is due to the fact that `h2o.deeplearning()` results are only reproducible when trained on a single core.  More info [here](https://github.com/h2oai/h2o-3/discussions/15514).\\n- The [SNOW](https://cran.r-project.org/web/packages/snow/) cluster functionality is not active at this time (see the `parallel` option of the `h2o.ensemble` function).  There is a conflict with using the R parallel functionality in conjunction with the H2O parallel functionality.  The `h2o.*` base learning algorithms will use all cores available, so even when the `h2o.ensemble()` function is executed with the default `parallel = \"seq\"` option, the H2O algorithms will be training in parallel.  The `parallel` argument was intended to parallelize the cross-validation and base learning steps, but this functionality either needs to be re-architected to work in concert with H2O parallelism or removed in a future release.\\n- Passing the `validation_frame` to `h2o.ensemble()` does not currently do anything.  Right now, you must use the `predict()` / `predict.h2o.ensemble()` function to generate predictions on a test set.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Bug Reports\\n\\n- Please report any bugs or issues you may be having (or just general questions) to [h2ostream](https://groups.google.com/forum/#!forum/h2ostream) or [Stack Overflow](https://stackoverflow.com/questions/tagged/h2o).\\n- We also recommend filing a bug report (with a reproducible code example) on the [H2O JIRA](http://jira.h2o.ai/) under the \"PUBDEV\" project and the \"h2oEnsemble\" component.'}\n",
      "{'filename': 'h2o-3-master/h2o-r/ensemble/README.md', 'section': '## Benchmarks\\n\\nBenchmarking code for **h2oEnsemble Classic** (compatible with H2O version 2.0, aka \"H2O Classic\") is available here: [https://github.com/ledell/h2oEnsemble-benchmarks](https://github.com/ledell/h2oEnsemble-benchmarks)  These benchmarks are out of date -- a major rewrite of the `h2o.ensemble` backend occured in version 0.0.5.  [New benchmarks forthcoming](https://github.com/ledell/h2oEnsemble-benchmarks/tree/v0.1.8).'}\n",
      "{'filename': 'h2o-3-master/h2o-web/README.md', 'section': '## OSX\\n\\nInstall Node.js\\n\\n    brew install node\\n\\nTo install development dependencies and build the web client, run the top-level gradle build:\\n    \\n    cd /h2o-dev\\n    ./gradlew build -x test\\n    java -jar build/h2o.jar\\n\\nAnd then point your browser to [http://localhost:54321](http://localhost:54321)'}\n",
      "{'filename': 'h2o-3-master/h2o-web/README.md', 'section': '## Linux\\n\\nFirst, install Node.js by following the instructions on the [Node.js wiki](https://github.com/joyent/node/wiki/Installing-Node.js-via-package-manager)\\n\\nTo install development dependencies and build the web client, run the top-level gradle build:\\n    \\n    cd /h2o-dev\\n    ./gradlew build -x test\\n    java -jar build/h2o.jar\\n\\n\\nAnd then point your browser to [http://localhost:54321/](http://localhost:54321/)'}\n",
      "{'filename': 'h2o-3-master/h2o-web/README.md', 'section': '## Windows\\n\\nInstall Node.js [using the official installer](http://nodejs.org/download/). When done, you should have node.exe and npm.cmd in `\\\\Program Files\\\\node\\\\`. These should also be available on your PATH. If not, add the folder to your PATH.\\n\\nRun the top-level gradle build:\\n    \\n    cd /h2o-dev\\n    gradlew.bat build -x test\\n    java -jar build/h2o.jar\\n\\nAnd then point your browser to [http://localhost:54321/](http://localhost:54321/)'}\n",
      "{'filename': 'h2o-3-master/vagrant/README.md', 'section': \"## Instructions\\n\\n1. Install [VirtualBox](https://www.virtualbox.org/).\\n2. Install [Vagrant](http://www.vagrantup.com/downloads).\\n3. Create a working directory, fetch scripts, setup the virtual machine and `ssh` into it.\\n\\n        mkdir development\\n        cd development\\n        curl -sL https://raw.githubusercontent.com/h2oai/h2o-3/master/vagrant/Vagrantfile\\n        curl -sL https://raw.githubusercontent.com/h2oai/h2o-3/master/vagrant/bootstrap.sh\\n        vagrant up\\n        vagrant ssh\\n\\nIf everything goes correctly, you'll now be inside a virtual machine with git, Java, Python, R, Node.js installed, and with H2O's sources (master) fetched and built in `~/h2o-3`.\"}\n"
     ]
    }
   ],
   "source": [
    "for chunk in h2oai_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd76a95-6a55-48a4-b932-00a8d8488182",
   "metadata": {},
   "source": [
    "#### Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e980788-d742-4446-8c97-a79359fd0934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET API key from https://platform.openai.com/api-keys\n",
    "# create an environment variable with your key:\n",
    "# from command line run:\n",
    "# export OPENAI_API_KEY='your-api-key'\n",
    "# uv add openai\n",
    "# uv run jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a660db-488f-431c-acd1-7ab142f0628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e2014-d31b-4d8c-bf35-610cee83ac5c",
   "metadata": {},
   "source": [
    "##### Create a prompt\n",
    "\n",
    "The prompt asks the LLM to:\n",
    "\n",
    "Split the document logically (not just by length)\n",
    "\n",
    "Make sections self-contained\n",
    "\n",
    "Use a specific output format that's easy to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3704f58b-d993-4840-aa2b-0acd60f4d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93fedcd-eb05-406f-b917-9a57b60e82c3",
   "metadata": {},
   "source": [
    "##### Create a function for intelligent chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f256fd0-6882-47cc-8ea8-fde2f0529632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36d33e-3e61-4bca-a267-2286288c0e1f",
   "metadata": {},
   "source": [
    "##### Apply to entire document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f73fa-7068-434e-b0e2-2d23a61d6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "h2oai_chunks = []\n",
    "\n",
    "for doc in tqdm(h2oai):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        h2oai_chunks.append(section_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaffc6d-4782-4527-86e5-66a702b96929",
   "metadata": {},
   "source": [
    "##### Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary. For most applications, you don't need intelligent chunking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414e488-0c00-491c-9a2a-8c816ca85d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
